diff --git a/CMakeLists.txt b/CMakeLists.txt
index a3d3013..63122c4 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -26,10 +26,10 @@ if(NOT DEFINED LLVM_VERSION_MAJOR)
   set(LLVM_VERSION_MAJOR 7)
 endif()
 if(NOT DEFINED LLVM_VERSION_MINOR)
-  set(LLVM_VERSION_MINOR 1)
+  set(LLVM_VERSION_MINOR 0)
 endif()
 if(NOT DEFINED LLVM_VERSION_PATCH)
-  set(LLVM_VERSION_PATCH 0)
+  set(LLVM_VERSION_PATCH 1)
 endif()
 if(NOT DEFINED LLVM_VERSION_SUFFIX)
   set(LLVM_VERSION_SUFFIX "")
diff --git a/README.occlum.md b/README.occlum.md
new file mode 100644
index 0000000..3e5014f
--- /dev/null
+++ b/README.occlum.md
@@ -0,0 +1,60 @@
+# Occlum LLVM Toolchain
+
+Occlum's LLVM toolchain compiles C/C++ programs and generates ELF binaries that can be loaded and run by Occlum LibOS inside Intel SGX enclaves.
+
+## How to build
+
+Occlum toolchain requires [Clang](https://github.com/llvm-mirror/clang), the LLVM frontend for C family languages, and [LLD](https://github.com/occlum/lld), the LLVM linker. As the code of Clang and LLD are in separate repositories, we have to download them first into the `tools` directory of LLVM.
+
+    cd /path/to/occlum/llvm/tools
+
+    git clone https://github.com/llvm-mirror/clang
+    cd clang
+    git checkout 0513b409d5e
+
+    cd ../
+    git clone https://github.com/occlum/lld -b for_occlum
+
+Then, we can build LLVM, Clang, and LLD with the following commands:
+
+
+    cd /path/to/occlum/llvm/../
+    mkdir build-llvm
+    cd build-llvm
+    cmake -DCMAKE_EXPORT_COMPILE_COMMANDS=True -DLLVM_TARGETS_TO_BUILD="X86" -DCMAKE_INSTALL_PREFIX=/usr/local/occlum/ ../llvm/
+    make
+    sudo make install
+
+After LLVM is built, make sure LLVM binaries can be found by adding the following line to the config file for your shell (e.g., `~/.bashrc`)
+
+    export PATH="/usr/local/occlum/bin:$PATH"
+
+
+## How to use
+
+Occlum LLVM toolchain can be used to compile C/C++ programs just like GCC, but with some extra flags. Here is a sample code for Makefile
+
+    CC := clang
+    CLANG_BIN_PATH := $(shell clang -print-prog-name=clang)
+    LLVM_PATH := $(abspath $(dir $(CLANG_BIN_PATH))../)
+
+    CFLAGS := -fPIC
+    LDFLAGS := -fuse-ld=ld -pie
+
+### Extra Compiler Options
+
+Occlum LLVM toolchain has extra compiler arguments are used for controlling the instrumentation of Multi-Domain Software Fault Isolation (MDSFI). Here is the list of the extra arguments and its default value.
+
+Options | Default Values | Descriptions  
+----------------| ------------|---------------
+check-store-only |  true| set this to true will insert guard before memory writes only  
+disable-SFI   |   false  |   set this to true will not insert any data guards  
+enable-x86-mdsfidg | true | set this to false will not translate memory guards instruction. I.e. all memory guard is eliminated  
+enable-x86-mdsfidg-opt | true| set this to false will disable all optimizations on memory guards except GOT optimization  
+enable-x86-mdsfidg-loop-opt | true | set this to false will disable loop optimizations on memory guards  
+enable-x86-mdsficg | true | set this to false will disable all CFI instrument in binary  
+enable-x86-fs-relocate | false| set this to true will insert FS relocate code.  
+
+## Note
+
+Occlum LLVM toolchain is not meant to compile C programs for native platforms (e.g., Linux). For more concrete examples of how to use this toolchain with Occlum LibOS, see [the Makefile](https://github.com/occlum/libos/blob/master/test/test_common.mk) for compiling the C test programs in Occlum LibOS.
diff --git a/buildtoolchain.sh b/buildtoolchain.sh
new file mode 100755
index 0000000..2efd748
--- /dev/null
+++ b/buildtoolchain.sh
@@ -0,0 +1,76 @@
+#!/bin/sh 
+#Usage If you want to install toolchain at a high privilege level location: sudo ./buildtoolchain.sh
+
+SRCROOT=`pwd`/toolchain
+mkdir -p ${SRCROOT}
+cd ${SRCROOT}
+PREFIX=/usr/local/occlum
+rm ${PREFIX}/* -rf
+#checkout repo
+git clone -b for_occlum https://github.com/occlum/llvm
+git clone -b for_occlum https://github.com/occlum/musl
+git clone -b for_occlum https://github.com/occlum/lld
+git clone -b release_70 https://github.com/llvm-mirror/clang
+git clone -b release_70 https://github.com/llvm-mirror/libcxx
+git clone -b release_70 https://github.com/llvm-mirror/libcxxabi
+git clone -b release_70 https://github.com/llvm-mirror/libunwind
+git clone -b release_70 https://github.com/llvm-mirror/compiler-rt
+
+# first stage
+mkdir stage_1
+pushd stage_1
+cmake -DLLVM_TARGETS_TO_BUILD="X86"  -DLLVM_ENABLE_PROJECTS="clang;lld" -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} ../llvm
+make install
+popd
+
+
+#second stage
+export PATH=/usr/local/occlum/bin:$PATH
+
+#build musl
+pushd musl
+CC=clang ./configure --prefix=${PREFIX} --enable-wrapper=clang
+make clean
+make install
+popd
+
+ln -s /usr/include/linux ${PREFIX}/include/linux
+ln -s /usr/include/asm ${PREFIX}/include/asm
+ln -s /usr/include/asm-generic ${PREFIX}/include/asm-generic
+
+mkdir stage_2
+pushd stage_2
+
+#build libunwind
+mkdir libunwind
+pushd libunwind
+cmake ../../libunwind -DCMAKE_INSTALL_PREFIX=${PREFIX} -DCMAKE_C_COMPILER=musl-clang -DCMAKE_CXX_COMPILER=musl-clang -DLIBUNWIND_ENABLE_SHARED=OFF -DLLVM_ENABLE_LIBCXX=ON -DCMAKE_C_FLAGS="-O2 -fPIC -locclum_stub" -DCMAKE_CXX_FLAGS="-O2 -fPIC -locclum_stub"
+make clean
+make install -j
+popd
+
+#build libcxx first round
+mkdir libcxx1
+pushd libcxx1
+cmake ../../libcxx -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${SRCROOT}/stage_2/cxxwithouabi -DCMAKE_C_COMPILER=musl-clang -DCMAKE_CXX_COMPILER=musl-clang -DCMAKE_C_FLAGS="-O2 -fPIC -locclum_stub" -DCMAKE_CXX_FLAGS="-O2 -fPIC -locclum_stub" -DLIBCXX_HAS_MUSL_LIBC=ON -DLIBCXX_ENABLE_SHARED=0
+make clean
+make install -j
+popd
+
+#build libcxxabi with libcxx
+mkdir libcxxabi
+pushd libcxxabi
+cmake ../../libcxxabi -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} -DCMAKE_C_COMPILER=musl-clang -DCMAKE_CXX_COMPILER=musl-clang -DLIBCXXABI_ENABLE_STATIC_UNWINDER=OFF -DLIBCXXABI_ENABLE_SHARED=OFF -DLIBCXXABI_USE_LLVM_UNWINDER=ON -DCMAKE_C_FLAGS="-O2 -fPIC -locclum_stub" -DCMAKE_CXX_FLAGS="-O2 -fPIC -locclum_stub" -DLIBCXXABI_LIBCXX_PATH=../cxxwithoutabi -DLIBCXXABI_ENABLE_PIC=ON -DLLVM_ENABLE_LIBCXX=ON
+make clean
+make install -j
+popd
+
+#build libcxx second round, with libcxxabi
+mkdir libcxx2
+pushd libcxx2
+cmake ../../libcxx -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=${PREFIX} -DCMAKE_C_COMPILER=musl-clang -DCMAKE_CXX_COMPILER=musl-clang -DLIBCXX_HAS_MUSL_LIBC=ON -DLIBCXX_CXX_ABI=libcxxabi -DLIBCXX_ENABLE_SHARED=0 -DLIBCXXABI_USE_LLVM_UNWINDER=ON -DLIBCXX_CXX_ABI_LIBRARY_PATH=${PREFIX}/lib -DLIBCXX_CXX_ABI_INCLUDE_PATHS=../../libcxxabi/include -DCMAKE_C_FLAGS="-O2 -fPIC -locclum_stub" -DCMAKE_CXX_FLAGS="-O2 -fPIC -locclum_stub"
+make clean
+make install -j
+popd
+popd
+
diff --git a/cmake/modules/AddLLVM.cmake b/cmake/modules/AddLLVM.cmake
index c0f90ba..ce2057f 100644
--- a/cmake/modules/AddLLVM.cmake
+++ b/cmake/modules/AddLLVM.cmake
@@ -83,7 +83,7 @@ function(add_llvm_symbol_exports target_name export_file)
     # FIXME: Don't write the "local:" line on OpenBSD.
     # in the export file, also add a linker script to version LLVM symbols (form: LLVM_N.M)
     add_custom_command(OUTPUT ${native_export_file}
-      COMMAND echo "LLVM_${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR} {" > ${native_export_file}
+      COMMAND echo "LLVM_${LLVM_VERSION_MAJOR} {" > ${native_export_file}
       COMMAND grep -q "[[:alnum:]]" ${export_file} && echo "  global:" >> ${native_export_file} || :
       COMMAND sed -e "s/$/;/" -e "s/^/    /" < ${export_file} >> ${native_export_file}
       COMMAND echo "  local: *;" >> ${native_export_file}
@@ -498,11 +498,9 @@ function(llvm_add_library name)
     if(UNIX AND NOT APPLE AND NOT ARG_SONAME)
       set_target_properties(${name}
         PROPERTIES
-		# Concatenate the version numbers since ldconfig expects exactly
-		# one component indicating the ABI version, while LLVM uses
-		# major+minor for that.
-        SOVERSION ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}
-        VERSION ${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}${LLVM_VERSION_SUFFIX})
+        # Since 4.0.0, the ABI version is indicated by the major version
+        SOVERSION ${LLVM_VERSION_MAJOR}
+        VERSION ${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX})
     endif()
   endif()
 
@@ -524,7 +522,7 @@ function(llvm_add_library name)
       if(${output_name} STREQUAL "output_name-NOTFOUND")
         set(output_name ${name})
       endif()
-      set(library_name ${output_name}-${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}${LLVM_VERSION_SUFFIX})
+      set(library_name ${output_name}-${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX})
       set(api_name ${output_name}-${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}.${LLVM_VERSION_PATCH}${LLVM_VERSION_SUFFIX})
       set_target_properties(${name} PROPERTIES OUTPUT_NAME ${library_name})
       llvm_install_library_symlink(${api_name} ${library_name} SHARED
diff --git a/docs/ReleaseNotes.rst b/docs/ReleaseNotes.rst
index 2fb253a..158f097 100644
--- a/docs/ReleaseNotes.rst
+++ b/docs/ReleaseNotes.rst
@@ -30,6 +30,9 @@ Non-comprehensive list of changes in this release
   is available on the Visual Studio Marketplace. The new integration
   supports Visual Studio 2017.
 
+* Libraries have been renamed from 7.0 to 7. This change also impacts
+  downstream libraries like lldb.
+
 * The LoopInstSimplify pass (``-loop-instsimplify``) has been removed.
 
 * Symbols starting with ``?`` are no longer mangled by LLVM when using the
diff --git a/include/llvm/ADT/Optional.h b/include/llvm/ADT/Optional.h
index 9fe9b28..353e5d0 100644
--- a/include/llvm/ADT/Optional.h
+++ b/include/llvm/ADT/Optional.h
@@ -108,6 +108,24 @@ template <typename T, bool IsPodLike> struct OptionalStorage {
   }
 };
 
+#if !defined(__GNUC__) || defined(__clang__) // GCC up to GCC7 miscompiles this.
+/// Storage for trivially copyable types only.
+template <typename T> struct OptionalStorage<T, true> {
+  AlignedCharArrayUnion<T> storage;
+  bool hasVal = false;
+
+  OptionalStorage() = default;
+
+  OptionalStorage(const T &y) : hasVal(true) { new (storage.buffer) T(y); }
+  OptionalStorage &operator=(const T &y) {
+    *reinterpret_cast<T *>(storage.buffer) = y;
+    hasVal = true;
+    return *this;
+  }
+
+  void reset() { hasVal = false; }
+};
+#endif
 } // namespace optional_detail
 
 template <typename T> class Optional {
diff --git a/include/llvm/Analysis/LoopIterator.h b/include/llvm/Analysis/LoopIterator.h
index 91c54b2..3c79d60 100644
--- a/include/llvm/Analysis/LoopIterator.h
+++ b/include/llvm/Analysis/LoopIterator.h
@@ -26,10 +26,16 @@
 
 #include "llvm/ADT/PostOrderIterator.h"
 #include "llvm/Analysis/LoopInfo.h"
+#include "llvm/CodeGen/MachineLoopInfo.h"
 
 namespace llvm {
 
 class LoopBlocksTraversal;
+class MachineLoopBlocksTraversal;
+class MachineLoop;
+class MachineLoopInfo;
+class MachineBasicBlock;
+class MachineFunction;
 
 // A traits type that is intended to be used in graph algorithms. The graph
 // traits starts at the loop header, and traverses the BasicBlocks that are in
@@ -255,6 +261,132 @@ finishPostorder(BasicBlock *BB) {
   LBT.finishPostorder(BB);
 }
 
+class MachineLoopBlocksDFS {
+public:
+	typedef std::vector<MachineBasicBlock *>::const_iterator POIterator;
+	typedef std::vector<MachineBasicBlock*>::const_reverse_iterator RPOIterator;
+
+	friend class MachineLoopBlocksTraversal;
+
+private:
+	MachineLoop *L;
+
+	DenseMap<MachineBasicBlock*, unsigned> PostNumbers;
+	std::vector<MachineBasicBlock*> PostBlocks;
+public:
+	MachineLoopBlocksDFS(MachineLoop *Container):
+	L(Container),PostNumbers(NextPowerOf2(Container->getNumBlocks())) {
+		PostBlocks.reserve(Container->getNumBlocks());
+	}
+
+	MachineLoop *getLoop() const {return L;}
+	void perform(MachineLoopInfo *MLI);
+	bool isComplete() const{return PostBlocks.size() == L->getNumBlocks();}
+
+	POIterator beginPostorder() const {
+		assert(isComplete() && "bad Loop DFS");
+		return PostBlocks.begin();
+	}
+	POIterator endPostorder() const{return PostBlocks.end();}
+
+	RPOIterator beginRPO() const {
+		assert(isComplete() && "bad Loop DFS");
+		return PostBlocks.rbegin();
+	}
+	RPOIterator endRPO() const {return PostBlocks.rend();}
+	bool hasPreorder(MachineBasicBlock *BB) const {return PostNumbers.count(BB);}\
+	bool hasPostorder(MachineBasicBlock *BB) const {
+		DenseMap<MachineBasicBlock *, unsigned> :: const_iterator I = PostNumbers.find(BB);
+		return I != PostNumbers.end() && I->second;
+	}
+	void clear() {
+		PostNumbers.clear();
+		PostBlocks.clear();
+	}
+};
+
+ /// Wrapper class to LoopBlocksDFS that provides a standard begin()/end()
+ /// interface for the DFS reverse post-order traversal of blocks in a loop body.
+ class MachineLoopBlocksRPO {
+ private:
+   MachineLoopBlocksDFS DFS;
+ 
+ public:
+   MachineLoopBlocksRPO(MachineLoop *Container) : DFS(Container) {}
+ 
+   /// Traverse the loop blocks and store the DFS result.
+   void perform(MachineLoopInfo *LI) {
+     DFS.perform(LI);
+   }
+ 
+   /// Reverse iterate over the cached postorder blocks.
+   MachineLoopBlocksDFS::RPOIterator begin() const { return DFS.beginRPO(); }
+   MachineLoopBlocksDFS::RPOIterator end() const { return DFS.endRPO(); }
+ };
+
+/// Specialize po_iterator_storage to record postorder numbers.
+template<> class po_iterator_storage<MachineLoopBlocksTraversal, true> {
+  MachineLoopBlocksTraversal &LBT;
+public:
+  po_iterator_storage(MachineLoopBlocksTraversal &lbs) : LBT(lbs) {}
+  // These functions are defined below.
+  bool insertEdge(Optional<MachineBasicBlock *> From, MachineBasicBlock *To);
+  void finishPostorder(MachineBasicBlock *BB);
+};
+
+ 
+class MachineLoopBlocksTraversal{
+public:
+	typedef po_iterator<MachineBasicBlock*,MachineLoopBlocksTraversal, true> POTIterator; 
+
+private:
+	MachineLoopBlocksDFS &DFS;
+	MachineLoopInfo *LI;
+
+public:
+	MachineLoopBlocksTraversal(MachineLoopBlocksDFS &Storage, MachineLoopInfo *LInfo) :
+	DFS(Storage), LI(LInfo){}
+	POTIterator begin() {
+		assert(DFS.PostBlocks.empty() && "Need clear DFS result before traversing");
+		assert(DFS.L->getNumBlocks() && "po_iterator cannot handle an empty graph");
+		return po_ext_begin(DFS.L->getHeader(), *this);
+	}
+	POTIterator end() {
+     // po_ext_end interface requires a basic block, but ignores its value.
+     return po_ext_end(DFS.L->getHeader(), *this);
+   }
+
+   /// Called by po_iterator upon reaching a block via a CFG edge. If this block
+   /// is contained in the loop and has not been visited, then mark it preorder
+   /// visited and return true.
+   ///
+   /// TODO: If anyone is interested, we could record preorder numbers here.
+   bool visitPreorder(MachineBasicBlock *BB) {
+     if (!DFS.L->contains(LI->getLoopFor(BB)))
+       return false;
+ 
+     return DFS.PostNumbers.insert(std::make_pair(BB, 0)).second;
+   }
+   /// Called by po_iterator each time it advances, indicating a block's
+   /// postorder.
+   void finishPostorder(MachineBasicBlock *BB) {
+     assert(DFS.PostNumbers.count(BB) && "Loop DFS skipped preorder");
+     DFS.PostBlocks.push_back(BB);
+     DFS.PostNumbers[BB] = DFS.PostBlocks.size();
+   }
+
+};
+
+inline bool po_iterator_storage<MachineLoopBlocksTraversal, true>::insertEdge(
+    Optional<MachineBasicBlock *> From, MachineBasicBlock *To) {
+  return LBT.visitPreorder(To);
+}
+
+inline void po_iterator_storage<MachineLoopBlocksTraversal, true>::
+finishPostorder(MachineBasicBlock *BB) {
+  LBT.finishPostorder(BB);
+}
+
 } // End namespace llvm
 
 #endif
diff --git a/include/llvm/CodeGen/RegsRange.h b/include/llvm/CodeGen/RegsRange.h
new file mode 100644
index 0000000..2170505
--- /dev/null
+++ b/include/llvm/CodeGen/RegsRange.h
@@ -0,0 +1,370 @@
+//===- llvm/CodeGen/RegRange.h - Register Range Set ----------*- C++ -*-===//
+//
+/// \file
+/// A set of register ranges. It is intended for backend range analysis.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef LLVM_CODEGEN_REGSRANGE_H
+#define LLVM_CODEGEN_REGSRANGE_H
+
+#include "llvm/CodeGen/MachineRegisterInfo.h"
+#include "llvm/CodeGen/TargetRegisterInfo.h"
+#include "llvm/MC/LaneBitmask.h"
+#include "llvm/MC/MCRegisterInfo.h"
+#include <memory>
+
+using std::unique_ptr;
+
+namespace llvm {
+
+#define DEBUG_TYPE "RegRange"
+class MachineInstr;
+class MachineBasicBlock;
+class raw_ostream;
+
+enum Range {
+  InRead,
+  InWrite,
+  SmallNum,
+  Undef,
+  Unknown,
+};
+class RangeInfo {
+public:
+  Range RangeClass = Undef;
+  int UpRange;
+  int LowRange;
+
+  RangeInfo(Range R, int up, int low) {
+    RangeClass = R;
+    UpRange = up;
+    LowRange = low;
+  }
+  RangeInfo(Range R) {
+    RangeClass = R;
+    UpRange = 0;
+    LowRange = 0;
+  }
+  RangeInfo() {
+    RangeClass = Undef;
+    UpRange = 0;
+    LowRange = 0;
+  }
+  /* RangeInfo() = delete; */
+
+  RangeInfo(const RangeInfo &R) {
+    RangeClass = R.RangeClass;
+    UpRange = R.UpRange;
+    LowRange = R.LowRange;
+  }
+
+  bool min(RangeInfo R) {
+    if (R.RangeClass != SmallNum)
+      return false;
+    if (RangeClass == Undef || RangeClass == Unknown)
+      return false;
+    UpRange = UpRange - R.UpRange;
+    LowRange = LowRange - R.LowRange;
+    return true;
+  }
+
+  bool add(RangeInfo R) {
+    if (R.RangeClass != SmallNum)
+      return false;
+    if (RangeClass == Undef || RangeClass == Unknown)
+      return false;
+    UpRange = UpRange + R.UpRange;
+    LowRange = LowRange + R.LowRange;
+    return true;
+  }
+
+  bool multiple(int i) {
+    if (RangeClass != SmallNum)
+      return false;
+    UpRange = UpRange * i;
+    LowRange = LowRange * i;
+    return true;
+  }
+
+  bool add(int i) {
+    if (RangeClass == Undef || RangeClass == Unknown)
+      return false;
+    UpRange = UpRange + i;
+    LowRange = LowRange + i;
+    return true;
+  }
+
+  bool isInRange(const RangeInfo &R) const {
+    if (RangeClass == Undef || RangeClass == Unknown)
+      return false;
+    if (RangeClass == R.RangeClass) {
+      if (UpRange < R.UpRange && LowRange > R.LowRange)
+        return true;
+      else
+        return false;
+    }
+
+    // FIXME
+    // depending on READ/WRITE region size
+    if (RangeClass == InWrite && R.RangeClass == InRead) {
+      return true;
+    }
+    return false;
+  }
+
+  bool equal(const RangeInfo R) {
+    if (RangeClass != R.RangeClass) {
+      return false;
+    }
+    if (UpRange != R.UpRange || LowRange != R.LowRange) {
+      return false;
+    }
+    return true;
+  }
+
+  // Lattic meet operation
+  //      ---------
+  //      | Undef |
+  //      ---------
+  // ---------
+  // |InWrite|
+  // ---------  ----------
+  // |InRead |  |SmallNum|
+  // ---------  ----------
+  //      ---------
+  //      |Unknown|
+  //      ---------
+  //
+  //  meet will extend it's range
+  static RangeInfo meet(RangeInfo &A, RangeInfo &B) {
+    if (A.RangeClass == Undef) {
+      return B;
+    } else if (B.RangeClass == Undef) {
+      return A;
+    }
+    if (A.RangeClass == Unknown || B.RangeClass == Unknown) {
+      return RangeInfo(Unknown);
+    }
+    if (A.RangeClass == B.RangeClass) {
+      int up = A.UpRange > B.UpRange ? A.UpRange : B.UpRange;
+      int low = A.LowRange < B.LowRange ? A.LowRange : B.LowRange;
+      return RangeInfo(A.RangeClass, up, low);
+    }
+    if (A.RangeClass == InWrite && B.RangeClass == InRead) {
+      return B;
+    }
+    if (A.RangeClass == InRead && B.RangeClass == InWrite) {
+      return A;
+    }
+    return RangeInfo(Unknown);
+  }
+  // cut current range to bound check's Range.
+  // if current range is exceed guard's range, cut it.
+  // otherwise, return current range;
+  static RangeInfo cutRange(RangeInfo &current, RangeInfo &guard) {
+    if (guard.RangeClass == current.RangeClass) {
+      // get lower one for up
+      int up =
+          current.UpRange > guard.UpRange ? guard.UpRange : current.UpRange;
+      // get higher one for low
+      int low =
+          current.LowRange < guard.LowRange ? guard.LowRange : current.LowRange;
+      return RangeInfo(current.RangeClass, up, low);
+    }
+    // If current and guard are not in the same RangeClass, then if current is
+    // smaller, then return current
+    if (current.RangeClass == InWrite) {
+      return current;
+    }
+    // other situation return guard
+    return guard;
+  }
+  void print(raw_ostream &OS) const {
+    OS << "RangeInfo:";
+    switch (RangeClass) {
+    case Undef:
+      OS << " Undef\n";
+      break;
+    case Unknown:
+      OS << " Unknown\n";
+      break;
+    case InWrite:
+      OS << " InWrite, Up: " << UpRange << " Low: " << LowRange << "\n";
+      break;
+    case InRead:
+      OS << " InRead, Up: " << UpRange << " Low: " << LowRange << "\n";
+      break;
+    case SmallNum:
+      OS << " Smallnum, Up: " << UpRange << " Low: " << LowRange << "\n";
+      break;
+    default:
+      break;
+    }
+  }
+};
+inline raw_ostream &operator<<(raw_ostream &OS, const RangeInfo &LR) {
+  LR.print(OS);
+  return OS;
+}
+class RegsRange {
+
+  const TargetRegisterInfo *TRI = nullptr;
+
+public:
+  std::map<unsigned, RangeInfo> Ranges;
+  RegsRange(const TargetRegisterInfo &TRI) { init(TRI); }
+  RegsRange() = delete;
+
+  void init(const TargetRegisterInfo &TRI) {
+    this->TRI = &TRI;
+    Ranges.clear();
+  }
+
+  // RegsRange A equals B iif every Range in A equal B's Range
+  // and every Range in B equal A's Range
+  bool equal(RegsRange &R) {
+    for (auto &RegIt : Ranges) {
+      if (!RegIt.second.equal(R.Ranges[RegIt.first])) {
+        return false;
+      }
+    }
+    for (auto &RegIt : R.Ranges) {
+      if (RegIt.second.RangeClass == Undef)
+        continue;
+      LLVM_DEBUG(dbgs() << TRI->getRegAsmName(RegIt.first) << "\nR.range "
+                        << RegIt.second);
+      LLVM_DEBUG(dbgs() << "range " << Ranges[RegIt.first]);
+      if (!RegIt.second.equal(Ranges[RegIt.first])) {
+        return false;
+      }
+      LLVM_DEBUG(dbgs() << "equals \n");
+    }
+    LLVM_DEBUG(dbgs() << "return equals \n");
+    return true;
+  }
+
+  void setAllUnknown() {
+    for (auto &RegIt : Ranges) {
+      if (RegIt.second.RangeClass != Undef)
+        RegIt.second = RangeInfo(Unknown);
+    }
+  }
+
+  // compare this with R, if any difference, set to unknown
+  // set Changed Unknown should always
+  void setChangedUnknown(RegsRange &R) {
+    LLVM_DEBUG(dbgs() << "setChangedUnknown\t");
+    for (auto &RegIt : Ranges) {
+      // do not be undef, so avoid register alias
+      if (RegIt.second.RangeClass != Undef) {
+        if (!RegIt.second.equal(R.Ranges[RegIt.first])) {
+          LLVM_DEBUG(dbgs() << TRI->getRegAsmName(RegIt.first) << "\t");
+          RangeInfo r(Unknown);
+          // use setRegRange so if R use different register, the programe won't
+          // set two overlaped register
+          setRegRange(RegIt.first, r);
+        }
+      }
+    }
+    LLVM_DEBUG(dbgs() << "\n");
+  }
+
+  // get a reg range of Reg.
+  RangeInfo getRegRange(unsigned Reg) {
+    LLVM_DEBUG(dbgs() << "getRegRange \t");
+    for (MCSubRegIterator SubRegs(Reg, TRI, true); SubRegs.isValid(); ++SubRegs)
+      if (Ranges[*SubRegs].RangeClass != Undef) {
+        LLVM_DEBUG(dbgs() << TRI->getRegAsmName(*SubRegs) << "\t");
+        LLVM_DEBUG(dbgs() << Ranges[*SubRegs] << "\t");
+        return Ranges[*SubRegs];
+      }
+    // if any of the super register has a range, then set it's range to
+    // unknown
+    for (MCSuperRegIterator SuperRegs(Reg, TRI, false); SuperRegs.isValid();
+         ++SuperRegs) {
+      LLVM_DEBUG(dbgs() << TRI->getRegAsmName(*SuperRegs) << "\t");
+      LLVM_DEBUG(dbgs() << Ranges[*SuperRegs] << "\t");
+      if (Ranges[*SuperRegs].RangeClass != Undef)
+        return RangeInfo(Unknown);
+    }
+    // else return Undef
+    return Ranges[Reg];
+  }
+
+  // This function will set all Reg's related range
+  // When the register is new defined, set it's range as fresh
+  // Like Move m, Reg
+  void setRegRange(unsigned Reg, RangeInfo &Range) {
+    LLVM_DEBUG(dbgs() << "setRegRange: \n");
+    for (MCSubRegIterator SubRegs(Reg, TRI, false); SubRegs.isValid();
+         ++SubRegs) {
+      LLVM_DEBUG(dbgs() << TRI->getRegAsmName(*SubRegs) << "\t");
+      Ranges[*SubRegs] = RangeInfo(Undef);
+      LLVM_DEBUG(dbgs() << Ranges[*SubRegs] << "\t");
+    }
+    for (MCSuperRegIterator SuperRegs(Reg, TRI, false); SuperRegs.isValid();
+         ++SuperRegs) {
+      LLVM_DEBUG(dbgs() << TRI->getRegAsmName(*SuperRegs) << "\t");
+      Ranges[*SuperRegs] = RangeInfo(Undef);
+      LLVM_DEBUG(dbgs() << Ranges[*SuperRegs] << "\t");
+    }
+    LLVM_DEBUG(dbgs() << TRI->getRegAsmName(Reg) << "\n");
+    Ranges[Reg] = Range;
+    LLVM_DEBUG(dbgs() << Ranges[Reg] << "\t");
+  }
+
+  // merge r into this regsrange
+  void merge(RegsRange &r) {
+    for (auto &RegIt : r.Ranges) {
+      // find all ranges in this range
+      if (RegIt.second.RangeClass != Undef) {
+        bool found;
+        // find subregister in this with Ranges
+        for (MCSubRegIterator SubRegs(RegIt.first, TRI, true);
+             SubRegs.isValid(); ++SubRegs) {
+          if (this->Ranges[*SubRegs].RangeClass != Undef) {
+            this->Ranges[*SubRegs] =
+                RangeInfo::meet(Ranges[*SubRegs], RegIt.second);
+            found = true;
+            break;
+          }
+        }
+        // if found r go to next register
+        if (found)
+          continue;
+        // find supregister in r with Ranges
+        for (MCSuperRegIterator SuperRegs(RegIt.first, TRI, false);
+             SuperRegs.isValid(); ++SuperRegs) {
+          if (this->Ranges[*SuperRegs].RangeClass != Undef) {
+            this->Ranges[*SuperRegs] = RangeInfo(Undef);
+            this->Ranges[RegIt.first] =
+                RangeInfo::meet(Ranges[*SuperRegs], RegIt.second);
+            found = true;
+            break;
+          }
+        }
+        if (!found) {
+          this->Ranges[RegIt.first] = RegIt.second;
+        }
+      }
+    }
+  }
+
+  void print(raw_ostream &OS) const {
+    OS << "RegsRange: \n";
+    for (auto &RegIt : this->Ranges) {
+      if (RegIt.second.RangeClass != Undef)
+        OS << TRI->getRegAsmName(RegIt.first) << " " << RegIt.second;
+    }
+    OS << "\n";
+  }
+};
+
+inline raw_ostream &operator<<(raw_ostream &OS, const RegsRange &LR) {
+  LR.print(OS);
+  return OS;
+}
+} // end of namespace llvm
+#undef DEBUG_TYPE
+#endif
diff --git a/include/llvm/DebugInfo/PDB/Native/GlobalsStream.h b/include/llvm/DebugInfo/PDB/Native/GlobalsStream.h
index dd04b5c..fdc58dc 100644
--- a/include/llvm/DebugInfo/PDB/Native/GlobalsStream.h
+++ b/include/llvm/DebugInfo/PDB/Native/GlobalsStream.h
@@ -30,6 +30,8 @@ class GSIHashIterator
           GSIHashIterator, FixedStreamArrayIterator<PSHashRecord>,
           std::random_access_iterator_tag, const uint32_t> {
 public:
+  GSIHashIterator() = default;
+
   template <typename T>
   GSIHashIterator(T &&v)
       : GSIHashIterator::iterator_adaptor_base(std::forward<T &&>(v)) {}
diff --git a/include/llvm/DebugInfo/PDB/Native/ModuleDebugStream.h b/include/llvm/DebugInfo/PDB/Native/ModuleDebugStream.h
index efc25e0..6602264 100644
--- a/include/llvm/DebugInfo/PDB/Native/ModuleDebugStream.h
+++ b/include/llvm/DebugInfo/PDB/Native/ModuleDebugStream.h
@@ -49,7 +49,7 @@ public:
   BinarySubstreamRef getC13LinesSubstream() const;
   BinarySubstreamRef getGlobalRefsSubstream() const;
 
-  ModuleDebugStreamRef &operator=(ModuleDebugStreamRef &&Other) = delete;
+  ModuleDebugStreamRef &operator=(ModuleDebugStreamRef &&Other) = default;
 
   iterator_range<DebugSubsectionIterator> subsections() const;
   codeview::DebugSubsectionArray getSubsectionsArray() const {
diff --git a/include/llvm/ExecutionEngine/Orc/Core.h b/include/llvm/ExecutionEngine/Orc/Core.h
index 11d7c09..fd03687 100644
--- a/include/llvm/ExecutionEngine/Orc/Core.h
+++ b/include/llvm/ExecutionEngine/Orc/Core.h
@@ -126,7 +126,7 @@ class MaterializationResponsibility {
 public:
   MaterializationResponsibility(MaterializationResponsibility &&) = default;
   MaterializationResponsibility &
-  operator=(MaterializationResponsibility &&) = delete;
+  operator=(MaterializationResponsibility &&) = default;
 
   /// Destruct a MaterializationResponsibility instance. In debug mode
   ///        this asserts that all symbols being tracked have been either
diff --git a/include/llvm/ExecutionEngine/Orc/OrcRemoteTargetClient.h b/include/llvm/ExecutionEngine/Orc/OrcRemoteTargetClient.h
index 45f95f6..739e5ba 100644
--- a/include/llvm/ExecutionEngine/Orc/OrcRemoteTargetClient.h
+++ b/include/llvm/ExecutionEngine/Orc/OrcRemoteTargetClient.h
@@ -70,7 +70,8 @@ public:
     RemoteRTDyldMemoryManager &
     operator=(const RemoteRTDyldMemoryManager &) = delete;
     RemoteRTDyldMemoryManager(RemoteRTDyldMemoryManager &&) = default;
-    RemoteRTDyldMemoryManager &operator=(RemoteRTDyldMemoryManager &&) = delete;
+    RemoteRTDyldMemoryManager &
+    operator=(RemoteRTDyldMemoryManager &&) = default;
 
     uint8_t *allocateCodeSection(uintptr_t Size, unsigned Alignment,
                                  unsigned SectionID,
diff --git a/include/llvm/IR/IntrinsicsX86.td b/include/llvm/IR/IntrinsicsX86.td
index 905afc1..b842df2 100644
--- a/include/llvm/IR/IntrinsicsX86.td
+++ b/include/llvm/IR/IntrinsicsX86.td
@@ -10,6 +10,10 @@
 // This file defines all of the X86-specific intrinsics.
 //
 //===----------------------------------------------------------------------===//
+let TargetPrefix = "x86" in {
+  def int_x86_checkload:Intrinsic<[],[llvm_anyptr_ty],[]>;
+  def int_x86_checkstore:Intrinsic<[],[llvm_anyptr_ty],[]>;
+}
 
 //===----------------------------------------------------------------------===//
 // Interrupt traps
diff --git a/include/llvm/MC/MCAsmBackend.h b/include/llvm/MC/MCAsmBackend.h
index 07835c2..030d3c0 100644
--- a/include/llvm/MC/MCAsmBackend.h
+++ b/include/llvm/MC/MCAsmBackend.h
@@ -165,11 +165,6 @@ public:
     return 0;
   }
 
-  /// Check whether a given symbol has been flagged with MICROMIPS flag.
-  virtual bool isMicroMips(const MCSymbol *Sym) const {
-    return false;
-  }
-
   /// Handles all target related code padding when starting to write a new
   /// basic block to an object file.
   ///
diff --git a/include/llvm/ProfileData/Coverage/CoverageMapping.h b/include/llvm/ProfileData/Coverage/CoverageMapping.h
index e820f71..ecb284d 100644
--- a/include/llvm/ProfileData/Coverage/CoverageMapping.h
+++ b/include/llvm/ProfileData/Coverage/CoverageMapping.h
@@ -641,6 +641,8 @@ public:
     this->operator++();
   }
 
+  LineCoverageIterator &operator=(const LineCoverageIterator &R) = default;
+
   bool operator==(const LineCoverageIterator &R) const {
     return &CD == &R.CD && Next == R.Next && Ended == R.Ended;
   }
diff --git a/include/llvm/Support/GenericDomTreeConstruction.h b/include/llvm/Support/GenericDomTreeConstruction.h
index 977f209..103ff8c 100644
--- a/include/llvm/Support/GenericDomTreeConstruction.h
+++ b/include/llvm/Support/GenericDomTreeConstruction.h
@@ -1186,20 +1186,6 @@ struct SemiNCAInfo {
                << '\t' << U << "\n");
     LLVM_DEBUG(dbgs() << "\n");
 
-    // Recalculate the DominatorTree when the number of updates
-    // exceeds a threshold, which usually makes direct updating slower than
-    // recalculation. We select this threshold proportional to the
-    // size of the DominatorTree. The constant is selected
-    // by choosing the one with an acceptable performance on some real-world
-    // inputs.
-
-    // Make unittests of the incremental algorithm work
-    if (DT.DomTreeNodes.size() <= 100) {
-      if (NumLegalized > DT.DomTreeNodes.size())
-        CalculateFromScratch(DT, &BUI);
-    } else if (NumLegalized > DT.DomTreeNodes.size() / 40)
-      CalculateFromScratch(DT, &BUI);
-
     // If the DominatorTree was recalculated at some point, stop the batch
     // updates. Full recalculations ignore batch updates and look at the actual
     // CFG.
diff --git a/include/llvm/Transforms/Utils/SSAUpdater.h b/include/llvm/Transforms/Utils/SSAUpdater.h
index d02607a..4a79116 100644
--- a/include/llvm/Transforms/Utils/SSAUpdater.h
+++ b/include/llvm/Transforms/Utils/SSAUpdater.h
@@ -76,10 +76,6 @@ public:
   /// block.
   bool HasValueForBlock(BasicBlock *BB) const;
 
-  /// Return the value for the specified block if the SSAUpdater has one,
-  /// otherwise return nullptr.
-  Value *FindValueForBlock(BasicBlock *BB) const;
-
   /// Construct SSA form, materializing a value that is live at the end
   /// of the specified block.
   Value *GetValueAtEndOfBlock(BasicBlock *BB);
diff --git a/include/llvm/Transforms/Utils/SSAUpdaterImpl.h b/include/llvm/Transforms/Utils/SSAUpdaterImpl.h
index cab0f3e..b7649ba 100644
--- a/include/llvm/Transforms/Utils/SSAUpdaterImpl.h
+++ b/include/llvm/Transforms/Utils/SSAUpdaterImpl.h
@@ -357,9 +357,10 @@ public:
       BBInfo *Info = *I;
 
       if (Info->DefBB != Info) {
-        // Record the available value to speed up subsequent uses of this
-        // SSAUpdater for the same value.
-        (*AvailableVals)[Info->BB] = Info->DefBB->AvailableVal;
+        // Record the available value at join nodes to speed up subsequent
+        // uses of this SSAUpdater for the same value.
+        if (Info->NumPreds > 1)
+          (*AvailableVals)[Info->BB] = Info->DefBB->AvailableVal;
         continue;
       }
 
diff --git a/lib/Analysis/MemorySSA.cpp b/lib/Analysis/MemorySSA.cpp
index 6e49a39..b38c0c4 100644
--- a/lib/Analysis/MemorySSA.cpp
+++ b/lib/Analysis/MemorySSA.cpp
@@ -119,6 +119,7 @@ class MemoryLocOrCall {
 public:
   bool IsCall = false;
 
+  MemoryLocOrCall() = default;
   MemoryLocOrCall(MemoryUseOrDef *MUD)
       : MemoryLocOrCall(MUD->getMemoryInst()) {}
   MemoryLocOrCall(const MemoryUseOrDef *MUD)
diff --git a/lib/CodeGen/MachineLoopInfo.cpp b/lib/CodeGen/MachineLoopInfo.cpp
index 2bce592..9b1d50e 100644
--- a/lib/CodeGen/MachineLoopInfo.cpp
+++ b/lib/CodeGen/MachineLoopInfo.cpp
@@ -16,6 +16,7 @@
 
 #include "llvm/CodeGen/MachineLoopInfo.h"
 #include "llvm/Analysis/LoopInfoImpl.h"
+#include "llvm/Analysis/LoopIterator.h"
 #include "llvm/CodeGen/MachineDominators.h"
 #include "llvm/CodeGen/Passes.h"
 #include "llvm/Config/llvm-config.h"
@@ -144,3 +145,18 @@ LLVM_DUMP_METHOD void MachineLoop::dump() const {
   print(dbgs());
 }
 #endif
+
+//===----------------------------------------------------------------------===//
+// MachineLoopBlocksDFS implementation
+//
+
+/// Traverse the loop blocks and store the DFS result.
+/// Useful for clients that just want the final DFS result and don't need to
+/// visit blocks during the initial traversal.
+void MachineLoopBlocksDFS::perform(MachineLoopInfo *LI) {
+  MachineLoopBlocksTraversal Traversal(*this, LI);
+  for (MachineLoopBlocksTraversal::POTIterator POI = Traversal.begin(),
+                                        POE = Traversal.end();
+       POI != POE; ++POI)
+    ;
+}
diff --git a/lib/CodeGen/RegisterScavenging.cpp b/lib/CodeGen/RegisterScavenging.cpp
index 3660586..2948b26 100644
--- a/lib/CodeGen/RegisterScavenging.cpp
+++ b/lib/CodeGen/RegisterScavenging.cpp
@@ -590,21 +590,26 @@ unsigned RegScavenger::scavengeRegisterBackwards(const TargetRegisterClass &RC,
   MachineBasicBlock::iterator SpillBefore = P.second;
   assert(Reg != 0 && "No register left to scavenge!");
   // Found an available register?
-  if (SpillBefore != MBB.end()) {
-    MachineBasicBlock::iterator ReloadAfter =
-      RestoreAfter ? std::next(MBBI) : MBBI;
-    MachineBasicBlock::iterator ReloadBefore = std::next(ReloadAfter);
-    if (ReloadBefore != MBB.end())
-      LLVM_DEBUG(dbgs() << "Reload before: " << *ReloadBefore << '\n');
-    ScavengedInfo &Scavenged = spill(Reg, RC, SPAdj, SpillBefore, ReloadBefore);
-    Scavenged.Restore = &*std::prev(SpillBefore);
-    LiveUnits.removeReg(Reg);
-    LLVM_DEBUG(dbgs() << "Scavenged register with spill: " << printReg(Reg, TRI)
-                      << " until " << *SpillBefore);
-  } else {
+  if (SpillBefore == MBB.end()) {
     LLVM_DEBUG(dbgs() << "Scavenged free register: " << printReg(Reg, TRI)
                       << '\n');
+    return Reg;
   }
+  // occlum do not use register scavenger's spill
+  bool AllowSpill=false;
+  if(!AllowSpill)
+    return 0;
+
+  MachineBasicBlock::iterator ReloadAfter =
+    RestoreAfter ? std::next(MBBI) : MBBI;
+  MachineBasicBlock::iterator ReloadBefore = std::next(ReloadAfter);
+  if (ReloadBefore != MBB.end())
+    LLVM_DEBUG(dbgs() << "Reload before: " << *ReloadBefore << '\n');
+  ScavengedInfo &Scavenged = spill(Reg, RC, SPAdj, SpillBefore, ReloadBefore);
+  Scavenged.Restore = &*std::prev(SpillBefore);
+  LiveUnits.removeReg(Reg);
+  LLVM_DEBUG(dbgs() << "Scavenged register with spill: " << printReg(Reg, TRI)
+                    << " until " << *SpillBefore);
   return Reg;
 }
 
diff --git a/lib/CodeGen/TargetLoweringObjectFileImpl.cpp b/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
index 16140f0..f6b91a2 100644
--- a/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
+++ b/lib/CodeGen/TargetLoweringObjectFileImpl.cpp
@@ -1156,11 +1156,10 @@ MCSection *TargetLoweringObjectFileCOFF::SelectSectionForGlobal(
       MCSymbol *Sym = TM.getSymbol(ComdatGV);
       StringRef COMDATSymName = Sym->getName();
 
-      // Append "$symbol" to the section name *before* IR-level mangling is
-      // applied when targetting mingw. This is what GCC does, and the ld.bfd
+      // Append "$symbol" to the section name when targetting mingw. The ld.bfd
       // COFF linker will not properly handle comdats otherwise.
       if (getTargetTriple().isWindowsGNUEnvironment())
-        raw_svector_ostream(Name) << '$' << ComdatGV->getName();
+        raw_svector_ostream(Name) << '$' << COMDATSymName;
 
       return getContext().getCOFFSection(Name, Characteristics, Kind,
                                          COMDATSymName, Selection, UniqueID);
diff --git a/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp b/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp
index 76f5e5e..1189be5 100644
--- a/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp
+++ b/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp
@@ -275,7 +275,7 @@ RuntimeDyldImpl::loadObjectImpl(const object::ObjectFile &Obj) {
           uint64_t Size = I->getCommonSize();
           if (!CommonAlign)
             CommonAlign = Align;
-          CommonSize = alignTo(CommonSize, Align) + Size;
+          CommonSize += alignTo(CommonSize, Align) + Size;
           CommonSymbolsToAllocate.push_back(*I);
         }
       } else
diff --git a/lib/MC/MCExpr.cpp b/lib/MC/MCExpr.cpp
index ef6f004..a4c99a0 100644
--- a/lib/MC/MCExpr.cpp
+++ b/lib/MC/MCExpr.cpp
@@ -524,11 +524,6 @@ static void AttemptToFoldSymbolOffsetDifference(
     if (Asm->isThumbFunc(&SA))
       Addend |= 1;
 
-    // If symbol is labeled as micromips, we set low-bit to ensure
-    // correct offset in .gcc_except_table
-    if (Asm->getBackend().isMicroMips(&SA))
-      Addend |= 1;
-
     // Clear the symbol expr pointers to indicate we have folded these
     // operands.
     A = B = nullptr;
diff --git a/lib/Target/AArch64/AArch64ISelLowering.cpp b/lib/Target/AArch64/AArch64ISelLowering.cpp
index cfc7aa9..de762a7 100644
--- a/lib/Target/AArch64/AArch64ISelLowering.cpp
+++ b/lib/Target/AArch64/AArch64ISelLowering.cpp
@@ -1515,50 +1515,39 @@ static SDValue emitComparison(SDValue LHS, SDValue RHS, ISD::CondCode CC,
 /// The CCMP/CCMN/FCCMP/FCCMPE instructions allow the conditional execution of
 /// a comparison. They set the NZCV flags to a predefined value if their
 /// predicate is false. This allows to express arbitrary conjunctions, for
-/// example "cmp 0 (and (setCA (cmp A)) (setCB (cmp B)))"
+/// example "cmp 0 (and (setCA (cmp A)) (setCB (cmp B))))"
 /// expressed as:
 ///   cmp A
 ///   ccmp B, inv(CB), CA
 ///   check for CB flags
 ///
-/// This naturally lets us implement chains of AND operations with SETCC
-/// operands. And we can even implement some other situations by transforming
-/// them:
-///   - We can implement (NEG SETCC) i.e. negating a single comparison by
-///     negating the flags used in a CCMP/FCCMP operations.
-///   - We can negate the result of a whole chain of CMP/CCMP/FCCMP operations
-///     by negating the flags we test for afterwards. i.e.
-///     NEG (CMP CCMP CCCMP ...) can be implemented.
-///   - Note that we can only ever negate all previously processed results.
-///     What we can not implement by flipping the flags to test is a negation
-///     of two sub-trees (because the negation affects all sub-trees emitted so
-///     far, so the 2nd sub-tree we emit would also affect the first).
-/// With those tools we can implement some OR operations:
-///   - (OR (SETCC A) (SETCC B)) can be implemented via:
-///     NEG (AND (NEG (SETCC A)) (NEG (SETCC B)))
-///   - After transforming OR to NEG/AND combinations we may be able to use NEG
-///     elimination rules from earlier to implement the whole thing as a
-///     CCMP/FCCMP chain.
+/// In general we can create code for arbitrary "... (and (and A B) C)"
+/// sequences. We can also implement some "or" expressions, because "(or A B)"
+/// is equivalent to "not (and (not A) (not B))" and we can implement some
+/// negation operations:
+/// We can negate the results of a single comparison by inverting the flags
+/// used when the predicate fails and inverting the flags tested in the next
+/// instruction; We can also negate the results of the whole previous
+/// conditional compare sequence by inverting the flags tested in the next
+/// instruction. However there is no way to negate the result of a partial
+/// sequence.
 ///
-/// As complete example:
-///     or (or (setCA (cmp A)) (setCB (cmp B)))
-///        (and (setCC (cmp C)) (setCD (cmp D)))"
-/// can be reassociated to:
-///     or (and (setCC (cmp C)) setCD (cmp D))
-//         (or (setCA (cmp A)) (setCB (cmp B)))
-/// can be transformed to:
-///     not (and (not (and (setCC (cmp C)) (setCD (cmp D))))
-///              (and (not (setCA (cmp A)) (not (setCB (cmp B))))))"
-/// which can be implemented as:
+/// Therefore on encountering an "or" expression we can negate the subtree on
+/// one side and have to be able to push the negate to the leafs of the subtree
+/// on the other side (see also the comments in code). As complete example:
+/// "or (or (setCA (cmp A)) (setCB (cmp B)))
+///     (and (setCC (cmp C)) (setCD (cmp D)))"
+/// is transformed to
+/// "not (and (not (and (setCC (cmp C)) (setCC (cmp D))))
+///           (and (not (setCA (cmp A)) (not (setCB (cmp B))))))"
+/// and implemented as:
 ///   cmp C
 ///   ccmp D, inv(CD), CC
 ///   ccmp A, CA, inv(CD)
 ///   ccmp B, CB, inv(CA)
 ///   check for CB flags
-///
-/// A counterexample is "or (and A B) (and C D)" which translates to
-/// not (and (not (and (not A) (not B))) (not (and (not C) (not D)))), we
-/// can only implement 1 of the inner (not) operations, but not both!
+/// A counterexample is "or (and A B) (and C D)" which cannot be implemented
+/// by conditional compare sequences.
 /// @{
 
 /// Create a conditional comparison; Use CCMP, CCMN or FCCMP as appropriate.
@@ -1596,23 +1585,14 @@ static SDValue emitConditionalComparison(SDValue LHS, SDValue RHS,
   return DAG.getNode(Opcode, DL, MVT_CC, LHS, RHS, NZCVOp, Condition, CCOp);
 }
 
-/// Returns true if @p Val is a tree of AND/OR/SETCC operations that can be
-/// expressed as a conjunction. See \ref AArch64CCMP.
-/// \param CanNegate    Set to true if we can negate the whole sub-tree just by
-///                     changing the conditions on the SETCC tests.
-///                     (this means we can call emitConjunctionRec() with
-///                      Negate==true on this sub-tree)
-/// \param MustBeFirst  Set to true if this subtree needs to be negated and we
-///                     cannot do the negation naturally. We are required to
-///                     emit the subtree first in this case.
-/// \param WillNegate   Is true if are called when the result of this
-///                     subexpression must be negated. This happens when the
-///                     outer expression is an OR. We can use this fact to know
-///                     that we have a double negation (or (or ...) ...) that
-///                     can be implemented for free.
-static bool canEmitConjunction(const SDValue Val, bool &CanNegate,
-                               bool &MustBeFirst, bool WillNegate,
-                               unsigned Depth = 0) {
+/// Returns true if @p Val is a tree of AND/OR/SETCC operations.
+/// CanPushNegate is set to true if we can push a negate operation through
+/// the tree in a was that we are left with AND operations and negate operations
+/// at the leafs only. i.e. "not (or (or x y) z)" can be changed to
+/// "and (and (not x) (not y)) (not z)"; "not (or (and x y) z)" cannot be
+/// brought into such a form.
+static bool isConjunctionDisjunctionTree(const SDValue Val, bool &CanNegate,
+                                         unsigned Depth = 0) {
   if (!Val.hasOneUse())
     return false;
   unsigned Opcode = Val->getOpcode();
@@ -1620,44 +1600,39 @@ static bool canEmitConjunction(const SDValue Val, bool &CanNegate,
     if (Val->getOperand(0).getValueType() == MVT::f128)
       return false;
     CanNegate = true;
-    MustBeFirst = false;
     return true;
   }
   // Protect against exponential runtime and stack overflow.
   if (Depth > 6)
     return false;
   if (Opcode == ISD::AND || Opcode == ISD::OR) {
-    bool IsOR = Opcode == ISD::OR;
     SDValue O0 = Val->getOperand(0);
     SDValue O1 = Val->getOperand(1);
     bool CanNegateL;
-    bool MustBeFirstL;
-    if (!canEmitConjunction(O0, CanNegateL, MustBeFirstL, IsOR, Depth+1))
+    if (!isConjunctionDisjunctionTree(O0, CanNegateL, Depth+1))
       return false;
     bool CanNegateR;
-    bool MustBeFirstR;
-    if (!canEmitConjunction(O1, CanNegateR, MustBeFirstR, IsOR, Depth+1))
-      return false;
-
-    if (MustBeFirstL && MustBeFirstR)
+    if (!isConjunctionDisjunctionTree(O1, CanNegateR, Depth+1))
       return false;
 
-    if (IsOR) {
-      // For an OR expression we need to be able to naturally negate at least
-      // one side or we cannot do the transformation at all.
+    if (Opcode == ISD::OR) {
+      // For an OR expression we need to be able to negate at least one side or
+      // we cannot do the transformation at all.
       if (!CanNegateL && !CanNegateR)
         return false;
-      // If we the result of the OR will be negated and we can naturally negate
-      // the leafs, then this sub-tree as a whole negates naturally.
-      CanNegate = WillNegate && CanNegateL && CanNegateR;
-      // If we cannot naturally negate the whole sub-tree, then this must be
-      // emitted first.
-      MustBeFirst = !CanNegate;
+      // We can however change a (not (or x y)) to (and (not x) (not y)) if we
+      // can negate the x and y subtrees.
+      CanNegate = CanNegateL && CanNegateR;
     } else {
-      assert(Opcode == ISD::AND && "Must be OR or AND");
-      // We cannot naturally negate an AND operation.
+      // If the operands are OR expressions then we finally need to negate their
+      // outputs, we can only do that for the operand with emitted last by
+      // negating OutCC, not for both operands.
+      bool NeedsNegOutL = O0->getOpcode() == ISD::OR;
+      bool NeedsNegOutR = O1->getOpcode() == ISD::OR;
+      if (NeedsNegOutL && NeedsNegOutR)
+        return false;
+      // We cannot negate an AND operation (it would become an OR),
       CanNegate = false;
-      MustBeFirst = MustBeFirstL || MustBeFirstR;
     }
     return true;
   }
@@ -1670,9 +1645,11 @@ static bool canEmitConjunction(const SDValue Val, bool &CanNegate,
 /// and conditional compare operations. @returns an NZCV flags producing node
 /// and sets @p OutCC to the flags that should be tested or returns SDValue() if
 /// transformation was not possible.
-/// \p Negate is true if we want this sub-tree being negated just by changing
-/// SETCC conditions.
-static SDValue emitConjunctionRec(SelectionDAG &DAG, SDValue Val,
+/// On recursive invocations @p PushNegate may be set to true to have negation
+/// effects pushed to the tree leafs; @p Predicate is an NZCV flag predicate
+/// for the comparisons in the current subtree; @p Depth limits the search
+/// depth to avoid stack overflow.
+static SDValue emitConjunctionDisjunctionTreeRec(SelectionDAG &DAG, SDValue Val,
     AArch64CC::CondCode &OutCC, bool Negate, SDValue CCOp,
     AArch64CC::CondCode Predicate) {
   // We're at a tree leaf, produce a conditional comparison operation.
@@ -1713,85 +1690,76 @@ static SDValue emitConjunctionRec(SelectionDAG &DAG, SDValue Val,
     return emitConditionalComparison(LHS, RHS, CC, CCOp, Predicate, OutCC, DL,
                                      DAG);
   }
-  assert(Val->hasOneUse() && "Valid conjunction/disjunction tree");
-
-  bool IsOR = Opcode == ISD::OR;
+  assert((Opcode == ISD::AND || (Opcode == ISD::OR && Val->hasOneUse())) &&
+         "Valid conjunction/disjunction tree");
 
+  // Check if both sides can be transformed.
   SDValue LHS = Val->getOperand(0);
-  bool CanNegateL;
-  bool MustBeFirstL;
-  bool ValidL = canEmitConjunction(LHS, CanNegateL, MustBeFirstL, IsOR);
-  assert(ValidL && "Valid conjunction/disjunction tree");
-  (void)ValidL;
-
   SDValue RHS = Val->getOperand(1);
-  bool CanNegateR;
-  bool MustBeFirstR;
-  bool ValidR = canEmitConjunction(RHS, CanNegateR, MustBeFirstR, IsOR);
-  assert(ValidR && "Valid conjunction/disjunction tree");
-  (void)ValidR;
-
-  // Swap sub-tree that must come first to the right side.
-  if (MustBeFirstL) {
-    assert(!MustBeFirstR && "Valid conjunction/disjunction tree");
-    std::swap(LHS, RHS);
-    std::swap(CanNegateL, CanNegateR);
-    std::swap(MustBeFirstL, MustBeFirstR);
-  }
-
-  bool NegateR;
-  bool NegateAfterR;
-  bool NegateL;
-  bool NegateAfterAll;
-  if (Opcode == ISD::OR) {
-    // Swap the sub-tree that we can negate naturally to the left.
-    if (!CanNegateL) {
-      assert(CanNegateR && "at least one side must be negatable");
-      assert(!MustBeFirstR && "invalid conjunction/disjunction tree");
-      assert(!Negate);
+
+  // In case of an OR we need to negate our operands and the result.
+  // (A v B) <=> not(not(A) ^ not(B))
+  bool NegateOpsAndResult = Opcode == ISD::OR;
+  // We can negate the results of all previous operations by inverting the
+  // predicate flags giving us a free negation for one side. The other side
+  // must be negatable by itself.
+  if (NegateOpsAndResult) {
+    // See which side we can negate.
+    bool CanNegateL;
+    bool isValidL = isConjunctionDisjunctionTree(LHS, CanNegateL);
+    assert(isValidL && "Valid conjunction/disjunction tree");
+    (void)isValidL;
+
+#ifndef NDEBUG
+    bool CanNegateR;
+    bool isValidR = isConjunctionDisjunctionTree(RHS, CanNegateR);
+    assert(isValidR && "Valid conjunction/disjunction tree");
+    assert((CanNegateL || CanNegateR) && "Valid conjunction/disjunction tree");
+#endif
+
+    // Order the side which we cannot negate to RHS so we can emit it first.
+    if (!CanNegateL)
       std::swap(LHS, RHS);
-      NegateR = false;
-      NegateAfterR = true;
-    } else {
-      // Negate the left sub-tree if possible, otherwise negate the result.
-      NegateR = CanNegateR;
-      NegateAfterR = !CanNegateR;
-    }
-    NegateL = true;
-    NegateAfterAll = !Negate;
   } else {
-    assert(Opcode == ISD::AND && "Valid conjunction/disjunction tree");
-    assert(!Negate && "Valid conjunction/disjunction tree");
-
-    NegateL = false;
-    NegateR = false;
-    NegateAfterR = false;
-    NegateAfterAll = false;
+    bool NeedsNegOutL = LHS->getOpcode() == ISD::OR;
+    assert((!NeedsNegOutL || RHS->getOpcode() != ISD::OR) &&
+           "Valid conjunction/disjunction tree");
+    // Order the side where we need to negate the output flags to RHS so it
+    // gets emitted first.
+    if (NeedsNegOutL)
+      std::swap(LHS, RHS);
   }
 
-  // Emit sub-trees.
+  // Emit RHS. If we want to negate the tree we only need to push a negate
+  // through if we are already in a PushNegate case, otherwise we can negate
+  // the "flags to test" afterwards.
   AArch64CC::CondCode RHSCC;
-  SDValue CmpR = emitConjunctionRec(DAG, RHS, RHSCC, NegateR, CCOp, Predicate);
-  if (NegateAfterR)
+  SDValue CmpR = emitConjunctionDisjunctionTreeRec(DAG, RHS, RHSCC, Negate,
+                                                   CCOp, Predicate);
+  if (NegateOpsAndResult && !Negate)
     RHSCC = AArch64CC::getInvertedCondCode(RHSCC);
-  SDValue CmpL = emitConjunctionRec(DAG, LHS, OutCC, NegateL, CmpR, RHSCC);
-  if (NegateAfterAll)
+  // Emit LHS. We may need to negate it.
+  SDValue CmpL = emitConjunctionDisjunctionTreeRec(DAG, LHS, OutCC,
+                                                   NegateOpsAndResult, CmpR,
+                                                   RHSCC);
+  // If we transformed an OR to and AND then we have to negate the result
+  // (or absorb the Negate parameter).
+  if (NegateOpsAndResult && !Negate)
     OutCC = AArch64CC::getInvertedCondCode(OutCC);
   return CmpL;
 }
 
-/// Emit expression as a conjunction (a series of CCMP/CFCMP ops).
-/// In some cases this is even possible with OR operations in the expression.
-/// See \ref AArch64CCMP.
-/// \see emitConjunctionRec().
-static SDValue emitConjunction(SelectionDAG &DAG, SDValue Val,
-                               AArch64CC::CondCode &OutCC) {
-  bool DummyCanNegate;
-  bool DummyMustBeFirst;
-  if (!canEmitConjunction(Val, DummyCanNegate, DummyMustBeFirst, false))
+/// Emit conjunction or disjunction tree with the CMP/FCMP followed by a chain
+/// of CCMP/CFCMP ops. See @ref AArch64CCMP.
+/// \see emitConjunctionDisjunctionTreeRec().
+static SDValue emitConjunctionDisjunctionTree(SelectionDAG &DAG, SDValue Val,
+                                              AArch64CC::CondCode &OutCC) {
+  bool CanNegate;
+  if (!isConjunctionDisjunctionTree(Val, CanNegate))
     return SDValue();
 
-  return emitConjunctionRec(DAG, Val, OutCC, false, SDValue(), AArch64CC::AL);
+  return emitConjunctionDisjunctionTreeRec(DAG, Val, OutCC, false, SDValue(),
+                                           AArch64CC::AL);
 }
 
 /// @}
@@ -1891,7 +1859,7 @@ static SDValue getAArch64Cmp(SDValue LHS, SDValue RHS, ISD::CondCode CC,
     }
 
     if (!Cmp && (RHSC->isNullValue() || RHSC->isOne())) {
-      if ((Cmp = emitConjunction(DAG, LHS, AArch64CC))) {
+      if ((Cmp = emitConjunctionDisjunctionTree(DAG, LHS, AArch64CC))) {
         if ((CC == ISD::SETNE) ^ RHSC->isNullValue())
           AArch64CC = AArch64CC::getInvertedCondCode(AArch64CC);
       }
diff --git a/lib/Target/AMDGPU/AMDGPULibFunc.cpp b/lib/Target/AMDGPU/AMDGPULibFunc.cpp
index f37795e..4671273 100644
--- a/lib/Target/AMDGPU/AMDGPULibFunc.cpp
+++ b/lib/Target/AMDGPU/AMDGPULibFunc.cpp
@@ -90,6 +90,7 @@ class UnmangledFuncInfo {
 
 public:
   using ID = AMDGPULibFunc::EFuncId;
+  UnmangledFuncInfo() = default;
   UnmangledFuncInfo(StringRef _Name, unsigned _NumArgs)
       : Name(_Name), NumArgs(_NumArgs) {}
   // Get index to Table by function name.
diff --git a/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.cpp b/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.cpp
index 3b1b94a..4397c97 100644
--- a/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.cpp
+++ b/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.cpp
@@ -25,7 +25,6 @@
 #include "llvm/MC/MCFixupKindInfo.h"
 #include "llvm/MC/MCObjectWriter.h"
 #include "llvm/MC/MCSubtargetInfo.h"
-#include "llvm/MC/MCSymbolELF.h"
 #include "llvm/MC/MCTargetOptions.h"
 #include "llvm/MC/MCValue.h"
 #include "llvm/Support/ErrorHandling.h"
@@ -569,14 +568,6 @@ bool MipsAsmBackend::shouldForceRelocation(const MCAssembler &Asm,
   }
 }
 
-bool MipsAsmBackend::isMicroMips(const MCSymbol *Sym) const {
-  if (const auto *ElfSym = dyn_cast<const MCSymbolELF>(Sym)) {
-    if (ElfSym->getOther() & ELF::STO_MIPS_MICROMIPS)
-      return true;
-  }
-  return false;
-}
-
 MCAsmBackend *llvm::createMipsAsmBackend(const Target &T,
                                          const MCSubtargetInfo &STI,
                                          const MCRegisterInfo &MRI,
diff --git a/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.h b/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.h
index 3035913..3d5e16f 100644
--- a/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.h
+++ b/lib/Target/Mips/MCTargetDesc/MipsAsmBackend.h
@@ -25,7 +25,6 @@ class MCAssembler;
 struct MCFixupKindInfo;
 class MCObjectWriter;
 class MCRegisterInfo;
-class MCSymbolELF;
 class Target;
 
 class MipsAsmBackend : public MCAsmBackend {
@@ -91,7 +90,6 @@ public:
   bool shouldForceRelocation(const MCAssembler &Asm, const MCFixup &Fixup,
                              const MCValue &Target) override;
 
-  bool isMicroMips(const MCSymbol *Sym) const override;
 }; // class MipsAsmBackend
 
 } // namespace
diff --git a/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.cpp b/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.cpp
index 21b01e8..7b9a025 100644
--- a/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.cpp
+++ b/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.cpp
@@ -15,7 +15,6 @@
 #include "llvm/MC/MCAssembler.h"
 #include "llvm/MC/MCCodeEmitter.h"
 #include "llvm/MC/MCContext.h"
-#include "llvm/MC/MCDwarf.h"
 #include "llvm/MC/MCInst.h"
 #include "llvm/MC/MCObjectWriter.h"
 #include "llvm/MC/MCSymbolELF.h"
@@ -54,22 +53,6 @@ void MipsELFStreamer::EmitInstruction(const MCInst &Inst,
   createPendingLabelRelocs();
 }
 
-void MipsELFStreamer::EmitCFIStartProcImpl(MCDwarfFrameInfo &Frame) {
-  Frame.Begin = getContext().createTempSymbol();
-  MCELFStreamer::EmitLabel(Frame.Begin);
-}
-
-MCSymbol *MipsELFStreamer::EmitCFILabel() {
-  MCSymbol *Label = getContext().createTempSymbol("cfi", true);
-  MCELFStreamer::EmitLabel(Label);
-  return Label;
-}
-
-void MipsELFStreamer::EmitCFIEndProcImpl(MCDwarfFrameInfo &Frame) {
-  Frame.End = getContext().createTempSymbol();
-  MCELFStreamer::EmitLabel(Frame.End);
-}
-
 void MipsELFStreamer::createPendingLabelRelocs() {
   MipsTargetELFStreamer *ELFTargetStreamer =
       static_cast<MipsTargetELFStreamer *>(getTargetStreamer());
diff --git a/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.h b/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.h
index 56a0ff9..d141f5d 100644
--- a/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.h
+++ b/lib/Target/Mips/MCTargetDesc/MipsELFStreamer.h
@@ -26,7 +26,6 @@ class MCAsmBackend;
 class MCCodeEmitter;
 class MCContext;
 class MCSubtargetInfo;
-struct MCDwarfFrameInfo;
 
 class MipsELFStreamer : public MCELFStreamer {
   SmallVector<std::unique_ptr<MipsOptionRecord>, 8> MipsOptionRecords;
@@ -61,12 +60,6 @@ public:
   void EmitValueImpl(const MCExpr *Value, unsigned Size, SMLoc Loc) override;
   void EmitIntValue(uint64_t Value, unsigned Size) override;
 
-  // Overriding these functions allows us to avoid recording of these labels
-  // in EmitLabel and later marking them as microMIPS.
-  void EmitCFIStartProcImpl(MCDwarfFrameInfo &Frame) override;
-  void EmitCFIEndProcImpl(MCDwarfFrameInfo &Frame) override;
-  MCSymbol *EmitCFILabel() override;
-
   /// Emits all the option records stored up until the point it's called.
   void EmitMipsOptionRecords();
 
diff --git a/lib/Target/PowerPC/P9InstrResources.td b/lib/Target/PowerPC/P9InstrResources.td
index c6cbb90..34df845 100644
--- a/lib/Target/PowerPC/P9InstrResources.td
+++ b/lib/Target/PowerPC/P9InstrResources.td
@@ -592,7 +592,6 @@ def : InstRW<[P9_PM_3C, IP_EXECO_1C, IP_EXECE_1C, DISP_1C, DISP_1C, DISP_1C],
     XXPERM,
     XXPERMR,
     XXSLDWI,
-    XXSLDWIs,
     XXSPLTIB,
     XXSPLTW,
     XXSPLTWs,
diff --git a/lib/Target/PowerPC/PPCISelLowering.cpp b/lib/Target/PowerPC/PPCISelLowering.cpp
index b5bdf47..331dbcb 100644
--- a/lib/Target/PowerPC/PPCISelLowering.cpp
+++ b/lib/Target/PowerPC/PPCISelLowering.cpp
@@ -8454,6 +8454,17 @@ SDValue PPCTargetLowering::LowerVECTOR_SHUFFLE(SDValue Op,
     if (V2.isUndef() && PPC::isSplatShuffleMask(SVOp, 4)) {
       int SplatIdx = PPC::getVSPLTImmediate(SVOp, 4, DAG);
 
+      // If the source for the shuffle is a scalar_to_vector that came from a
+      // 32-bit load, it will have used LXVWSX so we don't need to splat again.
+      if (Subtarget.hasP9Vector() &&
+          ((isLittleEndian && SplatIdx == 3) ||
+           (!isLittleEndian && SplatIdx == 0))) {
+        SDValue Src = V1.getOperand(0);
+        if (Src.getOpcode() == ISD::SCALAR_TO_VECTOR &&
+            Src.getOperand(0).getOpcode() == ISD::LOAD &&
+            Src.getOperand(0).hasOneUse())
+          return V1;
+      }
       SDValue Conv = DAG.getNode(ISD::BITCAST, dl, MVT::v4i32, V1);
       SDValue Splat = DAG.getNode(PPCISD::XXSPLT, dl, MVT::v4i32, Conv,
                                   DAG.getConstant(SplatIdx, dl, MVT::i32));
diff --git a/lib/Target/PowerPC/PPCInstrVSX.td b/lib/Target/PowerPC/PPCInstrVSX.td
index 781a327..183512a 100644
--- a/lib/Target/PowerPC/PPCInstrVSX.td
+++ b/lib/Target/PowerPC/PPCInstrVSX.td
@@ -877,12 +877,6 @@ let Uses = [RM] in {
                        "xxsldwi $XT, $XA, $XB, $SHW", IIC_VecPerm,
                        [(set v4i32:$XT, (PPCvecshl v4i32:$XA, v4i32:$XB,
                                                   imm32SExt16:$SHW))]>;
-
-  let isCodeGenOnly = 1 in
-  def XXSLDWIs : XX3Form_2s<60, 2,
-                       (outs vsrc:$XT), (ins vsfrc:$XA, u2imm:$SHW),
-                       "xxsldwi $XT, $XA, $XA, $SHW", IIC_VecPerm, []>;
-
   def XXSPLTW : XX2Form_2<60, 164,
                        (outs vsrc:$XT), (ins vsrc:$XB, u2imm:$UIM),
                        "xxspltw $XT, $XB, $UIM", IIC_VecPerm,
@@ -892,7 +886,6 @@ let Uses = [RM] in {
   def XXSPLTWs : XX2Form_2<60, 164,
                        (outs vsrc:$XT), (ins vfrc:$XB, u2imm:$UIM),
                        "xxspltw $XT, $XB, $UIM", IIC_VecPerm, []>;
-
 } // hasSideEffects
 } // UseVSXReg = 1
 
@@ -1473,6 +1466,8 @@ let AddedComplexity = 400 in { // Prefer VSX patterns over non-VSX patterns.
                    (f64 (PPCmtvsra (i64 (vector_extract v2i64:$S, 1)))))),
             (f32 (XSCVUXDSP (COPY_TO_REGCLASS (XXPERMDI $S, $S, 2), VSFRC)))>;
   }
+  def : Pat<(v4i32 (scalar_to_vector ScalarLoads.Li32)),
+            (v4i32 (XXSPLTWs (LIWAX xoaddr:$src), 1))>;
 
   // Instructions for converting float to i64 feeding a store.
   let Predicates = [NoP9Vector] in {
@@ -3055,47 +3050,13 @@ let AddedComplexity = 400, Predicates = [HasP9Vector] in {
             (STXVX $rS, xoaddr:$dst)>;
   def : Pat<(int_ppc_vsx_stxvd2x v2f64:$rS, xoaddr:$dst),
             (STXVX $rS, xoaddr:$dst)>;
-
-  let AddedComplexity = 400 in {
-    // LIWAX - This instruction is used for sign extending i32 -> i64.
-    // LIWZX - This instruction will be emitted for i32, f32, and when
-    //         zero-extending i32 to i64 (zext i32 -> i64).
-    let Predicates = [IsLittleEndian] in {
-
-      def : Pat<(v2i64 (scalar_to_vector (i64 (sextloadi32 xoaddr:$src)))),
-                (v2i64 (XXPERMDIs
-                (COPY_TO_REGCLASS (LIWAX xoaddr:$src), VSRC), 2))>;
-
-      def : Pat<(v2i64 (scalar_to_vector (i64 (zextloadi32 xoaddr:$src)))),
-                (v2i64 (XXPERMDIs
-                (COPY_TO_REGCLASS (LIWZX xoaddr:$src), VSRC), 2))>;
-
-      def : Pat<(v4i32 (scalar_to_vector (i32 (load xoaddr:$src)))),
-                (v4i32 (XXPERMDIs
-                (COPY_TO_REGCLASS (LIWZX xoaddr:$src), VSRC), 2))>;
-
-      def : Pat<(v4f32 (scalar_to_vector (f32 (load xoaddr:$src)))),
-                (v4f32 (XXPERMDIs
-                (COPY_TO_REGCLASS (LIWZX xoaddr:$src), VSRC), 2))>;
-    }
-
-    let Predicates = [IsBigEndian] in {
-      def : Pat<(v2i64 (scalar_to_vector (i64 (sextloadi32 xoaddr:$src)))),
-                (v2i64 (COPY_TO_REGCLASS (LIWAX xoaddr:$src), VSRC))>;
-
-      def : Pat<(v2i64 (scalar_to_vector (i64 (zextloadi32 xoaddr:$src)))),
-                (v2i64 (COPY_TO_REGCLASS (LIWZX xoaddr:$src), VSRC))>;
-
-      def : Pat<(v4i32 (scalar_to_vector (i32 (load xoaddr:$src)))),
-                (v4i32 (XXSLDWIs
-                (COPY_TO_REGCLASS (LIWZX xoaddr:$src), VSRC), 1))>;
-
-      def : Pat<(v4f32 (scalar_to_vector (f32 (load xoaddr:$src)))),
-                (v4f32 (XXSLDWIs
-                (COPY_TO_REGCLASS (LIWZX xoaddr:$src), VSRC), 1))>;
-    }
-
-  }
+  def : Pat<(v4i32 (scalar_to_vector (i32 (load xoaddr:$src)))),
+            (v4i32 (LXVWSX xoaddr:$src))>;
+  def : Pat<(v4f32 (scalar_to_vector (f32 (load xoaddr:$src)))),
+            (v4f32 (LXVWSX xoaddr:$src))>;
+  def : Pat<(v4f32 (scalar_to_vector
+                     (f32 (fpround (f64 (extloadf32 xoaddr:$src)))))),
+            (v4f32 (LXVWSX xoaddr:$src))>;
 
   // Build vectors from i8 loads
   def : Pat<(v16i8 (scalar_to_vector ScalarLoads.Li8)),
@@ -3257,39 +3218,6 @@ let AddedComplexity = 400, Predicates = [HasP9Vector] in {
   def : Pat<(f32 (fpround (f64 (extloadf32 ixaddr:$src)))),
             (f32 (DFLOADf32 ixaddr:$src))>;
 
-
-  let AddedComplexity = 400 in {
-  // The following pseudoinstructions are used to ensure the utilization
-  // of all 64 VSX registers.
-    let Predicates = [IsLittleEndian, HasP9Vector] in {
-      def : Pat<(v2i64 (scalar_to_vector (i64 (load ixaddr:$src)))),
-                (v2i64 (XXPERMDIs
-                (COPY_TO_REGCLASS (DFLOADf64 ixaddr:$src), VSRC), 2))>;
-      def : Pat<(v2i64 (scalar_to_vector (i64 (load xaddr:$src)))),
-                (v2i64 (XXPERMDIs
-		(COPY_TO_REGCLASS (XFLOADf64 xaddr:$src), VSRC), 2))>;
-
-      def : Pat<(v2f64 (scalar_to_vector (f64 (load ixaddr:$src)))),
-                (v2f64 (XXPERMDIs
-                (COPY_TO_REGCLASS (DFLOADf64 ixaddr:$src), VSRC), 2))>;
-      def : Pat<(v2f64 (scalar_to_vector (f64 (load xaddr:$src)))),
-                (v2f64 (XXPERMDIs
-                (COPY_TO_REGCLASS (XFLOADf64 xaddr:$src), VSRC), 2))>;
-    }
-
-    let Predicates = [IsBigEndian, HasP9Vector] in {
-      def : Pat<(v2i64 (scalar_to_vector (i64 (load ixaddr:$src)))),
-                (v2i64 (COPY_TO_REGCLASS (DFLOADf64 ixaddr:$src), VSRC))>;
-      def : Pat<(v2i64 (scalar_to_vector (i64 (load xaddr:$src)))),
-                (v2i64 (COPY_TO_REGCLASS (XFLOADf64 xaddr:$src), VSRC))>;
-
-      def : Pat<(v2f64 (scalar_to_vector (f64 (load ixaddr:$src)))),
-                (v2f64 (COPY_TO_REGCLASS (DFLOADf64 ixaddr:$src), VSRC))>;
-      def : Pat<(v2f64 (scalar_to_vector (f64 (load xaddr:$src)))),
-                (v2f64 (COPY_TO_REGCLASS (XFLOADf64 xaddr:$src), VSRC))>;
-    }
-  }
-
   let Predicates = [IsBigEndian, HasP9Vector] in {
 
     // (Un)Signed DWord vector extract -> QP
@@ -4004,4 +3932,3 @@ let AddedComplexity = 400 in {
               (v4i32 (VEXTSH2W $A))>;
   }
 }
-
diff --git a/lib/Target/X86/CMakeLists.txt b/lib/Target/X86/CMakeLists.txt
index 0dbc82b..98ad7b7 100644
--- a/lib/Target/X86/CMakeLists.txt
+++ b/lib/Target/X86/CMakeLists.txt
@@ -65,6 +65,9 @@ set(sources
   X86VZeroUpper.cpp
   X86WinAllocaExpander.cpp
   X86WinEHState.cpp
+  X86MDSFIDataGuard.cpp
+  X86MDSFIControlGuard.cpp
+  X86RegValueTracking.cpp
   )
 
 add_llvm_target(X86CodeGen ${sources})
diff --git a/lib/Target/X86/X86.h b/lib/Target/X86/X86.h
index 73bb0f2..80d4698 100644
--- a/lib/Target/X86/X86.h
+++ b/lib/Target/X86/X86.h
@@ -121,6 +121,12 @@ FunctionPass *createX86EvexToVexInsts();
 /// This pass creates the thunks for the retpoline feature.
 FunctionPass *createX86RetpolineThunksPass();
 
+/// This pass for MDSFI Data guards. 
+FunctionPass *createX86MDSFIDataGuard();
+
+/// This pass for MDSFI Control Guards at backend
+FunctionPass *createX86MDSFIControlGuard();
+
 InstructionSelector *createX86InstructionSelector(const X86TargetMachine &TM,
                                                   X86Subtarget &,
                                                   X86RegisterBankInfo &);
diff --git a/lib/Target/X86/X86InstrMPX.td b/lib/Target/X86/X86InstrMPX.td
index c1a8cc7..0905453 100644
--- a/lib/Target/X86/X86InstrMPX.td
+++ b/lib/Target/X86/X86InstrMPX.td
@@ -14,6 +14,26 @@
 //===----------------------------------------------------------------------===//
 
 // FIXME: Investigate a better scheduler class once MPX is used inside LLVM.
+let isPseudo = 1 in {
+  def checkload64m : PseudoI<(outs), (ins i64mem:$src),
+                          [(int_x86_checkload addr:$src)]>; 
+  def checkload32m : PseudoI<(outs), (ins i32mem:$src),
+                          [(int_x86_checkload addr:$src)]>; 
+  def checkstore64m : PseudoI<(outs), (ins i64mem:$src), 
+                          [(int_x86_checkstore addr:$src)]>; 
+  def checkstore32m : PseudoI<(outs), (ins i32mem:$src), 
+                          [(int_x86_checkstore addr:$src)]>; 
+  def checkload64r : PseudoI<(outs), (ins GR64:$src),
+                          [(int_x86_checkload addr:$src)]>; 
+  def checkstore64r : PseudoI<(outs), (ins GR64:$src), 
+                          [(int_x86_checkstore addr:$src)]>; 
+  def checkload32r: PseudoI<(outs), (ins GR64:$src),
+                          [(int_x86_checkload addr:$src)]>; 
+  def checkstore32r: PseudoI<(outs), (ins GR64:$src), 
+                          [(int_x86_checkstore addr:$src)]>; 
+}
+
+
 let SchedRW = [WriteSystem] in {
 
 multiclass mpx_bound_make<bits<8> opc, string OpcodeStr> {
diff --git a/lib/Target/X86/X86MDSFIControlGuard.cpp b/lib/Target/X86/X86MDSFIControlGuard.cpp
new file mode 100644
index 0000000..55eeadd
--- /dev/null
+++ b/lib/Target/X86/X86MDSFIControlGuard.cpp
@@ -0,0 +1,531 @@
+
+
+#include "X86.h"
+#include "X86InstrBuilder.h"
+#include "X86InstrInfo.h"
+#include "X86Subtarget.h"
+
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/CodeGen/RegisterScavenging.h"
+
+using namespace llvm;
+
+namespace llvm {
+void initializeX86MDSFIControlGuardPass(PassRegistry &);
+}
+
+#define DEBUG_TYPE "MDSFICG"
+
+static cl::opt<bool> enableX86MDSFICG("enable-x86-mdsficg", cl::init(true),
+                                      cl::Hidden,
+                                      cl::desc("Enable X86 cfi instrument."));
+static cl::opt<bool>
+    enableX86FSRelocate("enable-x86-fs-relocate", cl::init(false), cl::Hidden,
+                        cl::desc("Enable X86 relocate with FS."));
+
+namespace {
+class X86MDSFIControlGuard : public MachineFunctionPass {
+public:
+  static char ID;
+  X86MDSFIControlGuard() : MachineFunctionPass(ID) {
+    initializeX86MDSFIControlGuardPass(*PassRegistry::getPassRegistry());
+  }
+
+  bool runOnMachineFunction(MachineFunction &Fn) override;
+
+  const X86Subtarget *STI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+  std::unique_ptr<RegScavenger> RS;
+
+  /// Handle all Calls, invoke all indirect jumps
+  bool HandleCalls(MachineBasicBlock &MBB, MachineInstr &MI);
+
+  /// This function will replace one return instruction with a pop, CFI check
+  /// and an indirect jump
+  bool HandleRets(MachineBasicBlock &MBB, MachineInstr &MI);
+  bool HandleRet(MachineBasicBlock &MBB, MachineInstr &MI);
+
+  /// Handle all indirect jumps
+  bool HandleIndirectBrs(MachineBasicBlock &MBB, MachineInstr &MI);
+
+  bool HandleRegInst(MachineBasicBlock &MBB, MachineInstr &MI, unsigned LabelReg);
+  bool HandleMemInst(MachineBasicBlock &MBB, MachineInstr &MI, unsigned Opcode, unsigned TargetReg, unsigned LabelReg);
+
+  /// Insert CFI label before every function. If hasIndirectJump is true, then
+  /// insert CFI labels before every basicblock either.
+  bool InsertCFILabels(MachineFunction &Fn, bool hasIndirectJump);
+  bool InsertOneCFILabel(MachineInstr &MI);
+  bool InsertOneCFILabel(MachineInstr &MI, bool InsertAfter);
+  bool isCFILabel(MachineInstr & MI);
+
+  bool RelocatePIC(MachineFunction &Fn);
+  bool ds2fs(MachineBasicBlock &MBB, MachineBasicBlock::iterator MBBI,
+             unsigned indexofRIP);
+
+  bool CFIInstrument(MachineFunction &Fn);
+};
+} // namespace
+char X86MDSFIControlGuard::ID = 0;
+
+bool X86MDSFIControlGuard::HandleRets(MachineBasicBlock &MBB,
+                                      MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled return instruction\t" << MI);
+    return false;
+    break;
+  case X86::RET:
+  case X86::RETL:
+  case X86::RETQ:
+    HandleRet(MBB, MI);
+    break;
+  case X86::TAILJMPr64:
+    HandleRegInst(MBB, MI, X86::R10);
+    break;
+  case X86::TAILJMPm64:
+    HandleMemInst(MBB, MI, X86::TAILJMPr64,X86::R10, X86::R11);
+    break;
+    // jmp with imm target, no need to handle
+  case X86::TAILJMPd64:
+  case X86::TAILJMPd_CC:
+    break;
+  }
+  return true;
+}
+
+bool X86MDSFIControlGuard::HandleRet(MachineBasicBlock &MBB, MachineInstr &MI) {
+  const DebugLoc &DL = MI.getDebugLoc();
+  // popq %r11
+  BuildMI(MBB, MI, DL, TII->get(X86::POP64r), X86::R11);
+  // movq (%r11), %r10
+  BuildMI(MBB, MI, DL, TII->get(X86::MOV64rm))
+      .addReg(X86::R10)
+      .addReg(X86::R11)
+      .addImm(1)
+      .addReg(0)
+      .addImm(0)
+      .addReg(0);
+  // bndcu %r10, %bnd2
+  // bndcl %r10, %bnd2
+  BuildMI(MBB, MI, DL, TII->get(X86::BNDCU64rr), X86::BND2).addReg(X86::R10);
+  BuildMI(MBB, MI, DL, TII->get(X86::BNDCL64rr), X86::BND2).addReg(X86::R10);
+
+  // jmpq *%r11
+  BuildMI(MBB, MI, DL, TII->get(X86::JMP64r)).addReg(X86::R11);
+  // remove ret from code;
+  MI.eraseFromParent();
+  return true;
+}
+/// opcode is the replacement of the original instruction
+bool X86MDSFIControlGuard::HandleMemInst(MachineBasicBlock &MBB,
+                                         MachineInstr &MI, unsigned opcode, 
+                                         unsigned TargetReg, unsigned LabelReg) {
+  const DebugLoc &DL = MI.getDebugLoc();
+  LLVM_DEBUG(dbgs() << "HandleMemInst\t" << MI);
+  // We might need two register, one for target address, the other for CFI label.
+  unsigned MemReg;
+
+  // Does TargetReg need spill? Does LabelReg need spill?
+  bool spillTarget = false, spillLabel = false;
+
+  X86AddressMode AM;
+  // Which physic register is used in this Memory address?
+  AM = getAddressFromInstr(&MI, 0);
+  MemReg = AM.BaseType == X86AddressMode::RegBase ? AM.Base.Reg : AM.IndexReg;
+
+  // We didn't specific TargetReg
+  if(TargetReg == 0) {
+
+    // if MemReg is a general purpose register, we use MemReg as TargetReg 
+    // Note that GR64 class include RIP and RSP, and we have to exclude it
+    if(!X86::GR64RegClass.contains(MemReg) && MemReg!= X86::RSP && MemReg != X86::RIP){
+      TargetReg = MemReg;
+    } else {
+      // otherwise if MemReg is RSP or RIP, we have to get another register
+      RegScavenger *RegSc = RS.get();
+      RegSc->enterBasicBlockEnd(MBB);
+      TargetReg = RegSc->scavengeRegisterBackwards(
+          X86::GR64RegClass, MachineBasicBlock::iterator(MI), false, 0);
+      if (TargetReg == 0) {
+        LLVM_DEBUG(dbgs() << "Can not find free register for indirect branch\t"
+                          << MI);
+        // Since MemReg is not a GR, we use R10 directly.
+        TargetReg = X86::R10;
+        LLVM_DEBUG(dbgs() << "spill one for TargetReg\t"<<TRI->getRegAsmName(TargetReg)<<"\n");
+        spillTarget = true;
+      }
+    }
+  }
+
+  if(LabelReg == 0){
+    RegScavenger *RegSc = RS.get();
+    RegSc->enterBasicBlockEnd(MBB);
+    LabelReg = RegSc->scavengeRegisterBackwards(
+        X86::GR64RegClass, MachineBasicBlock::iterator(MI), false, 0);
+    if (LabelReg == 0) {
+      LLVM_DEBUG(dbgs() << "Can not find free register for indirect branch\t"
+                        << MI);
+      // If TargetReg already token R10, we use R11
+      LabelReg = TargetReg == X86::R10 ? X86::R11 : X86::R10;
+      LLVM_DEBUG(dbgs() << "spill one for LabelReg \t"<<TRI->getRegAsmName(LabelReg)<<"\n");
+      spillLabel = true;
+    }
+  }
+
+  LLVM_DEBUG(dbgs() << "TargetReg for memory based Instruction: "
+                    << TRI->getRegAsmName(TargetReg) << "\n");
+  LLVM_DEBUG(dbgs() << "LabelReg for memory based Instruction: "
+                    << TRI->getRegAsmName(LabelReg) << "\n");
+
+  if(spillTarget)
+    BuildMI(MBB, MI, DL, TII->get(X86::PUSH64r), TargetReg);
+  if(spillLabel)
+    BuildMI(MBB, MI, DL, TII->get(X86::PUSH64r), LabelReg);
+
+  // movq (x), x
+  addFullAddress(BuildMI(MBB, MI, DL, TII->get(X86::MOV64rm), TargetReg), AM);
+
+  // movq (%TargetReg) , %FreeReg; load cfi lable
+  addDirectMem(BuildMI(MBB, MI, DL, TII->get(X86::MOV64rm), LabelReg),
+               TargetReg);
+  // bndck %FreeReg
+  BuildMI(MBB, MI, DL, TII->get(X86::BNDCU64rr), X86::BND2).addReg(LabelReg);
+  BuildMI(MBB, MI, DL, TII->get(X86::BNDCL64rr), X86::BND2).addReg(LabelReg);
+  if(spillTarget)
+    BuildMI(MBB, MI, DL, TII->get(X86::POP64r), TargetReg);
+  if(spillLabel)
+    BuildMI(MBB, MI, DL, TII->get(X86::POP64r), LabelReg);
+
+  BuildMI(MBB, MI, DL, TII->get(opcode), TargetReg);
+  MI.eraseFromParent();
+  return true;
+}
+
+bool X86MDSFIControlGuard::HandleRegInst(MachineBasicBlock &MBB,
+                                         MachineInstr &MI, unsigned LabelReg) {
+  const DebugLoc &DL = MI.getDebugLoc();
+  LLVM_DEBUG(dbgs() << "HandleRegInst\t" << MI);
+  unsigned TargetReg;
+  bool spillLabel = false;
+  TargetReg = MI.getOperand(0).getReg();
+  if(LabelReg == 0){
+    RegScavenger *RegSc = RS.get();
+    RegSc->enterBasicBlockEnd(MBB);
+    LabelReg = RegSc->scavengeRegisterBackwards(
+        X86::GR64RegClass, MachineBasicBlock::iterator(MI), false, 0);
+    if (LabelReg == 0) {
+      LLVM_DEBUG(dbgs() << "Can not find free register for indirect branch\t"
+                        << MI);
+      LabelReg = TargetReg == X86::R10 ? X86::R11 : X86::R10;
+      LLVM_DEBUG(dbgs() << "spill one for LabelReg\t"<<TRI->getRegAsmName(LabelReg)<<"\n");
+      spillLabel = true;
+    }
+  }
+
+  LLVM_DEBUG(dbgs() << "LabelReg for register based instruction: "
+                    << TRI->getRegAsmName(LabelReg) << "\n");
+  if(spillLabel)
+    BuildMI(MBB, MI, DL, TII->get(X86::PUSH64r), LabelReg);
+  // movq (%TargetReg) , %FreeReg load cfi lable
+  addDirectMem(BuildMI(MBB, MI, DL, TII->get(X86::MOV64rm), LabelReg),
+               TargetReg);
+  BuildMI(MBB, MI, DL, TII->get(X86::BNDCU64rr), X86::BND2).addReg(LabelReg);
+  BuildMI(MBB, MI, DL, TII->get(X86::BNDCL64rr), X86::BND2).addReg(LabelReg);
+  if(spillLabel)
+    BuildMI(MBB, MI, DL, TII->get(X86::POP64r), LabelReg);
+  return true;
+}
+
+
+bool X86MDSFIControlGuard::HandleIndirectBrs(MachineBasicBlock &MBB,
+                                             MachineInstr &MI) {
+  switch (MI.getOpcode()) {
+  default:
+    LLVM_DEBUG(dbgs() << "Unhandled indirect branch instruction\t" << MI
+                      << "\n");
+    return false;
+    break;
+  case X86::JMP16r:
+  case X86::JMP32r:
+  case X86::JMP64r:
+    HandleRegInst(MBB, MI, 0);
+    break;
+  case X86::JMP16m:
+  case X86::JMP32m:
+  case X86::JMP64m:
+    HandleMemInst(MBB, MI, X86::JMP64r, 0, 0);
+    break;
+  }
+  return true;
+}
+bool X86MDSFIControlGuard::isCFILabel(MachineInstr & MI) {
+  if( MI.getOpcode() != X86::NOOPL || MI.getNumOperands()!= 5){
+    return false;
+  }
+  MachineOperand MO = MI.getOperand(0);
+  if(!MO.isReg() || MO.getReg() != X86::RBX){
+    return false;
+  }
+  MO = MI.getOperand(1);
+  if(!MO.isImm() || MO.getImm() != 1){
+    return false;
+  }
+  MO = MI.getOperand(2);
+  if(!MO.isReg() || MO.getReg() != X86::RBX){
+    return false;
+  }
+  MO = MI.getOperand(3);
+  if(!MO.isImm() || MO.getImm() != 512){
+    return false;
+  }
+  MO = MI.getOperand(4);
+  if(!MO.isImm() || MO.getImm() != 0){
+    return false;
+  }
+  return true;
+}
+
+//insert one CFI label before MI
+bool X86MDSFIControlGuard::InsertOneCFILabel(MachineInstr &MI){
+  const DebugLoc &DL = MI.getDebugLoc();
+  MachineBasicBlock &MBB = *MI.getParent();
+
+  if(isCFILabel(MI))
+    return false;
+  BuildMI(MBB, MI, DL, TII->get(X86::NOOPL))
+      .addReg(X86::RBX)
+      .addImm(1)
+      .addReg(X86::RBX)
+      .addImm(512)
+      .addReg(0);
+  return true;
+}
+
+bool X86MDSFIControlGuard::InsertOneCFILabel(MachineInstr &MI, bool InsertAfter){
+  const DebugLoc &DL = MI.getDebugLoc();
+  MachineBasicBlock &MBB = *MI.getParent();
+
+  if(isCFILabel(MI))
+    return false;
+  MachineInstr *LabelMI = BuildMI(MBB, MI, DL, TII->get(X86::NOOPL))
+      .addReg(X86::RBX)
+      .addImm(1)
+      .addReg(X86::RBX)
+      .addImm(512)
+      .addReg(0);
+  if(InsertAfter){
+    MachineBasicBlock::iterator pos = MI;
+    MBB.insertAfter(pos, LabelMI->removeFromParent());
+  }
+  return true;
+}
+  
+
+bool X86MDSFIControlGuard::InsertCFILabels(MachineFunction &Fn,
+                                          bool hasIndirectJump) {
+  MachineFunction::iterator FirstMBBI = Fn.begin();
+
+  // sometimes the first basicblock of function might be empty
+  // handle this by step to next basic block;
+  // this is because some function do not need to save frame
+  for (; FirstMBBI->empty();) {
+    auto NMBBI = std::next(FirstMBBI);
+    if (FirstMBBI == Fn.end()) {
+      // can't find non-empty BB, exist function
+      LLVM_DEBUG(dbgs() << "Error: no non-empty basic block at function "
+                        << Fn.getName());
+      return false;
+    }
+    FirstMBBI = NMBBI;
+  }
+  // now the firstMBB is the first non-empty BB at Fn
+  // insert CFI_LABEL before first instruction at the function
+  auto &FirstMBB = *FirstMBBI;
+  auto &FirstMI = *FirstMBB.getFirstNonPHI();
+  /* LLVM_DEBUG(dbgs() <<"First MI : " << FirstMI); */
+  InsertOneCFILabel(FirstMI);
+
+  // scan every instruction, insert CFI_LABEL after call
+  for (auto &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+
+    MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+    while (MBBI != E) {
+      MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+      MachineInstr &MI = *MBBI;
+
+      if(MI.isCall() && !MI.isReturn()){
+        InsertOneCFILabel(MI, true);
+      }
+      MBBI = NMBBI;
+    }
+  }
+  // if there is no direct jump in the function, then do not insert cfi_label
+  // before every bb just return;
+  if (!hasIndirectJump)
+    return true;
+  // insert CFI_LABEL before every BB.
+  // Because there are some indirect jump inside this funciton, it might
+  // want to jump to any of BB at this function
+  // We omit first basicblock because it's already have one CFI_LABEL
+  for (MachineFunction::iterator MBBI = ++Fn.begin(); MBBI != Fn.end();
+       MBBI++) {
+    MachineBasicBlock &MBB = *MBBI;
+    if (MBB.empty())
+      continue;
+    MachineBasicBlock::instr_iterator I = MBB.instr_begin();
+    MachineInstr &MI = *I;
+    InsertOneCFILabel(MI);
+  }
+  return true;
+}
+
+bool X86MDSFIControlGuard::runOnMachineFunction(MachineFunction &Fn) {
+  STI = &static_cast<const X86Subtarget &>(Fn.getSubtarget());
+  TRI = STI->getRegisterInfo();
+  TII = STI->getInstrInfo();
+  RS.reset(new RegScavenger());
+
+  if (Fn.getName().startswith("_boundchecker_"))
+    return false;
+  if (!enableX86MDSFICG) {
+    return false;
+  }
+
+  bool hasIndirectJump = false;
+  hasIndirectJump = CFIInstrument(Fn);
+  InsertCFILabels(Fn, hasIndirectJump);
+
+  // disable if for SPEC
+  if (enableX86FSRelocate)
+    RelocatePIC(Fn);
+
+  return true;
+}
+
+bool X86MDSFIControlGuard::ds2fs(MachineBasicBlock &MBB,
+                                 MachineBasicBlock::iterator MBBI,
+                                 unsigned indexofRIP) {
+  switch (MBBI->getOpcode()) {
+  case X86::LEA64r:
+  case X86::LEA64_32r:
+  case X86::LEA32r:
+  case X86::LEA16r: {
+
+    const DebugLoc &DL = MBBI->getDebugLoc();
+    auto NMBBI = std::next(MBBI);
+    MachineOperand &dest = MBBI->getOperand(0);
+
+    assert(dest.isReg() && "LEA dest is not register");
+
+    unsigned destReg = dest.getReg();
+    unsigned victimReg = destReg == X86::R10 ? X86::R11 : X86::R10;
+
+    BuildMI(MBB, MBBI, DL, TII->get(X86::PUSH64r), victimReg);
+    if (X86::GR64RegClass.contains(destReg))
+      BuildMI(MBB, MBBI, DL, TII->get(X86::RDFSBASE64), victimReg);
+    else if (X86::GR32RegClass.contains(destReg))
+      BuildMI(MBB, MBBI, DL, TII->get(X86::RDFSBASE), victimReg);
+    else
+      assert(false && "RDFSBASE to too small register");
+
+    addRegReg(BuildMI(MBB, NMBBI, DL, TII->get(X86::LEA64r), destReg),
+              victimReg, false, destReg, true);
+    BuildMI(MBB, NMBBI, DL, TII->get(X86::POP64r), victimReg);
+
+    return true;
+  } break;
+  }
+  unsigned segregidx = indexofRIP + 4;
+  MachineOperand &MO = MBBI->getOperand(segregidx);
+  assert(segregidx < MBBI->getNumOperands() && "Out of operands bounds");
+  assert(MO.getReg() == 0 && "Segments of RIP address  is zero");
+  MO.setReg(X86::FS);
+  return true;
+}
+
+bool X86MDSFIControlGuard::RelocatePIC(MachineFunction &Fn) {
+  for (auto &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+
+    MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+    while (MBBI != E) {
+      MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+
+      for (unsigned i = 0; i < MBBI->getNumOperands(); i++) {
+        MachineOperand &MO = MBBI->getOperand(i);
+        if (MO.isReg() && MO.getReg() == X86::RIP) {
+          ds2fs(MBB, MBBI, i);
+          // jump out for loop to next instruction
+          break;
+        }
+      }
+      MBBI = NMBBI;
+    }
+  }
+  return true;
+}
+
+bool X86MDSFIControlGuard::HandleCalls(MachineBasicBlock &MBB,
+                                       MachineInstr &MI) {
+  bool changed = false;
+  switch (MI.getOpcode()) {
+  default:
+    /* LLVM_DEBUG(dbgs() <<"Unhandled call instruction\t"<<MI<<"\n"); */
+    break;
+  case X86::CALL16r:
+  case X86::CALL32r:
+  case X86::CALL64r:
+    changed = HandleRegInst(MBB, MI, X86::R10);
+    break;
+  case X86::CALL16m:
+  case X86::CALL32m:
+  case X86::CALL64m:
+    changed = HandleMemInst(MBB, MI, X86::CALL64r, X86::R10, X86::R11);
+    break;
+  }
+  return changed;
+}
+
+bool X86MDSFIControlGuard::CFIInstrument(MachineFunction &Fn) {
+  bool hasIndirectJump = false;
+  for (auto &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+
+    MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+    while (MBBI != E) {
+      MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+
+      MachineInstr &MI = *MBBI;
+      MBBI = NMBBI;
+      if (MI.isReturn()) {
+        HandleRets(MBB, MI);
+      } else if (MI.isIndirectBranch()) {
+        hasIndirectJump = true;
+        HandleIndirectBrs(MBB, MI);
+      } else if (MI.isCall()) {
+        // note some pseudo which is both return and call will not reach here
+        HandleCalls(MBB, MI);
+      }
+    }
+  }
+  return hasIndirectJump;
+}
+#undef DEBUG_TYPE
+
+INITIALIZE_PASS(X86MDSFIControlGuard, "mdsfi-cg-instrument", "CFI Instrument",
+                false, false)
+
+FunctionPass *llvm::createX86MDSFIControlGuard() {
+  return new X86MDSFIControlGuard();
+}
diff --git a/lib/Target/X86/X86MDSFIDataGuard.cpp b/lib/Target/X86/X86MDSFIDataGuard.cpp
new file mode 100644
index 0000000..f09c823
--- /dev/null
+++ b/lib/Target/X86/X86MDSFIDataGuard.cpp
@@ -0,0 +1,779 @@
+#include "X86.h"
+#include "X86InstrBuilder.h"
+#include "X86InstrInfo.h"
+#include "X86RegValueTracking.h"
+#include "X86Subtarget.h"
+
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/Analysis/LoopIterator.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineFunctionPass.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/MachineLoopInfo.h"
+
+#include "llvm/CodeGen/MachineInstrBuilder.h"
+#include "llvm/CodeGen/Passes.h"
+#include "llvm/Support/Debug.h"
+
+#include "llvm/Analysis/LoopIterator.h"
+
+using namespace llvm;
+
+static cl::opt<bool>
+    enableX86MDSFIDGOpt("enable-x86-mdsfidg-opt", cl::init(true), cl::Hidden,
+                        cl::desc("Enable X86 MDSFI data guard optimization."));
+
+static cl::opt<bool>
+    NoSFI("disable-SFI", cl::desc("do not check memory operations"), cl::Hidden);
+
+static cl::opt<bool> enableX86MDSFIDGLoopOpt(
+    "enable-x86-mdsfidg-loop-opt", cl::init(true), cl::Hidden,
+    cl::desc("Enable X86 MDSFI data guard loop optimization."));
+
+static cl::opt<bool> enableX86MDSFIDG("enable-x86-mdsfidg", cl::init(true),
+                                      cl::Hidden,
+                                      cl::desc("Enable X86 MDSFI data guard."));
+
+static cl::opt<bool> CheckStoreOnly("check-store-only", cl::init(false),
+                                     cl::desc("only check store operation"),
+                                     cl::Hidden);
+
+namespace llvm {
+void initializeX86MDSFIDataGuardPass(PassRegistry &);
+}
+
+namespace {
+
+class X86MDSFIDataGuard : public MachineFunctionPass {
+public:
+  static char ID;
+  const X86Subtarget *STI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+  /* std::vector<const MachineBasicBlock *>RPOTBlocks; */
+  /* using rpot_iterator = MachineBasicBlock* std::vector<const
+   * MachineBasicBlock*>::const_iterator; */
+  /* rpot_iterator rpot_begin() const {return RPOTBlocks.begin();} */
+  /* rpot_iterator rpot_end() const {return RPOTBlocks.end();} */
+
+  X86RegValueTracking RT;
+  std::map<MachineInstr *, bool> EliminableList;
+
+  MachineLoopInfo *MLI;
+  X86MDSFIDataGuard() : MachineFunctionPass(ID) {
+    initializeX86MDSFIDataGuardPass(*PassRegistry::getPassRegistry());
+  }
+
+  bool runOnMachineFunction(MachineFunction &Fn) override;
+
+  // This function scan the whole machine function and insert proper 
+  // data guards for each instruction that may load or store.
+  bool InsertDataGuards(MachineFunction &Fn) ;
+
+  // This function take AM as the target address and build one data guard
+  // isLoad means a guard for load or for store
+  // if check store only is used, only insert store guards
+  bool insertOneDG(MachineInstr &MI, X86AddressMode &AM, bool isLoad);
+
+  // This function take AM as the target address and build one data guard
+  // isLoad means a guard for load or for store
+  // if check store only is used, only insert store guards
+  bool insertOneDG(MachineInstr &MI, unsigned BaseReg, bool isLoad);
+  
+  // handle a instruction that may load or may store. I.e. this instruction 
+  // may load but won't store or vice verse.
+  bool handleLoadOrStore(MachineInstr &MI, bool isLoad);
+
+  // Handle instruction store and load 
+  bool handleLoadAndStore(MachineInstr &MI);
+
+  bool OptimizeCheck(MachineFunction &Fn, bool canOpt);
+  bool OptimizeLoop(MachineFunction &Fn);
+  bool OptimizeMBB(MachineBasicBlock &MBB, X86RegValueTracking &RT);
+  bool LoweringMI(MachineFunction &Fn);
+  bool getX86AddressFromInstr(const MachineInstr &MI, X86AddressMode &AM);
+  void getAnalysisUsage(AnalysisUsage &AU) const override;
+  bool isLoopBody(MachineBasicBlock *MBB, MachineLoop *L);
+
+  bool checkRSP(MachineFunction &Fn);
+  bool computeLoop(MachineLoop *ML, MachineLoopInfo *MLI,
+                   X86RegValueTracking &UpLevelRT);
+  bool hoist(MachineLoop *ML, MachineLoopInfo *MLI, X86RegValueTracking &RT);
+  void LoopHoist();
+
+  bool hasIndirectCall(MachineFunction &Fn);
+  unsigned GuardZoneSize = 4 * 1024;
+};
+char X86MDSFIDataGuard::ID = 0;
+} // namespace
+
+#define DEBUG_TYPE "MDSFIDG"
+
+void X86MDSFIDataGuard::getAnalysisUsage(AnalysisUsage &AU) const {
+  MachineFunctionPass::getAnalysisUsage(AU);
+  AU.addRequired<MachineLoopInfo>();
+}
+
+bool X86MDSFIDataGuard::hasIndirectCall(MachineFunction &Fn) {
+  for (auto &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+
+    for (MachineBasicBlock::instr_iterator I = MBB.instr_begin();
+         I != MBB.instr_end(); I++) {
+      MachineInstr &MI = *I;
+      switch (MI.getOpcode()) {
+      default:
+        break;
+      case X86::JMP16m:
+      case X86::JMP32m:
+      case X86::JMP64m:
+        return true;
+      }
+    }
+  }
+  return false;
+}
+
+// if changed RSP ,should check RSP after that instructions
+bool X86MDSFIDataGuard::checkRSP(MachineFunction &Fn) {
+  bool ret = false;
+  for (auto &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+    MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+
+    while (MBBI != E) {
+      MachineInstr *MI = &*MBBI;
+      DebugLoc DL = MI->getDebugLoc();
+      MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+
+      for (auto &MO : MI->defs()) {
+        if (MO.isReg() && MO.getReg() == X86::RSP) {
+          MachineInstr *NewMI = addDirectMem(
+              BuildMI(Fn, DL, TII->get(X86::checkstore64m)), X86::RSP);
+          MBB.insertAfter(MBBI, NewMI);
+          /* LLVM_DEBUG(dbgs() << "Insert a check of RSP\n" << *MI << "\n"); */
+          /* errs() << "Insert a check of RSP\n" << *MI << "\n"; */
+          ret = true;
+        }
+      }
+      MBBI = NMBBI;
+    }
+  }
+  return ret;
+}
+
+// This function will extract Address mode from an instruction
+// If there is no memory operands in MI, return false, otherwise return true
+bool X86MDSFIDataGuard::getX86AddressFromInstr(const MachineInstr &MI, X86AddressMode &AM){
+  const MCInstrDesc &Desc = MI.getDesc();
+  int MemRefBegin = X86II::getMemoryOperandNo(Desc.TSFlags);
+  if(MemRefBegin < 0)
+    return false;
+  MemRefBegin += X86II::getOperandBias(Desc);
+  AM = getAddressFromInstr(&MI, MemRefBegin);
+  return true;
+}
+
+bool X86MDSFIDataGuard::insertOneDG(MachineInstr &MI, X86AddressMode &AM, bool isLoad){
+  MachineBasicBlock &MBB= *MI.getParent();
+  const DebugLoc &DL = MI.getDebugLoc();
+  if(isLoad == true ) {
+    if(CheckStoreOnly)
+      return false;
+    addFullAddress(BuildMI(MBB, MI, DL, TII->get(X86::checkload64m)), AM);
+  } else {
+    addFullAddress(BuildMI(MBB, MI, DL, TII->get(X86::checkstore64m)), AM);
+  }
+  return true;
+}
+
+bool X86MDSFIDataGuard::insertOneDG(MachineInstr &MI, unsigned BaseReg, bool isLoad){
+  MachineBasicBlock &MBB= *MI.getParent();
+  const DebugLoc &DL = MI.getDebugLoc();
+  if(isLoad == true) {
+    if(CheckStoreOnly)
+      return false;
+    addDirectMem(BuildMI(MBB, MI, DL, TII->get(X86::checkload64m)), BaseReg);
+  } else {
+    addDirectMem(BuildMI(MBB, MI, DL, TII->get(X86::checkstore64m)), BaseReg);
+  }
+  return true;
+}
+
+// This function tackle a instruction only load or only store.
+bool X86MDSFIDataGuard::handleLoadOrStore(MachineInstr &MI, bool isLoad) {
+  const MCInstrDesc &MCID = MI.getDesc();
+  unsigned NumOfPotentialPointer = 0;
+  // instructions only store
+  X86AddressMode AM;
+  if(getX86AddressFromInstr(MI, AM) == true) {
+    //instructions that mayStore or mayLoad and have one memory operand
+    insertOneDG(MI, AM, isLoad);
+    NumOfPotentialPointer++;
+  }
+  const MCPhysReg *ImplicitUses = MCID.getImplicitUses();
+  // If it has implicit uses, we scan the implicit uses list and check if there
+  // are any potential pointer.
+  if(ImplicitUses){
+    for (unsigned i = 0; ImplicitUses[i]; ++i){
+      if(TRI->getPointerRegClass(*MI.getMF())->contains(ImplicitUses[i])){
+        if(ImplicitUses[i] == X86::RSP){
+          // we don't care RSP, just check it and omit it as a potential pointer
+          // Since we assumed that the RSP is used to access variable on stack, so we 
+          // do not treat it as an potential pointer.
+          insertOneDG(MI, X86::RSP, isLoad);
+        } else
+          NumOfPotentialPointer++;
+      }
+    }
+  }
+  // If this instruciton have use two potential pointers(we omit RSP)
+  // then there must be one implicitly used pointer.
+  // Since we don't konw whether should we check the implicitly used 
+  // register (maybe it's just an normal register but not a pointer)
+  // So we have to handle this kind of instructions ad-hoc(one by one).
+  // Every time the compiler encounter an instruction potentially implicitly 
+  // access memory but we can't handle, add it here like DIVs.
+  if(NumOfPotentialPointer > 1) {
+    switch(MI.getOpcode()){
+      default:
+        errs() << "mayStore or mayLoad: " <<MI;
+        break;
+        // mayLoad instructions we handle here
+        // mayStore instructions we handle here
+      case X86::DIV8m:
+      case X86::IDIV8m:
+      case X86::DIV16m:
+      case X86::IDIV16m:
+      case X86::DIV32m:
+      case X86::IDIV32m:
+      case X86::DIV64m:
+      case X86::IDIV64m:
+        insertOneDG(MI, AM, true);
+        break;
+    }
+  }
+  return true;
+}
+
+bool X86MDSFIDataGuard::handleLoadAndStore(MachineInstr &MI) {
+  // Even if the instruction may load and store at the same time,
+  // if it only have one potential pointer, we can check it
+  const MCInstrDesc &MCID = MI.getDesc();
+  unsigned NumOfPotentialPointer = 0, ImplicitReg = 0;
+  bool haveMemOp = false;
+   
+  X86AddressMode AM;
+  if(getX86AddressFromInstr(MI, AM) == true) {
+    haveMemOp = true;
+    NumOfPotentialPointer++;
+  }
+  const MCPhysReg *ImplicitUses = MCID.getImplicitUses();
+  // If it has implicit uses, we scan the implicit uses list and check if there
+  // are any potential pointer.
+  if(ImplicitUses){
+    for (unsigned i = 0; ImplicitUses[i]; ++i){
+      if(TRI->getPointerRegClass(*MI.getMF())->contains(ImplicitUses[i])){
+          NumOfPotentialPointer++;
+          ImplicitReg = ImplicitUses[i];
+      }
+    }
+  }
+
+  // Every time the compiler encounter an instruction potentially implicitly 
+  // access memory but we can't handle, add it here.
+  if(NumOfPotentialPointer == 1){
+    if(haveMemOp){
+      insertOneDG(MI, AM, false);
+      insertOneDG(MI, AM, true);
+    } else  {
+      insertOneDG(MI, ImplicitReg, false);
+      insertOneDG(MI, ImplicitReg, true);
+    }
+    return true;
+  }
+  
+  // If one instruction Both Load and Store and have more than one pointer, 
+  // we don't know which pointer is used for store and which for load.
+  // Hence we have to handle it ad-hoc:
+  // That is, once more instructions is reported, we handle it.
+  // Notice that we can't trusted the report. Here we are trying to construct 
+  // a benign compiler. This report code is just help us. we still may not 
+  // cover all instrutions.
+  switch(MI.getOpcode()) {
+    default:
+      errs() << "mayStore and mayLoad: " <<MI;
+      break;
+    case X86::checkstore64m:
+    case X86::checkload64m:
+      break;
+    case X86::PUSH64rmm:
+      // push read from AM and push into stack
+      insertOneDG(MI, AM, true);
+      insertOneDG(MI, X86::RSP, false);
+      break;
+    case X86::POP64rmm:
+      // pop read from stack and write to AM
+      insertOneDG(MI, AM, false);
+      insertOneDG(MI, X86::RSP, true);
+      break;
+  }
+  return true;
+}
+
+bool X86MDSFIDataGuard::InsertDataGuards(MachineFunction &Fn) {
+  for (MachineBasicBlock &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+    for(auto &MI: MBB) {
+      // insert store guards
+      if(!MI.mayLoadOrStore()) {
+        // instructions won't access memory according to it's 
+        // attribute mayLoad and may Store. For in case, we dump the instruction.
+        LLVM_DEBUG(dbgs()<< "MI won't access memory" <<MI);
+      } else if(MI.mayStore() && !MI.mayLoad()) {
+        handleLoadOrStore(MI, false);
+      } else if(MI.mayLoad() && !MI.mayStore()) {
+        // instructions only load
+        handleLoadOrStore(MI, true);
+      } else if(MI.mayLoad() && MI.mayStore() ) {
+        handleLoadAndStore(MI);
+      }
+    } //end of MBB
+  } // end of Fn
+  return true;
+}
+
+bool X86MDSFIDataGuard::runOnMachineFunction(MachineFunction &Fn) {
+  if(NoSFI)
+    return false;
+  STI = &static_cast<const X86Subtarget &>(Fn.getSubtarget());
+  TII = STI->getInstrInfo();
+  TRI = STI->getRegisterInfo();
+  MLI = &getAnalysis<MachineLoopInfo>();
+  InsertDataGuards(Fn);
+
+  // if there are any indirectCall, then we can not optimize loops
+  // in this function because any program may jump before any basicblock
+  bool canOptimize = !hasIndirectCall(Fn);
+
+  enableX86MDSFIDGLoopOpt = enableX86MDSFIDGOpt && enableX86MDSFIDGLoopOpt;
+
+  // compute for LoopHoist
+  if (canOptimize && enableX86MDSFIDGOpt) {
+    checkRSP(Fn);
+    RT.init(Fn);
+    RT.computeRange(Fn);
+  }
+
+  if (canOptimize && enableX86MDSFIDGLoopOpt) {
+    LoopHoist();
+  }
+
+  // after loophoist, compute it again
+  if (canOptimize && enableX86MDSFIDGOpt) {
+    RT.init(Fn);
+    RT.computeRange(Fn);
+  }
+
+  if (enableX86MDSFIDGOpt) {
+    OptimizeCheck(Fn, canOptimize);
+  }
+  LoweringMI(Fn);
+  return true;
+}
+
+bool X86MDSFIDataGuard::OptimizeMBB(MachineBasicBlock &MBB,
+                                    X86RegValueTracking &RT) {
+  RangeInfo ReadRegion(InRead, GuardZoneSize, -GuardZoneSize);
+  RangeInfo WriteRegion(InWrite, GuardZoneSize, -GuardZoneSize);
+  // copy Range in BasicblockRanges
+  RegsRange Ranges = RT.InRanges.at(&MBB);
+
+  LLVM_DEBUG(dbgs() << "In Range for: ");
+  LLVM_DEBUG(MBB.printAsOperand(dbgs(), false));
+  LLVM_DEBUG(dbgs() << "\n" << Ranges);
+  LLVM_DEBUG(dbgs() << "Out Range for: ");
+  LLVM_DEBUG(MBB.printAsOperand(dbgs(), false));
+  LLVM_DEBUG(dbgs() << "\n" << RT.OutRanges.at(&MBB));
+  LLVM_DEBUG(MBB.printAsOperand(dbgs(), false));
+  LLVM_DEBUG(dbgs() << " Has Predesuccess: ");
+
+  for (auto &it : MBB.predecessors()) {
+    LLVM_DEBUG(it->printAsOperand(dbgs(), false));
+    LLVM_DEBUG(dbgs() << ",");
+  }
+  LLVM_DEBUG(dbgs() << "\n\n");
+
+  MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+  while (MBBI != E) {
+    MachineInstr *MI = &*MBBI;
+
+    MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+
+    switch (MI->getOpcode()) {
+    case X86::checkstore64m: {
+      /* LLVM_DEBUG(dbgs() << "guard: " << *MI); */
+      X86AddressMode AM = getAddressFromInstr(MI, 0);
+      EliminableList[MI] = RT.isInRange(AM, WriteRegion, Ranges);
+    } break;
+    case X86::checkload64m: {
+      /* LLVM_DEBUG(dbgs() << "guard: " << *MI); */
+      X86AddressMode AM = getAddressFromInstr(MI, 0);
+      EliminableList[MI] = RT.isInRange(AM, ReadRegion, Ranges);
+    } break;
+    default:
+      break;
+    }
+
+    // If eliminable, we will delete this guard instruction.
+    // However, we can still use step function here
+    // because eliminable means MI is in Ranges.
+    // So this instruction can't bring any information, InferenceCheck in step
+    // won't have any side effect
+    RT.step(MI, Ranges);
+    MBBI = NMBBI;
+  }
+  return true;
+}
+
+// OptimizeCheck will store every eliminable MI in EliminableList
+bool X86MDSFIDataGuard::OptimizeCheck(MachineFunction &Fn, bool canOpt) {
+  LLVM_DEBUG(dbgs() << "Global Optimizing\n");
+  MachineBasicBlock &MBB = Fn.front();
+  MachineBasicBlock::iterator MBBI = MBB.getFirstNonPHI();
+  DebugLoc DL;
+  if (MBBI != MBB.end())
+    DL = MBBI->getDebugLoc();
+
+  for (MachineBasicBlock &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+    // if can't Opt, only scan inside a bb
+    // so init RT at the begining of every MBB
+    if (!canOpt) {
+      RT.init(Fn);
+    }
+    OptimizeMBB(MBB, RT);
+  }
+  // Loop Optimization is definitely between BBs
+  if (canOpt && enableX86MDSFIDGLoopOpt) {
+    OptimizeLoop(Fn);
+  }
+
+  return false;
+}
+
+bool X86MDSFIDataGuard::isLoopBody(MachineBasicBlock *MBB, MachineLoop *L) {
+  return L->contains(MBB) && MBB != L->getHeader();
+}
+
+void X86MDSFIDataGuard::LoopHoist() {
+  for (auto &L : MLI->getBase()) {
+    ReversePostOrderTraversal<MachineLoop *> RPOT(L);
+    for (MachineLoop *ML : RPOT) {
+      if (ML->getLoopDepth() != 1) {
+        continue;
+      }
+      hoist(ML, MLI, RT);
+    }
+  }
+}
+
+bool X86MDSFIDataGuard::hoist(MachineLoop *ML, MachineLoopInfo *MLI,
+                              X86RegValueTracking &UpLevelRT) {
+
+  MachineLoopBlocksRPO RPOTBlocks(ML);
+  RPOTBlocks.perform(MLI);
+  X86RegValueTracking CurLoopRT;
+  MachineBasicBlock *Header = ML->getHeader();
+  CurLoopRT.init(*(Header->getParent()));
+
+  for (auto &it : Header->predecessors()) {
+    MachineBasicBlock *beforeMBB = &*it;
+    if (ML->contains(beforeMBB)) {
+      continue;
+    }
+    CurLoopRT.OutRanges.at(beforeMBB) = UpLevelRT.OutRanges.at(beforeMBB);
+  }
+
+  // In nature loop, except header, ie, for BB in loop body, should not have
+  // any predecessor outside of the loop
+  for (MachineBasicBlock *MBB : RPOTBlocks) {
+    for (auto &it : MBB->predecessors()) {
+      MachineBasicBlock *beforeMBB = &*it;
+      CurLoopRT.InRanges.at(MBB).merge(CurLoopRT.OutRanges.at(beforeMBB));
+    }
+    CurLoopRT.transfer(MBB);
+  }
+
+  // who needs to be hoisted?
+  // the baseregister of check function and it's indexreg is inRange in one
+  // compute
+  std::set<unsigned> hoistreg;
+  for (MachineBasicBlock *MBB : ML->blocks()) {
+    for (MachineBasicBlock::instr_iterator I = MBB->instr_begin();
+         I != MBB->instr_end(); I++) {
+      MachineInstr *MI = &*I;
+      switch (MI->getOpcode()) {
+      case X86::checkload64m:
+      case X86::checkstore64m: {
+        X86AddressMode AM = getAddressFromInstr(MI, 0);
+        if (AM.BaseType == X86AddressMode::RegBase) {
+          if (AM.IndexReg != 0) {
+            if (CurLoopRT.InRanges.at(MBB)
+                    .getRegRange(AM.IndexReg)
+                    .RangeClass == SmallNum)
+              hoistreg.insert(AM.Base.Reg);
+          } else {
+            hoistreg.insert(AM.Base.Reg);
+          }
+        }
+      }
+      }
+    }
+  }
+
+  // and who can be hoisted?
+  // not modified register
+  for (MachineBasicBlock *MBB : ML->blocks()) {
+    for (MachineBasicBlock::instr_iterator I = MBB->instr_begin();
+         I != MBB->instr_end(); I++) {
+      MachineInstr *MI = &*I;
+      for (auto &MO : MI->defs()) {
+        if (MO.isReg()) {
+          auto Reg = MO.getReg();
+          hoistreg.erase(Reg);
+        }
+      }
+    }
+  }
+
+  MachineBasicBlock *Preheader = ML->getLoopPreheader();
+  // TODO if preheader not exist, insert one
+  if (Preheader == nullptr) {
+    return false;
+  }
+  LLVM_DEBUG(dbgs() << "Loop Preheader is: ");
+  LLVM_DEBUG(Preheader->printAsOperand(dbgs(), false));
+  LLVM_DEBUG(dbgs() << "\n");
+
+  MachineBasicBlock &preheader = *Preheader;
+  MachineBasicBlock::iterator MBBI = Preheader->getLastNonDebugInstr();
+  DebugLoc DL;
+  if (MBBI != Preheader->end()) {
+    DL = MBBI->getDebugLoc();
+  } else {
+  }
+
+  LLVM_DEBUG(dbgs() << "Loop Hoist Register: ");
+  for (unsigned reg : hoistreg) {
+    LLVM_DEBUG(dbgs() << TRI->getRegAsmName(reg) << "\t");
+    addDirectMem(BuildMI(preheader, MBBI, DL, TII->get(X86::checkstore64m)),
+                 reg);
+  }
+  LLVM_DEBUG(dbgs() << "\n");
+
+  for (MachineLoop *SubLoop : *ML) {
+    hoist(SubLoop, MLI, CurLoopRT);
+  }
+  return true;
+}
+
+bool X86MDSFIDataGuard::computeLoop(MachineLoop *ML, MachineLoopInfo *MLI,
+                                    X86RegValueTracking &UpLevelRT) {
+
+  MachineLoopBlocksRPO RPOTBlocks(ML);
+  RPOTBlocks.perform(MLI);
+  X86RegValueTracking CurLoopRT;
+  MachineBasicBlock *Header = ML->getHeader();
+  CurLoopRT.init(*(Header->getParent()));
+
+  for (auto &it : Header->predecessors()) {
+    MachineBasicBlock *beforeMBB = &*it;
+    if (ML->contains(beforeMBB)) {
+      continue;
+    }
+    CurLoopRT.OutRanges.at(beforeMBB) = UpLevelRT.OutRanges.at(beforeMBB);
+  }
+
+  // In nature loop, except header, ie, for BB in loop body, should not have
+  // any predecessor outside of the loop
+  for (MachineBasicBlock *MBB : RPOTBlocks) {
+    for (auto &it : MBB->predecessors()) {
+      MachineBasicBlock *beforeMBB = &*it;
+      LLVM_DEBUG(dbgs() << "OutRanges of header's predecessors\n");
+      LLVM_DEBUG(beforeMBB->printAsOperand(dbgs()));
+      LLVM_DEBUG(dbgs() << "\n" << CurLoopRT.OutRanges.at(beforeMBB) << "\n");
+      CurLoopRT.InRanges.at(MBB).merge(CurLoopRT.OutRanges.at(beforeMBB));
+    }
+    LLVM_DEBUG(dbgs() << "InRanges of MBB before transfer in loop\n");
+    LLVM_DEBUG(MBB->printAsOperand(dbgs()));
+    LLVM_DEBUG(dbgs() << "\n" << CurLoopRT.InRanges.at(MBB) << "\n");
+
+    CurLoopRT.transfer(MBB);
+  }
+
+  LLVM_DEBUG(dbgs() << "Loop Optimizing \n");
+  for (MachineBasicBlock *MBB : RPOTBlocks) {
+    OptimizeMBB(*MBB, CurLoopRT);
+  }
+
+  LLVM_DEBUG(dbgs() << "Loop: " << *ML);
+  for (MachineLoop *SubLoop : *ML) {
+    computeLoop(SubLoop, MLI, CurLoopRT);
+  }
+  return true;
+}
+
+#if 0
+RegsRange collectLoopPattern(RegsRange r1, RegsRange r2) {
+  RegsRange pattern;
+  for (auto &RegIt : r1.Ranges) {
+    if (RegIt.second.equal(r2.Ranges[RegIt.first]) &&RegIt.second.RangeClass =
+            r2.Ranges[RegIt.first].RangeClass) {
+      RangeInfo r = RegIt.second.min(r2.Ranges[RegIt.first]);
+      if (r.UpRange < GuardZoneSize && r.LowRange < GuardZoneSize)
+        pattern.setRegRange(RegIt.first, RegIt.second);
+    }
+  }
+  return pattern;
+}
+bool computeLoop(MachineLoop *ML, MachineLoopInfo *MLI,
+                 X86RegValueTracking &UpLevelRT) {
+
+  MachineLoopBlocksRPO RPOTBlocks(ML);
+  RPOTBlocks.perform(MLI);
+  X86RegValueTracking CurLoopRT;
+  MachineBasicBlock *Header = ML->getHeader();
+  for (auto &it : Header->predecessors()) {
+    MachineBasicBlock *beforeMBB = &*it;
+    if (isLoopBody(beforeMBB, ML)) {
+      continue;
+    }
+    CurLoopRT.InRanges.at(Header).merge(UpLeverlRT.OutRanges.at(beforeMBB));
+  }
+
+  RegsRange RoundRange[2];
+  for (int i = 0; i < 2; i++) {
+
+    // In nature loop, except header, ie, for BB in loop body, should not have
+    // any predecessor outside of the loop
+    for (MachineBasicBlock *MBB : RPOTBlocks) {
+      for (auto &it : MBB->predecessors()) {
+        MachineBasicBlock *beforeMBB = &*it;
+        if (!ML->contains(beforeMBB)) {
+          continue;
+        }
+        CurLoopRT.InRanges.at(MBB).merge(CurLoopRT.OutRanges.at(beforeMBB));
+      }
+
+      CurLoopRT.transfer(MBB);
+    }
+
+    // collect round result
+    for (auto &it : Header->predecessors()) {
+      MachineBasicBlock *beforeMBB = &*it;
+      if (!ML->contains(beforeMBB)) {
+        continue;
+      }
+      RoundRange[i].merge(CurLoopRT.OutRanges.at(beforeMBB));
+    }
+  }
+  // compare two round result, and get Loop Pattern
+  RegsRange LoopPattern = collectLoopPattern(RoundRange[0], RoundRange[1]);
+
+  // with LoopPattern, check if any guard is eliminable
+  for (auto &it : Header->predecessors()) {
+    MachineBasicBlock *beforeMBB = &*it;
+    CurLoopRT.InRanges.at(Header).merge(UpLeverlRT.OutRanges.at(beforeMBB));
+  }
+
+  LLVM_DEBUG(dbgs() << "Loop: " << *ML);
+  for (MachineLoop *SubLoop : *ML) {
+    computeLoop(SubLoop, MLI, CurLoopRT);
+  }
+}
+#endif
+
+// TODO currently we can only handle nature loop
+// no opt on irreducible loop will not cause security problem
+bool X86MDSFIDataGuard::OptimizeLoop(MachineFunction &Fn) {
+  for (auto &L : MLI->getBase()) {
+    // FIXME ReversePostOrder to scan subloop
+    // currently this API works weird (seems does not work fine with sub loops)
+    // so we natively use Loop's API to scan subloops
+    ReversePostOrderTraversal<MachineLoop *> RPOT(L);
+    for (MachineLoop *ML : RPOT) {
+      if (ML->getLoopDepth() != 1) {
+        continue;
+      }
+      computeLoop(ML, MLI, RT);
+    }
+  }
+  return true;
+}
+
+bool X86MDSFIDataGuard::LoweringMI(MachineFunction &Fn) {
+  for (MachineBasicBlock &MBB : Fn) {
+    if (MBB.empty())
+      continue;
+    MachineBasicBlock::iterator MBBI = MBB.begin(), E = MBB.end();
+
+    while (MBBI != E) {
+      MachineInstr *MI = &*MBBI;
+      DebugLoc DL = MI->getDebugLoc();
+      MachineInstr *Upcheck, *Lowcheck;
+
+      MachineBasicBlock::iterator NMBBI = std::next(MBBI);
+
+      switch (MI->getOpcode()) {
+      case X86::checkstore64m:
+      case X86::checkload64m: {
+        if (!enableX86MDSFIDG) {
+          MBBI->eraseFromParent();
+          break;
+        }
+        if (EliminableList[MI]) {
+          LLVM_DEBUG(dbgs() << "eliminable guard " << *MI);
+          MBBI->eraseFromParent();
+          break;
+        }
+
+        if (MI->getOpcode() == X86::checkstore64m) {
+          Upcheck = BuildMI(MBB, MBBI, DL, TII->get(X86::BNDCU64rm), X86::BND1);
+          Lowcheck =
+              BuildMI(MBB, MBBI, DL, TII->get(X86::BNDCL64rm), X86::BND1);
+        } else if (MI->getOpcode() == X86::checkload64m) {
+          Upcheck = BuildMI(MBB, MBBI, DL, TII->get(X86::BNDCU64rm), X86::BND0);
+          Lowcheck =
+              BuildMI(MBB, MBBI, DL, TII->get(X86::BNDCL64rm), X86::BND0);
+        }
+        for (const MachineOperand &MO : MBBI->operands()) {
+          Upcheck->addOperand(MO);
+          Lowcheck->addOperand(MO);
+        }
+        MBBI->eraseFromParent();
+        break;
+      }
+      }
+      MBBI = NMBBI;
+    }
+  }
+  return true;
+}
+
+#undef DEBUG_TYPE
+
+INITIALIZE_PASS_BEGIN(X86MDSFIDataGuard, "x86-constraint-check",
+                      "X86 MDSFI data guard", false, false)
+INITIALIZE_PASS_DEPENDENCY(MachineLoopInfo)
+INITIALIZE_PASS_END(X86MDSFIDataGuard, "x86-constraint-check",
+                    "X86 MDSFI data guard", false, false)
+
+FunctionPass *llvm::createX86MDSFIDataGuard() {
+  return new X86MDSFIDataGuard();
+}
diff --git a/lib/Target/X86/X86RegValueTracking.cpp b/lib/Target/X86/X86RegValueTracking.cpp
new file mode 100644
index 0000000..e0f9589
--- /dev/null
+++ b/lib/Target/X86/X86RegValueTracking.cpp
@@ -0,0 +1,386 @@
+#include "X86.h"
+#include "X86InstrBuilder.h"
+#include "X86InstrInfo.h"
+#include "X86Subtarget.h"
+
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/RegsRange.h"
+
+using namespace llvm;
+#define DEBUG_TYPE "VT"
+class X86RegValueTracking {
+public:
+  const X86Subtarget *STI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+
+  bool computeRange(MachineFunction &Fn);
+  void step(MachineInstr *MI, RegsRange &R);
+  void InferenceCheck(MachineInstr *MI, RegsRange &Ranges);
+  void transfer(MachineBasicBlock *MBB);
+  std::map<MachineBasicBlock *, RegsRange> OutRanges;
+  std::map<MachineBasicBlock *, RegsRange> InRanges;
+  bool isInRange(X86AddressMode &AM);
+  bool isCFILabel(const MachineInstr *MI);
+  bool isInRange(X86AddressMode &AM, RangeInfo R, RegsRange &Ranges);
+  void init(MachineFunction &Fn);
+};
+
+bool X86RegValueTracking::isCFILabel(const MachineInstr * MI) {
+  if( MI->getOpcode() != X86::NOOPL || MI->getNumOperands()!= 5){
+    return false;
+  }
+  MachineOperand MO = MI->getOperand(0);
+  if(!MO.isReg() || MO.getReg() != X86::RBX){
+    return false;
+  }
+  MO = MI->getOperand(1);
+  if(!MO.isImm() || MO.getImm() != 1){
+    return false;
+  }
+  MO = MI->getOperand(2);
+  if(!MO.isReg() || MO.getReg() != X86::RBX){
+    return false;
+  }
+  MO = MI->getOperand(3);
+  if(!MO.isImm() || MO.getImm() != 512){
+    return false;
+  }
+  MO = MI->getOperand(4);
+  if(!MO.isReg() || MO.getReg() != 0){
+    return false;
+  }
+  return true;
+}
+
+void X86RegValueTracking::init(MachineFunction &Fn) {
+  STI = &static_cast<const X86Subtarget &>(Fn.getSubtarget());
+  TRI = STI->getRegisterInfo();
+  OutRanges.clear();
+  InRanges.clear();
+  RangeInfo writeable(InWrite, 0, 0);
+
+  // FIXME
+  // maybe only Ranges of used MBB requires init
+  // This can save memorys
+  for (MachineBasicBlock &MBB : Fn) {
+    // insert
+    if (OutRanges.find(&MBB) == OutRanges.end()) {
+      OutRanges.emplace(&MBB, RegsRange(*TRI));
+      /* LLVM_DEBUG(dbgs() << "scaned MBB "); */
+      /* LLVM_DEBUG(MBB.printAsOperand(dbgs(), false)); */
+      /* LLVM_DEBUG(dbgs() << "\n"); */
+    }
+    if (InRanges.find(&MBB) == InRanges.end()) {
+      InRanges.emplace(&MBB, RegsRange(*TRI));
+      InRanges.at(&MBB).setRegRange(X86::RSP, writeable);
+    }
+  }
+}
+
+#define WIDEN_GUARD 10
+bool X86RegValueTracking::computeRange(MachineFunction &Fn) {
+
+  unsigned widen = 0;
+  bool Changed = true;
+
+  while (Changed) {
+    Changed = false;
+    widen++;
+
+    ReversePostOrderTraversal<MachineFunction *> RPOT(&Fn);
+
+    for (MachineBasicBlock *MBB : RPOT) {
+
+      RegsRange oldout = OutRanges.at(MBB);
+
+      for (auto &it : MBB->predecessors()) {
+        MachineBasicBlock *beforeMBB = &*it;
+        InRanges.at(MBB).merge(OutRanges.at(beforeMBB));
+      }
+
+      transfer(MBB);
+
+      if (!oldout.equal(OutRanges.at(MBB))) {
+        Changed = true;
+        if (widen == WIDEN_GUARD) {
+          OutRanges.at(MBB).setChangedUnknown(oldout);
+        }
+      }
+    }
+  }
+  LLVM_DEBUG(dbgs() << "finished compute Range\n");
+}
+#undef WINDEN_GUARD
+
+void X86RegValueTracking::transfer(MachineBasicBlock *MBB) {
+  // get InRanges copy
+  RegsRange Ranges = InRanges.at(MBB);
+  for (auto &I : MBB->instrs()) {
+    LLVM_DEBUG(dbgs() << "Before step: " << Ranges);
+    MachineInstr *MI = &I;
+    step(MI, Ranges);
+    LLVM_DEBUG(dbgs() << "After step: " << Ranges);
+  }
+  OutRanges.at(MBB) = Ranges;
+}
+
+void X86RegValueTracking::step(MachineInstr *MI, RegsRange &Ranges) {
+  LLVM_DEBUG(dbgs() << *MI << "\n");
+  int i = 0;
+  /* LLVM_DEBUG(dbgs() << "Machine Operands: "); */
+  /* for (const MachineOperand &MO : MI->operands()) { */
+  /*   LLVM_DEBUG(dbgs() << " " << i << " : " << MO); */
+  /*   i++; */
+  /* } */
+  /* LLVM_DEBUG(dbgs() << "\n"); */
+  switch (MI->getOpcode()) {
+  // check ;
+  case X86::checkload64m:
+  case X86::checkstore64m: {
+    InferenceCheck(MI, Ranges);
+  } break;
+  // register to register
+  // TODO  handle more instructions
+  // xor
+  case X86::XOR8rr:
+  case X86::XOR16rr:
+  case X86::XOR32rr:
+  case X86::XOR64rr: {
+    unsigned dst = MI->getOperand(0).getReg();
+    unsigned src = MI->getOperand(1).getReg();
+    if (src == dst) {
+      RangeInfo r1(SmallNum, 0, 0);
+      Ranges.setRegRange(dst, r1);
+    } else {
+      // we don't make complicated arithmetic now
+      RangeInfo r1(Unknown);
+      Ranges.setRegRange(dst, r1);
+    }
+  } break;
+  // lea
+  case X86::LEA16r:
+  case X86::LEA32r:
+  case X86::LEA64r:
+  case X86::LEA64_32r: {
+    X86AddressMode AM;
+    unsigned dst = MI->getOperand(0).getReg();
+    if (!MI->getOperand(4).isImm())
+      break;
+    AM = getAddressFromInstr(MI, 1);
+
+    if (AM.GV != nullptr) {
+      RangeInfo r1(Unknown);
+      Ranges.setRegRange(dst, r1);
+      break;
+    }
+    if (AM.BaseType == X86AddressMode::RegBase) {
+      if (AM.IndexReg == 0) {
+        RangeInfo r1 = Ranges.getRegRange(AM.Base.Reg);
+        bool cal = r1.add(AM.Disp);
+        if (cal) {
+          Ranges.setRegRange(dst, r1);
+        } else {
+          RangeInfo unknownr = RangeInfo(Unknown);
+          Ranges.setRegRange(dst, unknownr);
+        }
+      }
+    }
+  } break;
+  // add
+  case X86::ADD64ri8:
+  case X86::ADD32ri8:
+  case X86::ADD16ri8:
+  case X86::ADD8ri8: {
+    unsigned dst = MI->getOperand(0).getReg();
+    if (!MI->getOperand(2).isImm()) {
+      break;
+    }
+    int imm = MI->getOperand(2).getImm();
+    RangeInfo r1 = Ranges.getRegRange(dst);
+    if (r1.add(imm))
+      Ranges.setRegRange(dst, r1);
+  } break;
+  // movri
+  // X86::MOV64ri movabs
+  case X86::MOV64ri:
+  case X86::MOV64ri32:
+  case X86::MOV32ri:
+  case X86::MOV16ri:
+  case X86::MOV8ri: {
+    unsigned dst = MI->getOperand(0).getReg();
+    if (!MI->getOperand(1).isImm()) {
+      break;
+    }
+    int imm = MI->getOperand(1).getImm();
+    RangeInfo r1(SmallNum, imm, imm);
+    Ranges.setRegRange(dst, r1);
+  } break;
+  default: {
+    if (MI->isMoveReg() && !MI->mayLoad() && !MI->mayStore()) {
+      unsigned dst = MI->getOperand(0).getReg();
+      unsigned src = MI->getOperand(1).getReg();
+      RangeInfo r1 = Ranges.getRegRange(src);
+      LLVM_DEBUG(dbgs() << "isMoveReg get Range: " << r1 << "\n");
+      Ranges.setRegRange(dst, r1);
+    }
+    // memory to register
+    // read from memory will earse register's range
+    // if mayLoad, then find the def register and set it's range to unknown
+    else if (MI->mayLoad()) {
+      for (auto &MO : MI->defs()) {
+        if (MO.isReg()) {
+          auto Reg = MO.getReg();
+          if (X86::GR64RegClass.contains(Reg) ||
+              X86::GR32RegClass.contains(Reg) ||
+              X86::GR16RegClass.contains(Reg) ||
+              X86::GR8RegClass.contains(Reg) || Reg == X86::RSP) {
+            RangeInfo r(Unknown);
+            Ranges.setRegRange(Reg, r);
+          }
+        }
+      }
+    }
+    // unhandled instructions
+    else if (!MI->mayLoad() && !MI->mayStore()) {
+      for (auto &MO : MI->defs()) {
+        if (MO.isReg()) {
+          auto Reg = MO.getReg();
+          if (X86::GR64RegClass.contains(Reg) ||
+              X86::GR32RegClass.contains(Reg) ||
+              X86::GR16RegClass.contains(Reg) ||
+              X86::GR8RegClass.contains(Reg)) {
+            RangeInfo r(Unknown);
+            Ranges.setRegRange(Reg, r);
+          }
+        }
+      }
+    }
+  } break;
+  }
+
+  // call instruction
+  // set all range to unknown;
+  // actual should be CFI_LABEL
+  if (isCFILabel(MI)) {
+    LLVM_DEBUG(dbgs() << "Meet CFI label: " << *MI << "\n");
+    Ranges.setAllUnknown();
+    RangeInfo writeable(InWrite, 0, 0);
+    Ranges.setRegRange(X86::RSP, writeable);
+  }
+}
+
+#define getCheckRegion(MI)                                                     \
+  MI->getOpcode() == X86::checkload64m ? InRead : InWrite
+
+// X64 address mode
+// Base + [1,2,4,8] * IndexReg + Disp32
+// Index:        0     |    1        2       3           4
+// Meaning:   DestReg, | BaseReg,  Scale, IndexReg, Displacement
+// OperandTy: VirtReg, | VirtReg, UnsImm, VirtReg,   SignExtImm
+// Two common ways to construct an address we observered
+// Firstly is the most common way, basereg + offset
+// BaseReg, Scale , IndexReg, Displacement
+//  base  ,   1   ,   NoReg   ,   Imm/Expr
+//
+// Secondly is for an array
+// BaseReg, Scale, IndexReg, Displacement
+//    0   ,   4  ,   index ,    baseaddr of array
+// The scale is the sizeof(elementType)
+// the index usually is rax or other gerenal register
+// the displacement is the BaseAddr of the array
+//
+// Summary:
+// The address mode of X84 is X + a*Y + b
+// X is basereg, and Y is index reg, while a and b are imm value
+// Consider X and Y,if only X exist, we can check X if b is less then 4K.
+// if Y is not noreg, we need range analysis to know the range of Y
+void X86RegValueTracking::InferenceCheck(MachineInstr *MI, RegsRange &Ranges) {
+  X86AddressMode AM;
+  AM = getAddressFromInstr(MI, 0);
+  if (AM.GV != nullptr) {
+    // do not handle GV case now
+    return;
+  }
+  if (AM.BaseType == X86AddressMode::RegBase) {
+    if (AM.IndexReg == 0) {
+      // FIXME
+      // This check function might be eliminable in the future, which means the
+      // range we infered here is not reliable
+      RangeInfo r1 = Ranges.getRegRange(AM.Base.Reg);
+      RangeInfo r2(getCheckRegion(MI), -AM.Disp, -AM.Disp);
+
+      // r2 should be new range, it's not about meet
+      RangeInfo result = RangeInfo::cutRange(r1, r2);
+      // if meet is Unknown, set it's range to check result;
+      Ranges.setRegRange(AM.Base.Reg, result);
+    } else {
+      /* RangeInfo BaseRange = Ranges.getRegRange(AM.Base.Reg); */
+      /* RangeInfo IndexRange = Ranges.getRegRange(AM.IndexReg); */
+      /* // if BaseRange is in one region, then cal the index range as small num
+       */
+      /* if(BaseRange.RangeClass == InRead || BaseRange.RangeClass == InWrite){
+       */
+      /*   RangeInfo newrange(SmallNum, -(BaseRange.UpRange + AM.Disp)/AM.Scale,
+       * -(BaseRange.LowRange + AM.Disp)/AM.Scale); */
+      /*   RangeInfo result = RangeInfo::cutRange(IndexRange, newrange); */
+      /* Ranges.setRegRange(AM.IndexReg, result); */
+      /* return; */
+      /* }else if (IndexRange.RangeClass == SmallNum){ */
+      /*   RangeInfo r2 (getCheckRegion(MI), -IndexRange.UpRange*AM.Scale -
+       * AM.Disp, - IndexRange.LowRange *AM.Scale -AM.Disp); */
+      /*   RangeInfo result = RangeInfo::cutRange(BaseRange, r2); */
+      /*   Ranges.setRegRange(AM.Base.Reg, result); */
+      /*   return; */
+      /* } */
+      return;
+    }
+  } else {
+    // TODO
+    // for benign compiler, GV , which is a symbol, must located in the data
+    // domain as a result, we can assume GV is in write region then inference
+    // Index reg's range for example checkstore GV(,%rax,4) which means rax must
+    // less then guardzone/4 nothing I can do now
+  }
+}
+
+bool X86RegValueTracking::isInRange(X86AddressMode &AM, RangeInfo R,
+                                    RegsRange &Ranges) {
+  bool calculable;
+  if (AM.BaseType == X86AddressMode::RegBase) {
+    if (AM.Base.Reg == X86::RIP){
+      LLVM_DEBUG(dbgs() << "RIP as BaseReg\n");
+      return true;
+    }
+    if (AM.GV != nullptr) {
+      return false;
+    }
+    RangeInfo r1 = Ranges.getRegRange(AM.Base.Reg);
+    if (AM.IndexReg != 0) {
+      // if Index Reg is not noreg, then
+      // Index Reg should be a small number
+      // AM.GV or BaseReg can only exist one
+
+      calculable = r1.add(AM.Disp);
+      RangeInfo indexrange = Ranges.getRegRange(AM.IndexReg);
+      calculable = indexrange.multiple(AM.Scale);
+      calculable = r1.add(indexrange);
+      return calculable ? r1.isInRange(R) : false;
+    }
+
+    calculable = r1.add(AM.Disp);
+    LLVM_DEBUG(dbgs() << "isInRange: " << TRI->getRegAsmName(AM.Base.Reg) << " "
+                      << r1 << "\n");
+    // if R contains r1, then meet will return R;
+    return calculable ? r1.isInRange(R) : false;
+  } else {
+    // FIXME
+    // currently we assum the GV is in the range
+    // GV + indexreg * scale
+    return false;
+  }
+}
+#undef DEBUG_TYPE
diff --git a/lib/Target/X86/X86RegValueTracking.h b/lib/Target/X86/X86RegValueTracking.h
new file mode 100644
index 0000000..b924e03
--- /dev/null
+++ b/lib/Target/X86/X86RegValueTracking.h
@@ -0,0 +1,31 @@
+#ifndef LLVM_LIB_TARGET_X86_X86REGVALUETRACKING_H
+#define LLVM_LIB_TARGET_X86_X86REGVALUETRACKING_H
+#include "X86.h"
+#include "X86InstrBuilder.h"
+#include "X86InstrInfo.h"
+#include "X86Subtarget.h"
+
+#include "llvm/ADT/PostOrderIterator.h"
+#include "llvm/CodeGen/MachineBasicBlock.h"
+#include "llvm/CodeGen/MachineFunction.h"
+#include "llvm/CodeGen/MachineInstr.h"
+#include "llvm/CodeGen/RegsRange.h"
+using namespace llvm;
+class X86RegValueTracking {
+public:
+  const X86Subtarget *STI;
+  const TargetInstrInfo *TII;
+  const TargetRegisterInfo *TRI;
+
+  bool computeRange(MachineFunction &Fn);
+  void step(MachineInstr *MI, RegsRange &R);
+  void InferenceCheck(MachineInstr *MI, RegsRange &Ranges);
+  void transfer(MachineBasicBlock *MBB);
+  std::map<MachineBasicBlock *, RegsRange> OutRanges;
+  std::map<MachineBasicBlock *, RegsRange> InRanges;
+  bool isInRange(X86AddressMode &AM);
+  bool isInRange(X86AddressMode &AM, RangeInfo R, RegsRange &Ranges);
+  void init(MachineFunction &Fn);
+};
+
+#endif
diff --git a/lib/Target/X86/X86TargetMachine.cpp b/lib/Target/X86/X86TargetMachine.cpp
index 374bf3d..a3b5253 100644
--- a/lib/Target/X86/X86TargetMachine.cpp
+++ b/lib/Target/X86/X86TargetMachine.cpp
@@ -69,6 +69,7 @@ void initializeX86ExecutionDomainFixPass(PassRegistry &);
 void initializeX86DomainReassignmentPass(PassRegistry &);
 void initializeX86AvoidSFBPassPass(PassRegistry &);
 void initializeX86FlagsCopyLoweringPassPass(PassRegistry &);
+void initializeX86MDSFIControlGuardPass(PassRegistry &);
 
 } // end namespace llvm
 
@@ -90,6 +91,7 @@ extern "C" void LLVMInitializeX86Target() {
   initializeX86DomainReassignmentPass(PR);
   initializeX86AvoidSFBPassPass(PR);
   initializeX86FlagsCopyLoweringPassPass(PR);
+  initializeX86MDSFIControlGuardPass(PR);
 }
 
 static std::unique_ptr<TargetLoweringObjectFile> createTLOF(const Triple &TT) {
@@ -488,7 +490,9 @@ void X86PassConfig::addPostRegAlloc() {
   addPass(createX86FloatingPointStackifierPass());
 }
 
-void X86PassConfig::addPreSched2() { addPass(createX86ExpandPseudoPass()); }
+void X86PassConfig::addPreSched2() { 
+  addPass(createX86ExpandPseudoPass()); 
+}
 
 void X86PassConfig::addPreEmitPass() {
   if (getOptLevel() != CodeGenOpt::None) {
@@ -518,4 +522,6 @@ void X86PassConfig::addPreEmitPass2() {
   const Triple &TT = TM->getTargetTriple();
   if (!TT.isOSDarwin() && !TT.isOSWindows())
     addPass(createCFIInstrInserter());
+  addPass(createX86MDSFIControlGuard());
+  addPass(createX86MDSFIDataGuard()); 
 }
diff --git a/lib/Transforms/InstCombine/InstCombineCompares.cpp b/lib/Transforms/InstCombine/InstCombineCompares.cpp
index e1bae11..6de92a4 100644
--- a/lib/Transforms/InstCombine/InstCombineCompares.cpp
+++ b/lib/Transforms/InstCombine/InstCombineCompares.cpp
@@ -2924,20 +2924,12 @@ static Value *foldICmpWithLowBitMaskedVal(ICmpInst &I,
     //  x & (-1 >> y) s>= x    ->    x s<= (-1 >> y)
     if (X != I.getOperand(1)) // X must be on RHS of comparison!
       return nullptr;         // Ignore the other case.
-    if (!match(M, m_Constant())) // Can not do this fold with non-constant.
-      return nullptr;
-    if (!match(M, m_NonNegative())) // Must not have any -1 vector elements.
-      return nullptr;
     DstPred = ICmpInst::Predicate::ICMP_SLE;
     break;
   case ICmpInst::Predicate::ICMP_SLT:
     //  x & (-1 >> y) s< x    ->    x s> (-1 >> y)
     if (X != I.getOperand(1)) // X must be on RHS of comparison!
       return nullptr;         // Ignore the other case.
-    if (!match(M, m_Constant())) // Can not do this fold with non-constant.
-      return nullptr;
-    if (!match(M, m_NonNegative())) // Must not have any -1 vector elements.
-      return nullptr;
     DstPred = ICmpInst::Predicate::ICMP_SGT;
     break;
   case ICmpInst::Predicate::ICMP_SLE:
diff --git a/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp b/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp
index f03fcc9..bb0e437 100644
--- a/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp
+++ b/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp
@@ -231,17 +231,17 @@ struct TransformedFunction {
   TransformedFunction& operator=(TransformedFunction&&) = default;
 
   /// Type of the function before the transformation.
-  FunctionType *OriginalType;
+  FunctionType* const OriginalType;
 
   /// Type of the function after the transformation.
-  FunctionType *TransformedType;
+  FunctionType* const TransformedType;
 
   /// Transforming a function may change the position of arguments.  This
   /// member records the mapping from each argument's old position to its new
   /// position.  Argument positions are zero-indexed.  If the transformation
   /// from F to F' made the first argument of F into the third argument of F',
   /// then ArgumentIndexMapping[0] will equal 2.
-  std::vector<unsigned> ArgumentIndexMapping;
+  const std::vector<unsigned> ArgumentIndexMapping;
 };
 
 /// Given function attributes from a call site for the original function,
diff --git a/lib/Transforms/Utils/LCSSA.cpp b/lib/Transforms/Utils/LCSSA.cpp
index 53d444b..a1f8e74 100644
--- a/lib/Transforms/Utils/LCSSA.cpp
+++ b/lib/Transforms/Utils/LCSSA.cpp
@@ -41,7 +41,6 @@
 #include "llvm/IR/Dominators.h"
 #include "llvm/IR/Function.h"
 #include "llvm/IR/Instructions.h"
-#include "llvm/IR/IntrinsicInst.h"
 #include "llvm/IR/PredIteratorCache.h"
 #include "llvm/Pass.h"
 #include "llvm/Transforms/Utils.h"
@@ -202,21 +201,6 @@ bool llvm::formLCSSAForInstructions(SmallVectorImpl<Instruction *> &Worklist,
       SSAUpdate.RewriteUse(*UseToRewrite);
     }
 
-    SmallVector<DbgValueInst *, 4> DbgValues;
-    llvm::findDbgValues(DbgValues, I);
-
-    // Update pre-existing debug value uses that reside outside the loop.
-    auto &Ctx = I->getContext();
-    for (auto DVI : DbgValues) {
-      BasicBlock *UserBB = DVI->getParent();
-      if (InstBB == UserBB || L->contains(UserBB))
-        continue;
-      // We currently only handle debug values residing in blocks where we have
-      // inserted a PHI instruction.
-      if (Value *V = SSAUpdate.FindValueForBlock(UserBB))
-        DVI->setOperand(0, MetadataAsValue::get(Ctx, ValueAsMetadata::get(V)));
-    }
-
     // SSAUpdater might have inserted phi-nodes inside other loops. We'll need
     // to post-process them to keep LCSSA form.
     for (PHINode *InsertedPN : InsertedPHIs) {
diff --git a/lib/Transforms/Utils/SSAUpdater.cpp b/lib/Transforms/Utils/SSAUpdater.cpp
index 9e5fb0e..4a1fd8d 100644
--- a/lib/Transforms/Utils/SSAUpdater.cpp
+++ b/lib/Transforms/Utils/SSAUpdater.cpp
@@ -64,11 +64,6 @@ bool SSAUpdater::HasValueForBlock(BasicBlock *BB) const {
   return getAvailableVals(AV).count(BB);
 }
 
-Value *SSAUpdater::FindValueForBlock(BasicBlock *BB) const {
-  AvailableValsTy::iterator AVI = getAvailableVals(AV).find(BB);
-  return (AVI != getAvailableVals(AV).end()) ? AVI->second : nullptr;
-}
-
 void SSAUpdater::AddAvailableValue(BasicBlock *BB, Value *V) {
   assert(ProtoType && "Need to initialize SSAUpdater");
   assert(ProtoType == V->getType() &&
diff --git a/test/CodeGen/AArch64/arm64-ccmp.ll b/test/CodeGen/AArch64/arm64-ccmp.ll
index 6b497e8..b18e638 100644
--- a/test/CodeGen/AArch64/arm64-ccmp.ll
+++ b/test/CodeGen/AArch64/arm64-ccmp.ll
@@ -526,8 +526,8 @@ define i32 @select_or_olt_one(double %v0, double %v1, double %v2, double %v3, i3
 ; CHECK-LABEL: select_or_one_olt:
 ; CHECK-LABEL: ; %bb.0:
 ; CHECK-NEXT: fcmp d0, d1
-; CHECK-NEXT: fccmp d0, d1, #8, le
-; CHECK-NEXT: fccmp d2, d3, #8, pl
+; CHECK-NEXT: fccmp d0, d1, #1, ne
+; CHECK-NEXT: fccmp d2, d3, #8, vs
 ; CHECK-NEXT: csel w0, w0, w1, mi
 ; CHECK-NEXT: ret
 define i32 @select_or_one_olt(double %v0, double %v1, double %v2, double %v3, i32 %a, i32 %b) #0 {
@@ -556,8 +556,8 @@ define i32 @select_or_olt_ueq(double %v0, double %v1, double %v2, double %v3, i3
 ; CHECK-LABEL: select_or_ueq_olt:
 ; CHECK-LABEL: ; %bb.0:
 ; CHECK-NEXT: fcmp d0, d1
-; CHECK-NEXT: fccmp d0, d1, #1, ne
-; CHECK-NEXT: fccmp d2, d3, #8, vc
+; CHECK-NEXT: fccmp d0, d1, #8, le
+; CHECK-NEXT: fccmp d2, d3, #8, mi
 ; CHECK-NEXT: csel w0, w0, w1, mi
 ; CHECK-NEXT: ret
 define i32 @select_or_ueq_olt(double %v0, double %v1, double %v2, double %v3, i32 %a, i32 %b) #0 {
@@ -656,68 +656,4 @@ define i32 @f128_select_and_olt_oge(fp128 %v0, fp128 %v1, fp128 %v2, fp128 %v3,
   ret i32 %sel
 }
 
-; This testcase resembles the core problem of http://llvm.org/PR39550
-; (an OR operation is 2 levels deep but needs to be implemented first)
-; CHECK-LABEL: deep_or
-; CHECK: cmp w2, #20
-; CHECK-NEXT: ccmp w2, #15, #4, ne
-; CHECK-NEXT: ccmp w1, #0, #4, eq
-; CHECK-NEXT: ccmp w0, #0, #4, ne
-; CHECK-NEXT: csel w0, w4, w5, ne
-; CHECK-NEXT: ret
-define i32 @deep_or(i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %x, i32 %y) {
-  %c0 = icmp ne i32 %a0, 0
-  %c1 = icmp ne i32 %a1, 0
-  %c2 = icmp eq i32 %a2, 15
-  %c3 = icmp eq i32 %a2, 20
-
-  %or = or i1 %c2, %c3
-  %and0 = and i1 %or, %c1
-  %and1 = and i1 %and0, %c0
-  %sel = select i1 %and1, i32 %x, i32 %y
-  ret i32 %sel
-}
-
-; Variation of deep_or, we still need to implement the OR first though.
-; CHECK-LABEL: deep_or1
-; CHECK: cmp w2, #20
-; CHECK-NEXT: ccmp w2, #15, #4, ne
-; CHECK-NEXT: ccmp w0, #0, #4, eq
-; CHECK-NEXT: ccmp w1, #0, #4, ne
-; CHECK-NEXT: csel w0, w4, w5, ne
-; CHECK-NEXT: ret
-define i32 @deep_or1(i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %x, i32 %y) {
-  %c0 = icmp ne i32 %a0, 0
-  %c1 = icmp ne i32 %a1, 0
-  %c2 = icmp eq i32 %a2, 15
-  %c3 = icmp eq i32 %a2, 20
-
-  %or = or i1 %c2, %c3
-  %and0 = and i1 %c0, %or
-  %and1 = and i1 %and0, %c1
-  %sel = select i1 %and1, i32 %x, i32 %y
-  ret i32 %sel
-}
-
-; Variation of deep_or, we still need to implement the OR first though.
-; CHECK-LABEL: deep_or2
-; CHECK: cmp w2, #20
-; CHECK-NEXT: ccmp w2, #15, #4, ne
-; CHECK-NEXT: ccmp w1, #0, #4, eq
-; CHECK-NEXT: ccmp w0, #0, #4, ne
-; CHECK-NEXT: csel w0, w4, w5, ne
-; CHECK-NEXT: ret
-define i32 @deep_or2(i32 %a0, i32 %a1, i32 %a2, i32 %a3, i32 %x, i32 %y) {
-  %c0 = icmp ne i32 %a0, 0
-  %c1 = icmp ne i32 %a1, 0
-  %c2 = icmp eq i32 %a2, 15
-  %c3 = icmp eq i32 %a2, 20
-
-  %or = or i1 %c2, %c3
-  %and0 = and i1 %c0, %c1
-  %and1 = and i1 %and0, %or
-  %sel = select i1 %and1, i32 %x, i32 %y
-  ret i32 %sel
-}
-
 attributes #0 = { nounwind }
diff --git a/test/CodeGen/Mips/micromips-gcc-except-table.ll b/test/CodeGen/Mips/micromips-gcc-except-table.ll
deleted file mode 100644
index 38a7692..0000000
--- a/test/CodeGen/Mips/micromips-gcc-except-table.ll
+++ /dev/null
@@ -1,37 +0,0 @@
-; RUN: llc -mtriple=mips-linux-gnu -mcpu=mips32r2 -mattr=+micromips -O3 -filetype=obj < %s | llvm-objdump -s -j .gcc_except_table - | FileCheck %s
-
-; CHECK: Contents of section .gcc_except_table:
-; CHECK-NEXT: 0000 ff9b1501 0c011100 00110e1f 011f1800
-; CHECK-NEXT: 0010 00010000 00000000
-
-@_ZTIi = external constant i8*
-
-define dso_local i32 @main() local_unnamed_addr norecurse personality i8* bitcast (i32 (...)* @__gxx_personality_v0 to i8*) {
-entry:
-  %exception.i = tail call i8* @__cxa_allocate_exception(i32 4) nounwind
-  %0 = bitcast i8* %exception.i to i32*
-  store i32 5, i32* %0, align 16
-  invoke void @__cxa_throw(i8* %exception.i, i8* bitcast (i8** @_ZTIi to i8*), i8* null) noreturn
-          to label %.noexc unwind label %return
-
-.noexc:
-  unreachable
-
-return:
-  %1 = landingpad { i8*, i32 }
-          catch i8* null
-  %2 = extractvalue { i8*, i32 } %1, 0
-  %3 = tail call i8* @__cxa_begin_catch(i8* %2) nounwind
-  tail call void @__cxa_end_catch()
-  ret i32 0
-}
-
-declare i32 @__gxx_personality_v0(...)
-
-declare i8* @__cxa_begin_catch(i8*) local_unnamed_addr
-
-declare void @__cxa_end_catch() local_unnamed_addr
-
-declare i8* @__cxa_allocate_exception(i32) local_unnamed_addr
-
-declare void @__cxa_throw(i8*, i8*, i8*) local_unnamed_addr
diff --git a/test/CodeGen/PowerPC/VSX-XForm-Scalars.ll b/test/CodeGen/PowerPC/VSX-XForm-Scalars.ll
index 643ec90..e38c5be 100644
--- a/test/CodeGen/PowerPC/VSX-XForm-Scalars.ll
+++ b/test/CodeGen/PowerPC/VSX-XForm-Scalars.ll
@@ -1,46 +1,35 @@
 ; RUN: llc < %s -mcpu=pwr8 -mtriple=powerpc64le-unknown-unknown \
-; RUN:   -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names -verify-machineinstrs \
-; RUN:   | FileCheck %s --check-prefix=CHECK-P8
+; RUN:   -verify-machineinstrs | FileCheck %s --check-prefix=CHECK-P8
 ; RUN: llc < %s -mcpu=pwr9 -mtriple=powerpc64le-unknown-unknown \
-; RUN:   -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names -verify-machineinstrs \
-; RUN:   | FileCheck %s --check-prefix=CHECK-P9
+; RUN:   -verify-machineinstrs | FileCheck %s --check-prefix=CHECK-P9
 
 @a = external local_unnamed_addr global <4 x i32>, align 16
 @pb = external local_unnamed_addr global float*, align 8
 
 define void @testExpandPostRAPseudo(i32* nocapture readonly %ptr) {
-; CHECK-P8-LABEL: testExpandPostRAPseudo:
-; CHECK-P8:  # %bb.0: # %entry
-; CHECK-P8:    lfiwzx f0, 0, r3
-; CHECK-P8:    ld r4, .LC0@toc@l(r4)
-; CHECK-P8:    xxpermdi vs0, f0, f0, 2
-; CHECK-P8:    xxspltw v2, vs0, 3
-; CHECK-P8:    stvx v2, 0, r4
-; CHECK-P8:    lis r4, 1024
-; CHECK-P8:    lfiwax f0, 0, r3
-; CHECK-P8:    addis r3, r2, .LC1@toc@ha
-; CHECK-P8:    ld r3, .LC1@toc@l(r3)
-; CHECK-P8:    xscvsxdsp f0, f0
-; CHECK-P8:    ld r3, 0(r3)
-; CHECK-P8:    stfsx f0, r3, r4
-; CHECK-P8:    blr
-;
-; CHECK-P9-LABEL: testExpandPostRAPseudo:
-; CHECK-P9:  # %bb.0: # %entry
-; CHECK-P9:    lfiwzx f0, 0, r3
-; CHECK-P9:    addis r4, r2, .LC0@toc@ha
-; CHECK-P9:    ld r4, .LC0@toc@l(r4)
-; CHECK-P9:    xxpermdi vs0, f0, f0, 2
-; CHECK-P9:    xxspltw vs0, vs0, 3
-; CHECK-P9:    stxvx vs0, 0, r4
-; CHECK-P9:    lis r4, 1024
-; CHECK-P9:    lfiwax f0, 0, r3
-; CHECK-P9:    addis r3, r2, .LC1@toc@ha
-; CHECK-P9:    ld r3, .LC1@toc@l(r3)
-; CHECK-P9:    xscvsxdsp f0, f0
-; CHECK-P9:    ld r3, 0(r3)
-; CHECK-P9:    stfsx f0, r3, r4
-; CHECK-P9:    blr
+; CHECK-P8-LABEL:     testExpandPostRAPseudo:
+; CHECK-P8:           lxsiwax 34, 0, 3
+; CHECK-P8-NEXT:      xxspltw 34, 34, 1
+; CHECK-P8-NEXT:      stvx 2, 0, 4
+; CHECK-P8:           #APP
+; CHECK-P8-NEXT:      #Clobber Rigisters
+; CHECK-P8-NEXT:      #NO_APP
+; CHECK-P8-NEXT:      lis 4, 1024
+; CHECK-P8-NEXT:      lfiwax 0, 0, 3
+; CHECK-P8:           stfsx 0, 3, 4
+; CHECK-P8-NEXT:      blr
+
+; CHECK-P9-LABEL:     testExpandPostRAPseudo:
+; CHECK-P9:           lxvwsx 0, 0, 3
+; CHECK-P9:           stxvx 0, 0, 4
+; CHECK-P9:           #APP
+; CHECK-P9-NEXT:      #Clobber Rigisters
+; CHECK-P9-NEXT:      #NO_APP
+; CHECK-P9-NEXT:      lis 4, 1024
+; CHECK-P9-NEXT:      lfiwax 0, 0, 3
+; CHECK-P9:           stfsx 0, 3, 4
+; CHECK-P9-NEXT:      blr
+
 entry:
   %0 = load i32, i32* %ptr, align 4
   %splat.splatinsert = insertelement <4 x i32> undef, i32 %0, i32 0
diff --git a/test/CodeGen/PowerPC/build-vector-tests.ll b/test/CodeGen/PowerPC/build-vector-tests.ll
index d192baf..f074e2a 100644
--- a/test/CodeGen/PowerPC/build-vector-tests.ll
+++ b/test/CodeGen/PowerPC/build-vector-tests.ll
@@ -109,8 +109,8 @@
 ;vector int spltRegVali(int val) {                                            //
 ;  return (vector int) val;                                                   //
 ;}                                                                            //
-;// P8: (LE) lfiwzx, xxpermdi, xxspltw (BE): lfiwzx, xxsldwi, xxspltw         //
-;// P9: (LE) lfiwzx, xxpermdi, xxspltw (BE): lfiwzx, xxsldwi, xxspltw         //
+;// P8: lxsiwax, xxspltw                                                      //
+;// P9: lxvwsx                                                                //
 ;vector int spltMemVali(int *ptr) {                                           //
 ;  return (vector int)*ptr;                                                   //
 ;}                                                                            //
@@ -284,8 +284,8 @@
 ;vector unsigned int spltRegValui(unsigned int val) {                         //
 ;  return (vector unsigned int) val;                                          //
 ;}                                                                            //
-;// P8: (LE) lfiwzx, xxpermdi, xxspltw (BE): lfiwzx, xxsldwi, xxspltw         //
-;// P9: (LE) lfiwzx, xxpermdi, xxspltw (BE): lfiwzx, xxsldwi, xxspltw         //
+;// P8: lxsiwax, xxspltw                                                      //
+;// P9: lxvwsx                                                                //
 ;vector unsigned int spltMemValui(unsigned int *ptr) {                        //
 ;  return (vector unsigned int)*ptr;                                          //
 ;}                                                                            //
@@ -1202,21 +1202,15 @@ entry:
 ; P9LE-LABEL: spltMemVali
 ; P8BE-LABEL: spltMemVali
 ; P8LE-LABEL: spltMemVali
-; P9BE: lfiwzx f0, 0, r3
-; P9BE: xxsldwi vs0, f0, f0, 1
-; P9BE: xxspltw v2, vs0, 0
+; P9BE: lxvwsx v2, 0, r3
 ; P9BE: blr
-; P9LE: lfiwzx f0, 0, r3
-; P9LE: xxpermdi vs0, f0, f0, 2
-; P9LE: xxspltw v2, vs0, 3
+; P9LE: lxvwsx v2, 0, r3
 ; P9LE: blr
-; P8BE: lfiwzx f0, 0, r3
-; P8BE: xxsldwi vs0, f0, f0, 1
-; P8BE: xxspltw v2, vs0, 0
+; P8BE: lxsiwax {{[vsf0-9]+}}, 0, r3
+; P8BE: xxspltw v2, {{[vsf0-9]+}}, 1
 ; P8BE: blr
-; P8LE: lfiwzx f0, 0, r3
-; P8LE: xxpermdi vs0, f0, f0, 2
-; P8LE: xxspltw v2, vs0, 3
+; P8LE: lxsiwax {{[vsf0-9]+}}, 0, r3
+; P8LE: xxspltw v2, {{[vsf0-9]+}}, 1
 ; P8LE: blr
 }
 
@@ -2344,21 +2338,15 @@ entry:
 ; P9LE-LABEL: spltMemValui
 ; P8BE-LABEL: spltMemValui
 ; P8LE-LABEL: spltMemValui
-; P9BE: lfiwzx f0, 0, r3
-; P9BE: xxsldwi vs0, f0, f0, 1
-; P9BE: xxspltw v2, vs0, 0
+; P9BE: lxvwsx v2, 0, r3
 ; P9BE: blr
-; P9LE: lfiwzx f0, 0, r3
-; P9LE: xxpermdi vs0, f0, f0, 2
-; P9LE: xxspltw v2, vs0, 3
+; P9LE: lxvwsx v2, 0, r3
 ; P9LE: blr
-; P8BE: lfiwzx f0, 0, r3
-; P8BE: xxsldwi vs0, f0, f0, 1
-; P8BE: xxspltw v2, vs0, 0
+; P8BE: lxsiwax {{[vsf0-9]+}}, 0, r3
+; P8BE: xxspltw v2, {{[vsf0-9]+}}, 1
 ; P8BE: blr
-; P8LE: lfiwzx f0, 0, r3
-; P8LE: xxpermdi vs0, f0, f0, 2
-; P8LE: xxspltw v2, vs0, 3
+; P8LE: lxsiwax {{[vsf0-9]+}}, 0, r3
+; P8LE: xxspltw v2, {{[vsf0-9]+}}, 1
 ; P8LE: blr
 }
 
diff --git a/test/CodeGen/PowerPC/load-v4i8-improved.ll b/test/CodeGen/PowerPC/load-v4i8-improved.ll
index f1fa299..36f3472 100644
--- a/test/CodeGen/PowerPC/load-v4i8-improved.ll
+++ b/test/CodeGen/PowerPC/load-v4i8-improved.ll
@@ -1,27 +1,15 @@
-; RUN: llc -verify-machineinstrs -mcpu=pwr8 -mtriple=powerpc64le-unknown-linux-gnu < %s \
-; RUN:   -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names | FileCheck --check-prefix=CHECK-LE \
+; RUN: llc -verify-machineinstrs -mcpu=pwr8 -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck \
 ; RUN:   -implicit-check-not vmrg -implicit-check-not=vperm %s
-; RUN: llc -verify-machineinstrs -mcpu=pwr8 -mtriple=powerpc64-unknown-linux-gnu < %s \
-; RUN:   -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names | FileCheck \
+; RUN: llc -verify-machineinstrs -mcpu=pwr8 -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck \
 ; RUN:   -implicit-check-not vmrg -implicit-check-not=vperm %s
 
 define <16 x i8> @test(i32* %s, i32* %t) {
-; CHECK-LE-LABEL: test:
-; CHECK-LE:       # %bb.0: # %entry
-; CHECK-LE-NEXT:    lfiwzx f0, 0, r3
-; CHECK-LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-LE-NEXT:    xxspltw v2, vs0, 3
-; CHECK-LE-NEXT:    blr
-
-; CHECK-LABEL: test:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lfiwzx f0, 0, r3
-; CHECK-NEXT:    xxsldwi vs0, f0, f0, 1
-; CHECK-NEXT:    xxspltw v2, vs0, 0
-; CHECK-NEXT:    blr
 entry:
   %0 = bitcast i32* %s to <4 x i8>*
   %1 = load <4 x i8>, <4 x i8>* %0, align 4
   %2 = shufflevector <4 x i8> %1, <4 x i8> undef, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 0, i32 1, i32 2, i32 3, i32 0, i32 1, i32 2, i32 3, i32 0, i32 1, i32 2, i32 3>
   ret <16 x i8> %2
+; CHECK-LABEL: test
+; CHECK: lxsiwax 34, 0, 3
+; CHECK: xxspltw 34, 34, 1
 }
diff --git a/test/CodeGen/PowerPC/power9-moves-and-splats.ll b/test/CodeGen/PowerPC/power9-moves-and-splats.ll
index 5ccba80..fc676cc 100644
--- a/test/CodeGen/PowerPC/power9-moves-and-splats.ll
+++ b/test/CodeGen/PowerPC/power9-moves-and-splats.ll
@@ -1,74 +1,47 @@
-; RUN: llc -mcpu=pwr9 -mtriple=powerpc64le-unknown-linux-gnu -ppc-vsr-nums-as-vr \
-; RUN:   -ppc-asm-full-reg-names < %s | FileCheck %s
-; RUN: llc -mcpu=pwr9 -mtriple=powerpc64-unknown-linux-gnu -ppc-vsr-nums-as-vr \
-; RUN:   -ppc-asm-full-reg-names < %s | FileCheck %s --check-prefix=CHECK-BE
+; RUN: llc -mcpu=pwr9 -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s
+; RUN: llc -mcpu=pwr9 -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s \
+; RUN:   --check-prefix=CHECK-BE
 
 @Globi = external global i32, align 4
 @Globf = external global float, align 4
 
 define <2 x i64> @test1(i64 %a, i64 %b) {
-; CHECK-LABEL: test1:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    mtvsrdd v2, r4, r3
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test1:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    mtvsrdd v2, r3, r4
-; CHECK-BE-NEXT:    blr
 entry:
 ; The FIXME below is due to the lowering for BUILD_VECTOR needing a re-vamp
 ; which will happen in a subsequent patch.
+; CHECK-LABEL: test1
+; CHECK: mtvsrdd 34, 4, 3
+; CHECK-BE-LABEL: test1
+; CHECK-BE: mtvsrdd 34, 3, 4
   %vecins = insertelement <2 x i64> undef, i64 %a, i32 0
   %vecins1 = insertelement <2 x i64> %vecins, i64 %b, i32 1
   ret <2 x i64> %vecins1
 }
 
 define i64 @test2(<2 x i64> %a) {
-; CHECK-LABEL: test2:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    mfvsrld r3, v2
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test2:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    mfvsrd r3, v2
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test2
+; CHECK: mfvsrld 3, 34
   %0 = extractelement <2 x i64> %a, i32 0
   ret i64 %0
 }
 
 define i64 @test3(<2 x i64> %a) {
-; CHECK-LABEL: test3:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    mfvsrd r3, v2
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test3:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    mfvsrld r3, v2
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-BE-LABEL: test3
+; CHECK-BE: mfvsrld 3, 34
   %0 = extractelement <2 x i64> %a, i32 1
   ret i64 %0
 }
 
 define <4 x i32> @test4(i32* nocapture readonly %in) {
-; CHECK-LABEL: test4:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lfiwzx f0, 0, r3
-; CHECK-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-NEXT:    xxspltw v2, vs0, 3
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test4:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    lfiwzx f0, 0, r3
-; CHECK-BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; CHECK-BE-NEXT:    xxspltw v2, vs0, 0
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test4
+; CHECK: lxvwsx 34, 0, 3
+; CHECK-NOT: xxspltw
+; CHECK-BE-LABEL: test4
+; CHECK-BE: lxvwsx 34, 0, 3
+; CHECK-BE-NOT: xxspltw
   %0 = load i32, i32* %in, align 4
   %splat.splatinsert = insertelement <4 x i32> undef, i32 %0, i32 0
   %splat.splat = shufflevector <4 x i32> %splat.splatinsert, <4 x i32> undef, <4 x i32> zeroinitializer
@@ -76,20 +49,13 @@ entry:
 }
 
 define <4 x float> @test5(float* nocapture readonly %in) {
-; CHECK-LABEL: test5:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lfiwzx f0, 0, r3
-; CHECK-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-NEXT:    xxspltw v2, vs0, 3
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test5:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    lfiwzx f0, 0, r3
-; CHECK-BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; CHECK-BE-NEXT:    xxspltw v2, vs0, 0
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test5
+; CHECK: lxvwsx 34, 0, 3
+; CHECK-NOT: xxspltw
+; CHECK-BE-LABEL: test5
+; CHECK-BE: lxvwsx 34, 0, 3
+; CHECK-BE-NOT: xxspltw
   %0 = load float, float* %in, align 4
   %splat.splatinsert = insertelement <4 x float> undef, float %0, i32 0
   %splat.splat = shufflevector <4 x float> %splat.splatinsert, <4 x float> undef, <4 x i32> zeroinitializer
@@ -97,24 +63,17 @@ entry:
 }
 
 define <4 x i32> @test6() {
-; CHECK-LABEL: test6:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    addis r3, r2, .LC0@toc@ha
-; CHECK-NEXT:    ld r3, .LC0@toc@l(r3)
-; CHECK-NEXT:    lfiwzx f0, 0, r3
-; CHECK-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-NEXT:    xxspltw v2, vs0, 3
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test6:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    addis r3, r2, .LC0@toc@ha
-; CHECK-BE-NEXT:    ld r3, .LC0@toc@l(r3)
-; CHECK-BE-NEXT:    lfiwzx f0, 0, r3
-; CHECK-BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; CHECK-BE-NEXT:    xxspltw v2, vs0, 0
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test6
+; CHECK: addis
+; CHECK: ld [[TOC:[0-9]+]], .LC0
+; CHECK: lxvwsx 34, 0, 3
+; CHECK-NOT: xxspltw
+; CHECK-BE-LABEL: test6
+; CHECK-BE: addis
+; CHECK-BE: ld [[TOC:[0-9]+]], .LC0
+; CHECK-BE: lxvwsx 34, 0, 3
+; CHECK-BE-NOT: xxspltw
   %0 = load i32, i32* @Globi, align 4
   %splat.splatinsert = insertelement <4 x i32> undef, i32 %0, i32 0
   %splat.splat = shufflevector <4 x i32> %splat.splatinsert, <4 x i32> undef, <4 x i32> zeroinitializer
@@ -122,24 +81,17 @@ entry:
 }
 
 define <4 x float> @test7() {
-; CHECK-LABEL: test7:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    addis r3, r2, .LC1@toc@ha
-; CHECK-NEXT:    ld r3, .LC1@toc@l(r3)
-; CHECK-NEXT:    lfiwzx f0, 0, r3
-; CHECK-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-NEXT:    xxspltw v2, vs0, 3
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test7:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    addis r3, r2, .LC1@toc@ha
-; CHECK-BE-NEXT:    ld r3, .LC1@toc@l(r3)
-; CHECK-BE-NEXT:    lfiwzx f0, 0, r3
-; CHECK-BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; CHECK-BE-NEXT:    xxspltw v2, vs0, 0
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test7
+; CHECK: addis
+; CHECK: ld [[TOC:[0-9]+]], .LC1
+; CHECK: lxvwsx 34, 0, 3
+; CHECK-NOT: xxspltw
+; CHECK-BE-LABEL: test7
+; CHECK-BE: addis
+; CHECK-BE: ld [[TOC:[0-9]+]], .LC1
+; CHECK-BE: lxvwsx 34, 0, 3
+; CHECK-BE-NOT: xxspltw
   %0 = load float, float* @Globf, align 4
   %splat.splatinsert = insertelement <4 x float> undef, float %0, i32 0
   %splat.splat = shufflevector <4 x float> %splat.splatinsert, <4 x float> undef, <4 x i32> zeroinitializer
@@ -147,120 +99,76 @@ entry:
 }
 
 define <16 x i8> @test8() {
-; CHECK-LABEL: test8:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxlxor v2, v2, v2
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test8:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxlxor v2, v2, v2
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test8
+; CHECK: xxlxor 34, 34, 34
+; CHECK-BE-LABEL: test8
+; CHECK-BE: xxlxor 34, 34, 34
   ret <16 x i8> zeroinitializer
 }
 
 define <16 x i8> @test9() {
-; CHECK-LABEL: test9:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxspltib v2, 1
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test9:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxspltib v2, 1
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test9
+; CHECK: xxspltib 34, 1
+; CHECK-BE-LABEL: test9
+; CHECK-BE: xxspltib 34, 1
   ret <16 x i8> <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
 }
 
 define <16 x i8> @test10() {
-; CHECK-LABEL: test10:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxspltib v2, 127
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test10:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxspltib v2, 127
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test10
+; CHECK: xxspltib 34, 127
+; CHECK-BE-LABEL: test10
+; CHECK-BE: xxspltib 34, 127
   ret <16 x i8> <i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127, i8 127>
 }
 
 define <16 x i8> @test11() {
-; CHECK-LABEL: test11:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxspltib v2, 128
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test11:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxspltib v2, 128
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test11
+; CHECK: xxspltib 34, 128
+; CHECK-BE-LABEL: test11
+; CHECK-BE: xxspltib 34, 128
   ret <16 x i8> <i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128, i8 -128>
 }
 
 define <16 x i8> @test12() {
-; CHECK-LABEL: test12:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxspltib v2, 255
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test12:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxspltib v2, 255
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test12
+; CHECK: xxspltib 34, 255
+; CHECK-BE-LABEL: test12
+; CHECK-BE: xxspltib 34, 255
   ret <16 x i8> <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
 }
 
 define <16 x i8> @test13() {
-; CHECK-LABEL: test13:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxspltib v2, 129
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test13:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxspltib v2, 129
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test13
+; CHECK: xxspltib 34, 129
+; CHECK-BE-LABEL: test13
+; CHECK-BE: xxspltib 34, 129
   ret <16 x i8> <i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127, i8 -127>
 }
 
 define <16 x i8> @test13E127() {
-; CHECK-LABEL: test13E127:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    xxspltib v2, 200
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test13E127:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    xxspltib v2, 200
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test13E127
+; CHECK: xxspltib 34, 200
+; CHECK-BE-LABEL: test13E127
+; CHECK-BE: xxspltib 34, 200
   ret <16 x i8> <i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200, i8 200>
 }
 
 define <4 x i32> @test14(<4 x i32> %a, i32* nocapture readonly %b) {
-; CHECK-LABEL: test14:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lwz r3, 0(r5)
-; CHECK-NEXT:    mtvsrws v2, r3
-; CHECK-NEXT:    addi r3, r3, 5
-; CHECK-NEXT:    stw r3, 0(r5)
-; CHECK-NEXT:    blr
-
-; CHECK-BE-LABEL: test14:
-; CHECK-BE:       # %bb.0: # %entry
-; CHECK-BE-NEXT:    lwz r3, 0(r5)
-; CHECK-BE-NEXT:    mtvsrws v2, r3
-; CHECK-BE-NEXT:    addi r3, r3, 5
-; CHECK-BE-NEXT:    stw r3, 0(r5)
-; CHECK-BE-NEXT:    blr
 entry:
+; CHECK-LABEL: test14
+; CHECK: lwz [[LD:[0-9]+]],
+; CHECK: mtvsrws 34, [[LD]]
+; CHECK-BE-LABEL: test14
+; CHECK-BE: lwz [[LD:[0-9]+]],
+; CHECK-BE: mtvsrws 34, [[LD]]
   %0 = load i32, i32* %b, align 4
   %splat.splatinsert = insertelement <4 x i32> undef, i32 %0, i32 0
   %splat.splat = shufflevector <4 x i32> %splat.splatinsert, <4 x i32> undef, <4 x i32> zeroinitializer
diff --git a/test/CodeGen/PowerPC/pr38087.ll b/test/CodeGen/PowerPC/pr38087.ll
index 2736ffa..af8704f 100644
--- a/test/CodeGen/PowerPC/pr38087.ll
+++ b/test/CodeGen/PowerPC/pr38087.ll
@@ -11,8 +11,9 @@ declare { i32, i1 } @llvm.usub.with.overflow.i32(i32, i32) #0
 define void @draw_llvm_vs_variant0() {
 ; CHECK-LABEL: draw_llvm_vs_variant0:
 ; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lfd f0, 0(r3)
-; CHECK-NEXT:    xxpermdi v2, f0, f0, 2
+; CHECK-NEXT:    ldx r3, 0, r3
+; CHECK-NEXT:    mtvsrd f0, r3
+; CHECK-NEXT:    xxswapd v2, vs0
 ; CHECK-NEXT:    vmrglh v2, v2, v2
 ; CHECK-NEXT:    vextsh2w v2, v2
 ; CHECK-NEXT:    xvcvsxwsp vs0, v2
diff --git a/test/CodeGen/PowerPC/qpx-load-splat.ll b/test/CodeGen/PowerPC/qpx-load-splat.ll
index 1afd272..0349618 100644
--- a/test/CodeGen/PowerPC/qpx-load-splat.ll
+++ b/test/CodeGen/PowerPC/qpx-load-splat.ll
@@ -1,44 +1,35 @@
-; RUN: llc -mtriple=powerpc64le-unknown-linux-gnu -ppc-vsr-nums-as-vr \
-; RUN:   -ppc-asm-full-reg-names -verify-machineinstrs < %s | FileCheck %s
+; RUN: llc -verify-machineinstrs < %s | FileCheck %s
+target datalayout = "E-m:e-i64:64-n32:64"
+target triple = "powerpc64-bgq-linux"
 
 ; Function Attrs: norecurse nounwind readonly
 define <4 x double> @foo(double* nocapture readonly %a) #0 {
-; CHECK-LABEL: foo:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lxvdsx v2, 0, r3
-; CHECK-NEXT:    vmr v3, v2
-; CHECK-NEXT:    blr
 entry:
   %0 = load double, double* %a, align 8
   %vecinit.i = insertelement <4 x double> undef, double %0, i32 0
   %shuffle.i = shufflevector <4 x double> %vecinit.i, <4 x double> undef, <4 x i32> zeroinitializer
   ret <4 x double> %shuffle.i
+
+; CHECK-LABEL: @foo
+; CHECK: lfd 1, 0(3)
+; CHECK: blr
 }
 
 define <4 x double> @foox(double* nocapture readonly %a, i64 %idx) #0 {
-; CHECK-LABEL: foox:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    sldi r4, r4, 3
-; CHECK-NEXT:    lxvdsx v2, r3, r4
-; CHECK-NEXT:    vmr v3, v2
-; CHECK-NEXT:    blr
 entry:
   %p = getelementptr double, double* %a, i64 %idx
   %0 = load double, double* %p, align 8
   %vecinit.i = insertelement <4 x double> undef, double %0, i32 0
   %shuffle.i = shufflevector <4 x double> %vecinit.i, <4 x double> undef, <4 x i32> zeroinitializer
   ret <4 x double> %shuffle.i
+
+; CHECK-LABEL: @foox
+; CHECK: sldi [[REG1:[0-9]+]], 4, 3
+; CHECK: lfdx 1, 3, [[REG1]]
+; CHECK: blr
 }
 
 define <4 x double> @fooxu(double* nocapture readonly %a, i64 %idx, double** %pptr) #0 {
-; CHECK-LABEL: fooxu:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    sldi r4, r4, 3
-; CHECK-NEXT:    lfdux f0, r3, r4
-; CHECK-NEXT:    xxspltd v2, vs0, 0
-; CHECK-NEXT:    std r3, 0(r5)
-; CHECK-NEXT:    vmr v3, v2
-; CHECK-NEXT:    blr
 entry:
   %p = getelementptr double, double* %a, i64 %idx
   %0 = load double, double* %p, align 8
@@ -46,36 +37,39 @@ entry:
   %shuffle.i = shufflevector <4 x double> %vecinit.i, <4 x double> undef, <4 x i32> zeroinitializer
   store double* %p, double** %pptr, align 8
   ret <4 x double> %shuffle.i
+
+; CHECK-LABEL: @foox
+; CHECK: sldi [[REG1:[0-9]+]], 4, 3
+; CHECK: lfdux 1, 3, [[REG1]]
+; CHECK: std 3, 0(5)
+; CHECK: blr
 }
 
 define <4 x float> @foof(float* nocapture readonly %a) #0 {
-; CHECK-LABEL: foof:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    lfiwzx f0, 0, r3
-; CHECK-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-NEXT:    xxspltw v2, vs0, 3
-; CHECK-NEXT:    blr
 entry:
   %0 = load float, float* %a, align 4
   %vecinit.i = insertelement <4 x float> undef, float %0, i32 0
   %shuffle.i = shufflevector <4 x float> %vecinit.i, <4 x float> undef, <4 x i32> zeroinitializer
   ret <4 x float> %shuffle.i
+
+; CHECK-LABEL: @foof
+; CHECK: lfs 1, 0(3)
+; CHECK: blr
 }
 
 define <4 x float> @foofx(float* nocapture readonly %a, i64 %idx) #0 {
-; CHECK-LABEL: foofx:
-; CHECK:       # %bb.0: # %entry
-; CHECK-NEXT:    sldi r4, r4, 2
-; CHECK-NEXT:    lfiwzx f0, r3, r4
-; CHECK-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-NEXT:    xxspltw v2, vs0, 3
-; CHECK-NEXT:    blr
 entry:
   %p = getelementptr float, float* %a, i64 %idx
   %0 = load float, float* %p, align 4
   %vecinit.i = insertelement <4 x float> undef, float %0, i32 0
   %shuffle.i = shufflevector <4 x float> %vecinit.i, <4 x float> undef, <4 x i32> zeroinitializer
   ret <4 x float> %shuffle.i
+
+; CHECK-LABEL: @foofx
+; CHECK: sldi [[REG1:[0-9]+]], 4, 2
+; CHECK: lfsx 1, 3, [[REG1]]
+; CHECK: blr
 }
 
+attributes #0 = { norecurse nounwind readonly "target-cpu"="a2q" "target-features"="+qpx,-altivec,-bpermd,-crypto,-direct-move,-extdiv,-power8-vector,-vsx" }
 
diff --git a/test/CodeGen/PowerPC/scalar_vector_test_1.ll b/test/CodeGen/PowerPC/scalar_vector_test_1.ll
deleted file mode 100644
index 1b5ddff..0000000
--- a/test/CodeGen/PowerPC/scalar_vector_test_1.ll
+++ /dev/null
@@ -1,292 +0,0 @@
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:		-mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9LE
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9BE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8LE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8BE
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test1(i64* nocapture readonly %int64, <2 x i64> %vec) {
-; P9LE-LABEL: s2v_test1:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 0(r3)
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test1:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 0(r3)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-entry:
-  %0 = load i64, i64* %int64, align 8
-  %vecins = insertelement <2 x i64> %vec, i64 %0, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test2(i64* nocapture readonly %int64, <2 x i64> %vec)  {
-; P9LE-LABEL: s2v_test2:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 8(r3)
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test2:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 8(r3)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds i64, i64* %int64, i64 1
-  %0 = load i64, i64* %arrayidx, align 8
-  %vecins = insertelement <2 x i64> %vec, i64 %0, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test3(i64* nocapture readonly %int64, <2 x i64> %vec, i32 signext %Idx)  {
-; P9LE-LABEL: s2v_test3:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    sldi r4, r7, 3
-; P9LE-NEXT:    lfdx f0, r3, r4
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test3
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    sldi r4, r7, 3
-; P9BE-NEXT:    lfdx f0, r3, r4
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-entry:
-  %idxprom = sext i32 %Idx to i64
-  %arrayidx = getelementptr inbounds i64, i64* %int64, i64 %idxprom
-  %0 = load i64, i64* %arrayidx, align 8
-  %vecins = insertelement <2 x i64> %vec, i64 %0, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test4(i64* nocapture readonly %int64, <2 x i64> %vec)  {
-; P9LE-LABEL: s2v_test4:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 8(r3)
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test4:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 8(r3)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds i64, i64* %int64, i64 1
-  %0 = load i64, i64* %arrayidx, align 8
-  %vecins = insertelement <2 x i64> %vec, i64 %0, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test5(<2 x i64> %vec, i64* nocapture readonly %ptr1)  {
-; P9LE-LABEL: s2v_test5:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 0(r5)
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test5:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 0(r5)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-entry:
-  %0 = load i64, i64* %ptr1, align 8
-  %vecins = insertelement <2 x i64> %vec, i64 %0, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x double> @s2v_test_f1(double* nocapture readonly %f64, <2 x double> %vec)  {
-; P9LE-LABEL: s2v_test_f1:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 0(r3)
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f1:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 0(r3)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f1:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfdx f0, 0, r3
-; P8LE-NEXT:    xxspltd vs0, vs0, 0
-; P8LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f1:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfdx f0, 0, r3
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %0 = load double, double* %f64, align 8
-  %vecins = insertelement <2 x double> %vec, double %0, i32 0
-  ret <2 x double> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x double> @s2v_test_f2(double* nocapture readonly %f64, <2 x double> %vec)  {
-; P9LE-LABEL: s2v_test_f2:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 8(r3)
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f2:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 8(r3)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f2:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 8
-; P8LE-NEXT:    lfdx f0, 0, r3
-; P8LE-NEXT:    xxspltd vs0, vs0, 0
-; P8LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f2:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    addi r3, r3, 8
-; P8BE-NEXT:    lfdx f0, 0, r3
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds double, double* %f64, i64 1
-  %0 = load double, double* %arrayidx, align 8
-  %vecins = insertelement <2 x double> %vec, double %0, i32 0
-  ret <2 x double> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x double> @s2v_test_f3(double* nocapture readonly %f64, <2 x double> %vec, i32 signext %Idx)  {
-; P9LE-LABEL: s2v_test_f3:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    sldi r4, r7, 3
-; P9LE-NEXT:    lfdx f0, r3, r4
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f3:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    sldi r4, r7, 3
-; P9BE-NEXT:    lfdx f0, r3, r4
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f3:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    sldi r4, r7, 3
-; P8LE-NEXT:    lfdx f0, r3, r4
-; P8LE-NEXT:    xxspltd vs0, vs0, 0
-; P8LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f3:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    sldi r4, r7, 3
-; P8BE-NEXT:    lfdx f0, r3, r4
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %idxprom = sext i32 %Idx to i64
-  %arrayidx = getelementptr inbounds double, double* %f64, i64 %idxprom
-  %0 = load double, double* %arrayidx, align 8
-  %vecins = insertelement <2 x double> %vec, double %0, i32 0
-  ret <2 x double> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x double> @s2v_test_f4(double* nocapture readonly %f64, <2 x double> %vec)  {
-; P9LE-LABEL: s2v_test_f4:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 8(r3)
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f4:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 8(r3)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f4:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 8
-; P8LE-NEXT:    lfdx f0, 0, r3
-; P8LE-NEXT:    xxspltd vs0, vs0, 0
-; P8LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f4:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    addi r3, r3, 8
-; P8BE-NEXT:    lfdx f0, 0, r3
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds double, double* %f64, i64 1
-  %0 = load double, double* %arrayidx, align 8
-  %vecins = insertelement <2 x double> %vec, double %0, i32 0
-  ret <2 x double> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x double> @s2v_test_f5(<2 x double> %vec, double* nocapture readonly %ptr1)  {
-; P9LE-LABEL: s2v_test_f5:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfd f0, 0(r5)
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f5:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfd f0, 0(r5)
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f5:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfdx f0, 0, r5
-; P8LE-NEXT:    xxspltd vs0, vs0, 0
-; P8LE-NEXT:    xxpermdi v2, v2, vs0, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f5:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfdx f0, 0, r5
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %0 = load double, double* %ptr1, align 8
-  %vecins = insertelement <2 x double> %vec, double %0, i32 0
-  ret <2 x double> %vecins
-}
-
diff --git a/test/CodeGen/PowerPC/scalar_vector_test_2.ll b/test/CodeGen/PowerPC/scalar_vector_test_2.ll
deleted file mode 100644
index da1b8bc..0000000
--- a/test/CodeGen/PowerPC/scalar_vector_test_2.ll
+++ /dev/null
@@ -1,118 +0,0 @@
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9LE
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9BE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8LE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8BE
-
-define void @test_liwzx1(<1 x float>* %A, <1 x float>* %B, <1 x float>* %C) {
-; P9LE-LABEL: test_liwzx1:
-; P9LE:       # %bb.0:
-; P9LE-NEXT:    lfiwzx f0, 0, r3
-; P9LE-NEXT:    lfiwzx f1, 0, r4
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi vs1, f1, f1, 2
-; P9LE-NEXT:    xvaddsp vs0, vs0, vs1
-; P9LE-NEXT:    xxsldwi vs0, vs0, vs0, 3
-; P9LE-NEXT:    xscvspdpn f0, vs0
-; P9LE-NEXT:    stfs f0, 0(r5)
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: test_liwzx1:
-; P9BE:       # %bb.0:
-; P9BE-NEXT:    lfiwzx f0, 0, r3
-; P9BE-NEXT:    lfiwzx f1, 0, r4
-; P9BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P9BE-NEXT:    xxsldwi vs1, f1, f1, 1
-; P9BE-NEXT:    xvaddsp vs0, vs0, vs1
-; P9BE-NEXT:    xscvspdpn f0, vs0
-; P9BE-NEXT:    stfs f0, 0(r5)
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: test_liwzx1:
-; P8LE:       # %bb.0:
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    lfiwzx f1, 0, r4
-; P8LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P8LE-NEXT:    xxpermdi vs1, f1, f1, 2
-; P8LE-NEXT:    xvaddsp vs0, vs0, vs1
-; P8LE-NEXT:    xxsldwi vs0, vs0, vs0, 3
-; P8LE-NEXT:    xscvspdpn f0, vs0
-; P8LE-NEXT:    stfsx f0, 0, r5
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: test_liwzx1:
-; P8BE:       # %bb.0:
-; P8BE-NEXT:    lfiwzx f0, 0, r3
-; P8BE-NEXT:    lfiwzx f1, 0, r4
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE-NEXT:    xxsldwi vs1, f1, f1, 1
-; P8BE-NEXT:    xvaddsp vs0, vs0, vs1
-; P8BE-NEXT:    xscvspdpn f0, vs0
-; P8BE-NEXT:    stfsx f0, 0, r5
-; P8BE-NEXT:    blr
-  %a = load <1 x float>, <1 x float>* %A
-  %b = load <1 x float>, <1 x float>* %B
-  %X = fadd <1 x float> %a, %b
-  store <1 x float> %X, <1 x float>* %C
-  ret void
-}
-
-define <1 x float>* @test_liwzx2(<1 x float>* %A, <1 x float>* %B, <1 x float>* %C) {
-; P9LE-LABEL: test_liwzx2:
-; P9LE:       # %bb.0:
-; P9LE-NEXT:    lfiwzx f0, 0, r3
-; P9LE-NEXT:    lfiwzx f1, 0, r4
-; P9LE-NEXT:    mr r3, r5
-; P9LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P9LE-NEXT:    xxpermdi vs1, f1, f1, 2
-; P9LE-NEXT:    xvsubsp vs0, vs0, vs1
-; P9LE-NEXT:    xxsldwi vs0, vs0, vs0, 3
-; P9LE-NEXT:    xscvspdpn f0, vs0
-; P9LE-NEXT:    stfs f0, 0(r5)
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: test_liwzx2:
-; P9BE:       # %bb.0:
-; P9BE-NEXT:    lfiwzx f0, 0, r3
-; P9BE-NEXT:    lfiwzx f1, 0, r4
-; P9BE-NEXT:    mr r3, r5
-; P9BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P9BE-NEXT:    xxsldwi vs1, f1, f1, 1
-; P9BE-NEXT:    xvsubsp vs0, vs0, vs1
-; P9BE-NEXT:    xscvspdpn f0, vs0
-; P9BE-NEXT:    stfs f0, 0(r5)
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: test_liwzx2:
-; P8LE:       # %bb.0:
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    lfiwzx f1, 0, r4
-; P8LE-NEXT:    mr r3, r5
-; P8LE-NEXT:    xxpermdi vs0, f0, f0, 2
-; P8LE-NEXT:    xxpermdi vs1, f1, f1, 2
-; P8LE-NEXT:    xvsubsp vs0, vs0, vs1
-; P8LE-NEXT:    xxsldwi vs0, vs0, vs0, 3
-; P8LE-NEXT:    xscvspdpn f0, vs0
-; P8LE-NEXT:    stfsx f0, 0, r5
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: test_liwzx2:
-; P8BE:       # %bb.0:
-; P8BE-NEXT:    lfiwzx f0, 0, r3
-; P8BE-NEXT:    lfiwzx f1, 0, r4
-; P8BE-NEXT:    mr r3, r5
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE-NEXT:    xxsldwi vs1, f1, f1, 1
-; P8BE-NEXT:    xvsubsp vs0, vs0, vs1
-; P8BE-NEXT:    xscvspdpn f0, vs0
-; P8BE-NEXT:    stfsx f0, 0, r5
-; P8BE-NEXT:    blr
-  %a = load <1 x float>, <1 x float>* %A
-  %b = load <1 x float>, <1 x float>* %B
-  %X = fsub <1 x float> %a, %b
-  store <1 x float> %X, <1 x float>* %C
-  ret <1 x float>* %C
-}
diff --git a/test/CodeGen/PowerPC/scalar_vector_test_3.ll b/test/CodeGen/PowerPC/scalar_vector_test_3.ll
deleted file mode 100644
index c63044a..0000000
--- a/test/CodeGen/PowerPC/scalar_vector_test_3.ll
+++ /dev/null
@@ -1,265 +0,0 @@
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9LE
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9BE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8LE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8BE
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test1(i32* nocapture readonly %int32, <2 x i64> %vec)  {
-; P9LE-LABEL: s2v_test1:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfiwax f0, 0, r3
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test1:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfiwax f0, 0, r3
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test1:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwax f0, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test1:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfiwax f0, 0, r3
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %0 = load i32, i32* %int32, align 4
-  %conv = sext i32 %0 to i64
-  %vecins = insertelement <2 x i64> %vec, i64 %conv, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test2(i32* nocapture readonly %int32, <2 x i64> %vec)  {
-; P9LE-LABEL: s2v_test2:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    addi r3, r3, 4
-; P9LE-NEXT:    lfiwax f0, 0, r3
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test2:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    addi r3, r3, 4
-; P9BE-NEXT:    lfiwax f0, 0, r3
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test2:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 4
-; P8LE-NEXT:    lfiwax f0, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test2:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    addi r3, r3, 4
-; P8BE-NEXT:    lfiwax f0, 0, r3
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds i32, i32* %int32, i64 1
-  %0 = load i32, i32* %arrayidx, align 4
-  %conv = sext i32 %0 to i64
-  %vecins = insertelement <2 x i64> %vec, i64 %conv, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test3(i32* nocapture readonly %int32, <2 x i64> %vec, i32 signext %Idx)  {
-; P9LE-LABEL: s2v_test3:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    sldi r4, r7, 2
-; P9LE-NEXT:    lfiwax f0, r3, r4
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test3:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    sldi r4, r7, 2
-; P9BE-NEXT:    lfiwax f0, r3, r4
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test3:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    sldi r4, r7, 2
-; P8LE-NEXT:    lfiwax f0, r3, r4
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test3:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    sldi r4, r7, 2
-; P8BE-NEXT:    lfiwax f0, r3, r4
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %idxprom = sext i32 %Idx to i64
-  %arrayidx = getelementptr inbounds i32, i32* %int32, i64 %idxprom
-  %0 = load i32, i32* %arrayidx, align 4
-  %conv = sext i32 %0 to i64
-  %vecins = insertelement <2 x i64> %vec, i64 %conv, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test4(i32* nocapture readonly %int32, <2 x i64> %vec)  {
-; P9LE-LABEL: s2v_test4:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    addi r3, r3, 4
-; P9LE-NEXT:    lfiwax f0, 0, r3
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test4:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    addi r3, r3, 4
-; P9BE-NEXT:    lfiwax f0, 0, r3
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test4:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 4
-; P8LE-NEXT:    lfiwax f0, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test4:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    addi r3, r3, 4
-; P8BE-NEXT:    lfiwax f0, 0, r3
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds i32, i32* %int32, i64 1
-  %0 = load i32, i32* %arrayidx, align 4
-  %conv = sext i32 %0 to i64
-  %vecins = insertelement <2 x i64> %vec, i64 %conv, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test5(<2 x i64> %vec, i32* nocapture readonly %ptr1)  {
-; P9LE-LABEL: s2v_test5:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfiwax f0, 0, r5
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test5:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfiwax f0, 0, r5
-; P9BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test5:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwax f0, 0, r5
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    xxpermdi v2, v2, v3, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test5:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfiwax f0, 0, r5
-; P8BE-NEXT:    xxpermdi v2, vs0, v2, 1
-; P8BE-NEXT:    blr
-entry:
-  %0 = load i32, i32* %ptr1, align 4
-  %conv = sext i32 %0 to i64
-  %vecins = insertelement <2 x i64> %vec, i64 %conv, i32 0
-  ret <2 x i64> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test6(i32* nocapture readonly %ptr)  {
-; P9LE-LABEL: s2v_test6:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfiwax f0, 0, r3
-; P9LE-NEXT:    xxpermdi v2, f0, f0, 2
-; P9LE-NEXT:    xxspltd v2, v2, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test6:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfiwax f0, 0, r3
-; P9BE-NEXT:    xxspltd v2, vs0, 0
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test6:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwax f0, 0, r3
-; P8LE-NEXT:    xxpermdi v2, f0, f0, 2
-; P8LE-NEXT:    xxspltd v2, v2, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test6:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfiwax f0, 0, r3
-; P8BE-NEXT:    xxspltd v2, vs0, 0
-; P8BE-NEXT:    blr
-entry:
-  %0 = load i32, i32* %ptr, align 4
-  %conv = sext i32 %0 to i64
-  %splat.splatinsert = insertelement <2 x i64> undef, i64 %conv, i32 0
-  %splat.splat = shufflevector <2 x i64> %splat.splatinsert, <2 x i64> undef, <2 x i32> zeroinitializer
-  ret <2 x i64> %splat.splat
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x i64> @s2v_test7(i32* nocapture readonly %ptr)  {
-; P9LE-LABEL: s2v_test7:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfiwax f0, 0, r3
-; P9LE-NEXT:    xxpermdi v2, f0, f0, 2
-; P9LE-NEXT:    xxspltd v2, v2, 1
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test7:
-; P9BE:       # %bb.0: # %entry
-; P9BE-NEXT:    lfiwax f0, 0, r3
-; P9BE-NEXT:    xxspltd v2, vs0, 0
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test7:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwax f0, 0, r3
-; P8LE-NEXT:    xxpermdi v2, f0, f0, 2
-; P8LE-NEXT:    xxspltd v2, v2, 1
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test7:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfiwax f0, 0, r3
-; P8BE-NEXT:    xxspltd v2, vs0, 0
-; P8BE-NEXT:    blr
-entry:
-  %0 = load i32, i32* %ptr, align 4
-  %conv = sext i32 %0 to i64
-  %splat.splatinsert = insertelement <2 x i64> undef, i64 %conv, i32 0
-  %splat.splat = shufflevector <2 x i64> %splat.splatinsert, <2 x i64> undef, <2 x i32> zeroinitializer
-  ret <2 x i64> %splat.splat
-}
-
diff --git a/test/CodeGen/PowerPC/scalar_vector_test_4.ll b/test/CodeGen/PowerPC/scalar_vector_test_4.ll
deleted file mode 100644
index aaaf0ba..0000000
--- a/test/CodeGen/PowerPC/scalar_vector_test_4.ll
+++ /dev/null
@@ -1,341 +0,0 @@
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9LE
-; RUN: llc -mcpu=pwr9 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P9BE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8LE
-; RUN: llc -mcpu=pwr8 -verify-machineinstrs -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
-; RUN:    -mtriple=powerpc64-unknown-linux-gnu < %s | FileCheck %s --check-prefix=P8BE
-
-; Function Attrs: norecurse nounwind readonly
-define <4 x i32> @s2v_test1(i32* nocapture readonly %int32, <4 x i32> %vec)  {
-; P8LE-LABEL: s2v_test1:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    addis r4, r2, .LCPI0_0@toc@ha
-; P8LE-NEXT:    addi r3, r4, .LCPI0_0@toc@l
-; P8LE-NEXT:    lvx v4, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vperm v2, v3, v2, v4
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test1:
-; P8BE:       # %bb.0: # %entry
-; P8BE:         lfiwzx f0, 0, r3
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE:         xxsldwi vs0, v2, vs0, 1
-; P8BE:         xxsldwi v2, vs0, vs0, 3
-; P8BE-NEXT:    blr
-entry:
-  %0 = load i32, i32* %int32, align 4
-  %vecins = insertelement <4 x i32> %vec, i32 %0, i32 0
-  ret <4 x i32> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <4 x i32> @s2v_test2(i32* nocapture readonly %int32, <4 x i32> %vec)  {
-; P8LE-LABEL: s2v_test2:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 4
-; P8LE-NEXT:    addis r4, r2, .LCPI1_0@toc@ha
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    addi r3, r4, .LCPI1_0@toc@l
-; P8LE-NEXT:    lvx v4, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vperm v2, v3, v2, v4
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test2:
-; P8BE:       # %bb.0: # %entry
-; P8BE:         addi r3, r3, 4
-; P8BE:         lfiwzx f0, 0, r3
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE:         xxsldwi vs0, v2, vs0, 1
-; P8BE:         xxsldwi v2, vs0, vs0, 3
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds i32, i32* %int32, i64 1
-  %0 = load i32, i32* %arrayidx, align 4
-  %vecins = insertelement <4 x i32> %vec, i32 %0, i32 0
-  ret <4 x i32> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <4 x i32> @s2v_test3(i32* nocapture readonly %int32, <4 x i32> %vec, i32 signext %Idx)  {
-; P8LE-LABEL: s2v_test3:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    sldi r5, r7, 2
-; P8LE-NEXT:    addis r4, r2, .LCPI2_0@toc@ha
-; P8LE-NEXT:    lfiwzx f0, r3, r5
-; P8LE-NEXT:    addi r3, r4, .LCPI2_0@toc@l
-; P8LE-NEXT:    lvx v4, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vperm v2, v3, v2, v4
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test3:
-; P8BE:       # %bb.0: # %entry
-; P8BE:         sldi r4, r7, 2
-; P8BE:         lfiwzx f0, r3, r4
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE:         xxsldwi vs0, v2, vs0, 1
-; P8BE:         xxsldwi v2, vs0, vs0, 3
-; P8BE-NEXT:    blr
-entry:
-  %idxprom = sext i32 %Idx to i64
-  %arrayidx = getelementptr inbounds i32, i32* %int32, i64 %idxprom
-  %0 = load i32, i32* %arrayidx, align 4
-  %vecins = insertelement <4 x i32> %vec, i32 %0, i32 0
-  ret <4 x i32> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <4 x i32> @s2v_test4(i32* nocapture readonly %int32, <4 x i32> %vec)  {
-; P8LE-LABEL: s2v_test4:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 4
-; P8LE-NEXT:    addis r4, r2, .LCPI3_0@toc@ha
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    addi r3, r4, .LCPI3_0@toc@l
-; P8LE-NEXT:    lvx v4, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vperm v2, v3, v2, v4
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test4:
-; P8BE:       # %bb.0: # %entry
-; P8BE:         addi r3, r3, 4
-; P8BE:         lfiwzx f0, 0, r3
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE:         xxsldwi vs0, v2, vs0, 1
-; P8BE:         xxsldwi v2, vs0, vs0, 3
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds i32, i32* %int32, i64 1
-  %0 = load i32, i32* %arrayidx, align 4
-  %vecins = insertelement <4 x i32> %vec, i32 %0, i32 0
-  ret <4 x i32> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <4 x i32> @s2v_test5(<4 x i32> %vec, i32* nocapture readonly %ptr1)  {
-; P8LE-LABEL: s2v_test5:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwzx f0, 0, r5
-; P8LE-NEXT:    addis r3, r2, .LCPI4_0@toc@ha
-; P8LE-NEXT:    addi r3, r3, .LCPI4_0@toc@l
-; P8LE-NEXT:    lvx v4, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vperm v2, v3, v2, v4
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test5:
-; P8BE:       # %bb.0: # %entry
-; P8BE:         lfiwzx f0, 0, r5
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE:         xxsldwi vs0, v2, vs0, 1
-; P8BE:         xxsldwi v2, vs0, vs0, 3
-; P8BE-NEXT:    blr
-entry:
-  %0 = load i32, i32* %ptr1, align 4
-  %vecins = insertelement <4 x i32> %vec, i32 %0, i32 0
-  ret <4 x i32> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <4 x float> @s2v_test_f1(float* nocapture readonly %f64, <4 x float> %vec)  {
-; P8LE-LABEL: s2v_test_f1:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    addis r4, r2, .LCPI5_0@toc@ha
-; P8LE-NEXT:    addi r3, r4, .LCPI5_0@toc@l
-; P8LE-NEXT:    lvx v4, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vperm v2, v3, v2, v4
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f1:
-; P8BE:       # %bb.0: # %entry
-; P8BE:         lfiwzx f0, 0, r3
-; P8BE-NEXT:    xxsldwi vs0, f0, f0, 1
-; P8BE:         xxsldwi vs0, v2, vs0, 1
-; P8BE:         xxsldwi v2, vs0, vs0, 3
-; P8BE-NEXT:    blr
-entry:
-  %0 = load float, float* %f64, align 4
-  %vecins = insertelement <4 x float> %vec, float %0, i32 0
-  ret <4 x float> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x float> @s2v_test_f2(float* nocapture readonly %f64, <2 x float> %vec)  {
-; P9LE-LABEL: s2v_test_f2:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    addi r3, r3, 4
-; P9LE-NEXT:    xxspltw v2, v2, 2
-; P9LE-NEXT:    lfiwzx f0, 0, r3
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    vmrglw v2, v2, v3
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f2:
-; P9BE:       # %bb.0: # %entry
-; P9BE:       addi r3, r3, 4
-; P9BE:       xxspltw v2, v2, 1
-; P9BE:       lfiwzx f0, 0, r3
-; P9BE-NEXT:  xxsldwi v3, f0, f0, 1
-; P9BE:       vmrghw v2, v3, v2
-; P9BE-NEXT:  blr
-
-; P8LE-LABEL: s2v_test_f2:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 4
-; P8LE-NEXT:    xxspltw v2, v2, 2
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vmrglw v2, v2, v3
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f2:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    addi r3, r3, 4
-; P8BE-NEXT:    xxspltw v2, v2, 1
-; P8BE-NEXT:    lfiwzx f0, 0, r3
-; P8BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P8BE-NEXT:    vmrghw v2, v3, v2
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds float, float* %f64, i64 1
-  %0 = load float, float* %arrayidx, align 8
-  %vecins = insertelement <2 x float> %vec, float %0, i32 0
-  ret <2 x float> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x float> @s2v_test_f3(float* nocapture readonly %f64, <2 x float> %vec, i32 signext %Idx)  {
-; P9LE-LABEL: s2v_test_f3:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    sldi r4, r7, 2
-; P9LE-NEXT:    xxspltw v2, v2, 2
-; P9LE-NEXT:    lfiwzx f0, r3, r4
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    vmrglw v2, v2, v3
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f3:
-; P9BE:       # %bb.0: # %entry
-; P9BE:         sldi r4, r7, 2
-; P9BE:         xxspltw v2, v2, 1
-; P9BE:         lfiwzx f0, r3, r4
-; P9BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P9BE:         vmrghw v2, v3, v2
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f3:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    sldi r4, r7, 2
-; P8LE-NEXT:    xxspltw v2, v2, 2
-; P8LE-NEXT:    lfiwzx f0, r3, r4
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vmrglw v2, v2, v3
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f3:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    sldi r4, r7, 2
-; P8BE-NEXT:    xxspltw v2, v2, 1
-; P8BE-NEXT:    lfiwzx f0, r3, r4
-; P8BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P8BE-NEXT:    vmrghw v2, v3, v2
-; P8BE-NEXT:    blr
-entry:
-  %idxprom = sext i32 %Idx to i64
-  %arrayidx = getelementptr inbounds float, float* %f64, i64 %idxprom
-  %0 = load float, float* %arrayidx, align 8
-  %vecins = insertelement <2 x float> %vec, float %0, i32 0
-  ret <2 x float> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x float> @s2v_test_f4(float* nocapture readonly %f64, <2 x float> %vec)  {
-; P9LE-LABEL: s2v_test_f4:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    addi r3, r3, 4
-; P9LE-NEXT:    xxspltw v2, v2, 2
-; P9LE-NEXT:    lfiwzx f0, 0, r3
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    vmrglw v2, v2, v3
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f4:
-; P9BE:       # %bb.0: # %entry
-; P9BE:         addi r3, r3, 4
-; P9BE:         xxspltw v2, v2, 1
-; P9BE:         lfiwzx f0, 0, r3
-; P9BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P9BE:         vmrghw v2, v3, v2
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f4:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    addi r3, r3, 4
-; P8LE-NEXT:    xxspltw v2, v2, 2
-; P8LE-NEXT:    lfiwzx f0, 0, r3
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vmrglw v2, v2, v3
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f4:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    addi r3, r3, 4
-; P8BE-NEXT:    xxspltw v2, v2, 1
-; P8BE-NEXT:    lfiwzx f0, 0, r3
-; P8BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P8BE-NEXT:    vmrghw v2, v3, v2
-; P8BE-NEXT:    blr
-entry:
-  %arrayidx = getelementptr inbounds float, float* %f64, i64 1
-  %0 = load float, float* %arrayidx, align 8
-  %vecins = insertelement <2 x float> %vec, float %0, i32 0
-  ret <2 x float> %vecins
-}
-
-; Function Attrs: norecurse nounwind readonly
-define <2 x float> @s2v_test_f5(<2 x float> %vec, float* nocapture readonly %ptr1)  {
-; P9LE-LABEL: s2v_test_f5:
-; P9LE:       # %bb.0: # %entry
-; P9LE-NEXT:    lfiwzx f0, 0, r5
-; P9LE-NEXT:    xxspltw v2, v2, 2
-; P9LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P9LE-NEXT:    vmrglw v2, v2, v3
-; P9LE-NEXT:    blr
-
-; P9BE-LABEL: s2v_test_f5:
-; P9BE:       # %bb.0: # %entry
-; P9BE:         lfiwzx f0, 0, r5
-; P9BE:         xxspltw v2, v2, 1
-; P9BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P9BE:         vmrghw v2, v3, v2
-; P9BE-NEXT:    blr
-
-; P8LE-LABEL: s2v_test_f5:
-; P8LE:       # %bb.0: # %entry
-; P8LE-NEXT:    lfiwzx f0, 0, r5
-; P8LE-NEXT:    xxspltw v2, v2, 2
-; P8LE-NEXT:    xxpermdi v3, f0, f0, 2
-; P8LE-NEXT:    vmrglw v2, v2, v3
-; P8LE-NEXT:    blr
-
-; P8BE-LABEL: s2v_test_f5:
-; P8BE:       # %bb.0: # %entry
-; P8BE-NEXT:    lfiwzx f0, 0, r5
-; P8BE-NEXT:    xxspltw v2, v2, 1
-; P8BE-NEXT:    xxsldwi v3, f0, f0, 1
-; P8BE-NEXT:    vmrghw v2, v3, v2
-; P8BE-NEXT:    blr
-entry:
-  %0 = load float, float* %ptr1, align 8
-  %vecins = insertelement <2 x float> %vec, float %0, i32 0
-  ret <2 x float> %vecins
-}
-
diff --git a/test/CodeGen/PowerPC/swaps-le-6.ll b/test/CodeGen/PowerPC/swaps-le-6.ll
index ac0bcc7..82c240e 100644
--- a/test/CodeGen/PowerPC/swaps-le-6.ll
+++ b/test/CodeGen/PowerPC/swaps-le-6.ll
@@ -1,15 +1,12 @@
-; RUN: llc -verify-machineinstrs -mcpu=pwr8 -ppc-vsr-nums-as-vr \
-; RUN:   -ppc-asm-full-reg-names -mtriple=powerpc64le-unknown-linux-gnu \
-; RUN:   -O3 < %s | FileCheck %s
+; RUN: llc -verify-machineinstrs -mcpu=pwr8 \
+; RUN:   -mtriple=powerpc64le-unknown-linux-gnu -O3 < %s | FileCheck %s
 
 ; RUN: llc -mcpu=pwr9 -mtriple=powerpc64le-unknown-linux-gnu -O3 \
-; RUN:   -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names -verify-machineinstrs \
-; RUN:   < %s | FileCheck %s --check-prefix=CHECK-P9 \
+; RUN:   -verify-machineinstrs < %s | FileCheck %s --check-prefix=CHECK-P9 \
 ; RUN:   --implicit-check-not xxswapd
 
 ; RUN: llc -mcpu=pwr9 -mtriple=powerpc64le-unknown-linux-gnu -O3 \
-; RUN:   -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names -verify-machineinstrs \
-; RUN:   -mattr=-power9-vector < %s | FileCheck %s
+; RUN:   -verify-machineinstrs -mattr=-power9-vector < %s | FileCheck %s
 
 ; These tests verify that VSX swap optimization works when loading a scalar
 ; into a vector register.
@@ -20,31 +17,6 @@
 @y = global double 1.780000e+00, align 8
 
 define void @bar0() {
-; CHECK-LABEL: bar0:
-; CHECK:   # %bb.0: # %entry
-; CHECK:     addis r3, r2, .LC0@toc@ha
-; CHECK:     addis r4, r2, .LC1@toc@ha
-; CHECK:     ld r3, .LC0@toc@l(r3)
-; CHECK:     addis r3, r2, .LC2@toc@ha
-; CHECK:     ld r3, .LC2@toc@l(r3)
-; CHECK:     xxpermdi vs0, vs0, vs1, 1
-; CHECK:     stxvd2x vs0, 0, r3
-; CHECK:     blr
-;
-; CHECK-P9-LABEL: bar0:
-; CHECK-P9:   # %bb.0: # %entry
-; CHECK-P9:     addis r3, r2, .LC0@toc@ha
-; CHECK-P9:     addis r4, r2, .LC1@toc@ha
-; CHECK-P9:     ld r3, .LC0@toc@l(r3)
-; CHECK-P9:     ld r4, .LC1@toc@l(r4)
-; CHECK-P9:     lfd f0, 0(r3)
-; CHECK-P9:     lxvx vs1, 0, r4
-; CHECK-P9:     addis r3, r2, .LC2@toc@ha
-; CHECK-P9:     ld r3, .LC2@toc@l(r3)
-; CHECK-P9:     xxpermdi vs0, f0, f0, 2
-; CHECK-P9:     xxpermdi vs0, vs1, vs0, 1
-; CHECK-P9:     stxvx vs0, 0, r3
-; CHECK-P9:     blr
 entry:
   %0 = load <2 x double>, <2 x double>* @x, align 16
   %1 = load double, double* @y, align 8
@@ -53,32 +25,21 @@ entry:
   ret void
 }
 
+; CHECK-LABEL: @bar0
+; CHECK-DAG: lxvd2x [[REG1:[0-9]+]]
+; CHECK-DAG: lfdx [[REG2:[0-9]+]]
+; CHECK: xxspltd [[REG4:[0-9]+]], [[REG2]], 0
+; CHECK: xxpermdi [[REG5:[0-9]+]], [[REG4]], [[REG1]], 1
+; CHECK: stxvd2x [[REG5]]
+
+; CHECK-P9-LABEL: @bar0
+; CHECK-P9-DAG: lxvx [[REG1:[0-9]+]]
+; CHECK-P9-DAG: lfd [[REG2:[0-9]+]], 0(3)
+; CHECK-P9: xxspltd [[REG4:[0-9]+]], [[REG2]], 0
+; CHECK-P9: xxpermdi [[REG5:[0-9]+]], [[REG1]], [[REG4]], 1
+; CHECK-P9: stxvx [[REG5]]
+
 define void @bar1() {
-; CHECK-LABEL: bar1:
-; CHECK:   # %bb.0: # %entry
-; CHECK:     addis r3, r2, .LC0@toc@ha
-; CHECK:     addis r4, r2, .LC1@toc@ha
-; CHECK:     ld r3, .LC0@toc@l(r3)
-; CHECK:     addis r3, r2, .LC2@toc@ha
-; CHECK:     ld r3, .LC2@toc@l(r3)
-; CHECK:     xxmrghd vs0, vs1, vs0
-; CHECK:     stxvd2x vs0, 0, r3
-; CHECK:     blr
-;
-; CHECK-P9-LABEL: bar1:
-; CHECK-P9:   # %bb.0: # %entry
-; CHECK-P9:     addis r3, r2, .LC0@toc@ha
-; CHECK-P9:     addis r4, r2, .LC1@toc@ha
-; CHECK-P9:     ld r3, .LC0@toc@l(r3)
-; CHECK-P9:     ld r4, .LC1@toc@l(r4)
-; CHECK-P9:     lfd f0, 0(r3)
-; CHECK-P9:     lxvx vs1, 0, r4
-; CHECK-P9:     addis r3, r2, .LC2@toc@ha
-; CHECK-P9:     ld r3, .LC2@toc@l(r3)
-; CHECK-P9:     xxpermdi vs0, f0, f0, 2
-; CHECK-P9:     xxmrgld vs0, vs0, vs1
-; CHECK-P9:     stxvx vs0, 0, r3
-; CHECK-P9:     blr
 entry:
   %0 = load <2 x double>, <2 x double>* @x, align 16
   %1 = load double, double* @y, align 8
@@ -87,3 +48,17 @@ entry:
   ret void
 }
 
+; CHECK-LABEL: @bar1
+; CHECK-DAG: lxvd2x [[REG1:[0-9]+]]
+; CHECK-DAG: lfdx [[REG2:[0-9]+]]
+; CHECK: xxspltd [[REG4:[0-9]+]], [[REG2]], 0
+; CHECK: xxmrghd [[REG5:[0-9]+]], [[REG1]], [[REG4]]
+; CHECK: stxvd2x [[REG5]]
+
+; CHECK-P9-LABEL: @bar1
+; CHECK-P9-DAG: lxvx [[REG1:[0-9]+]]
+; CHECK-P9-DAG: lfd [[REG2:[0-9]+]], 0(3)
+; CHECK-P9: xxspltd [[REG4:[0-9]+]], [[REG2]], 0
+; CHECK-P9: xxmrgld [[REG5:[0-9]+]], [[REG4]], [[REG1]]
+; CHECK-P9: stxvx [[REG5]]
+
diff --git a/test/CodeGen/PowerPC/vsx_insert_extract_le.ll b/test/CodeGen/PowerPC/vsx_insert_extract_le.ll
index ef7d8f3..df4cf35 100644
--- a/test/CodeGen/PowerPC/vsx_insert_extract_le.ll
+++ b/test/CodeGen/PowerPC/vsx_insert_extract_le.ll
@@ -1,125 +1,74 @@
-; RUN: llc -verify-machineinstrs -mcpu=pwr8 -mattr=+vsx -ppc-vsr-nums-as-vr \
-; RUN:   -ppc-asm-full-reg-names -mtriple=powerpc64le-unknown-linux-gnu < %s \
-; RUN:   | FileCheck %s
+; RUN: llc -verify-machineinstrs -mcpu=pwr8 -mattr=+vsx \
+; RUN:   -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s
 
-; RUN: llc -verify-machineinstrs -mcpu=pwr9 -mattr=-power9-vector -ppc-vsr-nums-as-vr \
-; RUN:   -ppc-asm-full-reg-names -mtriple=powerpc64le-unknown-linux-gnu < %s \
-; RUN:   | FileCheck --check-prefix=CHECK-P9-VECTOR %s
+; RUN: llc -verify-machineinstrs -mcpu=pwr9 -mattr=-power9-vector \
+; RUN:   -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s
 
-; RUN: llc -verify-machineinstrs -mcpu=pwr9 -ppc-vsr-nums-as-vr -ppc-asm-full-reg-names \
+; RUN: llc -verify-machineinstrs -mcpu=pwr9 \
 ; RUN:   -mtriple=powerpc64le-unknown-linux-gnu < %s | FileCheck %s \
 ; RUN:   --check-prefix=CHECK-P9 --implicit-check-not xxswapd
 
 define <2 x double> @testi0(<2 x double>* %p1, double* %p2) {
-; CHECK-LABEL: testi0:
-; CHECK:       # %bb.0:
-; CHECK-NEXT:    lxvd2x vs0, 0, r3
-; CHECK-NEXT:    lfdx f1, 0, r4
-; CHECK-NEXT:    xxswapd vs0, vs0
-; CHECK-NEXT:    xxspltd vs1, vs1, 0
-; CHECK-NEXT:    xxpermdi v2, vs0, vs1, 1
-; CHECK-NEXT:    blr
-;
-; CHECK-P9-VECTOR-LABEL: testi0:
-; CHECK-P9-VECTOR:       # %bb.0:
-; CHECK-P9-VECTOR-NEXT:    lxvd2x vs0, 0, r3
-; CHECK-P9-VECTOR-NEXT:    lfdx f1, 0, r4
-; CHECK-P9-VECTOR-NEXT:    xxspltd vs1, vs1, 0
-; CHECK-P9-VECTOR-NEXT:    xxswapd vs0, vs0
-; CHECK-P9-VECTOR-NEXT:    xxpermdi v2, vs0, vs1, 1
-; CHECK-P9-VECTOR-NEXT:    blr
-;
-; CHECK-P9-LABEL: testi0:
-; CHECK-P9:       # %bb.0:
-; CHECK-P9-NEXT:    lfd f0, 0(r4)
-; CHECK-P9-NEXT:    lxv vs1, 0(r3)
-; CHECK-P9-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-P9-NEXT:    xxpermdi v2, vs1, vs0, 1
-; CHECK-P9-NEXT:    blr
   %v = load <2 x double>, <2 x double>* %p1
   %s = load double, double* %p2
   %r = insertelement <2 x double> %v, double %s, i32 0
   ret <2 x double> %r
 
+; CHECK-LABEL: testi0
+; CHECK: lxvd2x 0, 0, 3
+; CHECK: lfdx 1, 0, 4
+; CHECK-DAG: xxspltd 1, 1, 0
+; CHECK-DAG: xxswapd 0, 0
+; CHECK: xxpermdi 34, 0, 1, 1
 
+; CHECK-P9-LABEL: testi0
+; CHECK-P9: lfd [[REG1:[0-9]+]], 0(4)
+; CHECK-P9: lxv [[REG2:[0-9]+]], 0(3)
+; CHECK-P9: xxspltd [[REG3:[0-9]+]], [[REG1]], 0
+; CHECK-P9: xxpermdi 34, [[REG2]], [[REG3]], 1
 }
 
 define <2 x double> @testi1(<2 x double>* %p1, double* %p2) {
-; CHECK-LABEL: testi1:
-; CHECK:       # %bb.0:
-; CHECK-NEXT:    lxvd2x vs0, 0, r3
-; CHECK-NEXT:    lfdx f1, 0, r4
-; CHECK-NEXT:    xxswapd vs0, vs0
-; CHECK-NEXT:    xxspltd vs1, vs1, 0
-; CHECK-NEXT:    xxmrgld v2, vs1, vs0
-; CHECK-NEXT:    blr
-;
-; CHECK-P9-VECTOR-LABEL: testi1:
-; CHECK-P9-VECTOR:       # %bb.0:
-; CHECK-P9-VECTOR-NEXT:    lxvd2x vs0, 0, r3
-; CHECK-P9-VECTOR-NEXT:    lfdx f1, 0, r4
-; CHECK-P9-VECTOR-NEXT:    xxspltd vs1, vs1, 0
-; CHECK-P9-VECTOR-NEXT:    xxswapd vs0, vs0
-; CHECK-P9-VECTOR-NEXT:    xxmrgld v2, vs1, vs0
-; CHECK-P9-VECTOR-NEXT:    blr
-;
-; CHECK-P9-LABEL: testi1:
-; CHECK-P9:       # %bb.0:
-; CHECK-P9-NEXT:    lfd f0, 0(r4)
-; CHECK-P9-NEXT:    lxv vs1, 0(r3)
-; CHECK-P9-NEXT:    xxpermdi vs0, f0, f0, 2
-; CHECK-P9-NEXT:    xxmrgld v2, vs0, vs1
-; CHECK-P9-NEXT:    blr
   %v = load <2 x double>, <2 x double>* %p1
   %s = load double, double* %p2
   %r = insertelement <2 x double> %v, double %s, i32 1
   ret <2 x double> %r
 
+; CHECK-LABEL: testi1
+; CHECK: lxvd2x 0, 0, 3
+; CHECK: lfdx 1, 0, 4
+; CHECK-DAG: xxspltd 1, 1, 0
+; CHECK-DAG: xxswapd 0, 0
+; CHECK: xxmrgld 34, 1, 0
 
+; CHECK-P9-LABEL: testi1
+; CHECK-P9: lfd [[REG1:[0-9]+]], 0(4)
+; CHECK-P9: lxv [[REG2:[0-9]+]], 0(3)
+; CHECK-P9: xxspltd [[REG3:[0-9]+]], [[REG1]], 0
+; CHECK-P9: xxmrgld 34, [[REG3]], [[REG2]]
 }
 
 define double @teste0(<2 x double>* %p1) {
-; CHECK-LABEL: teste0:
-; CHECK:       # %bb.0:
-; CHECK-NEXT:    lxvd2x vs1, 0, r3
-; CHECK:         blr
-;
-; CHECK-P9-VECTOR-LABEL: teste0:
-; CHECK-P9-VECTOR:       # %bb.0:
-; CHECK-P9-VECTOR-NEXT:    lxvd2x vs1, 0, r3
-; CHECK-P9-VECTOR:         blr
-;
-; CHECK-P9-LABEL: teste0:
-; CHECK-P9:       # %bb.0:
-; CHECK-P9-NEXT:    lfd f1, 0(r3)
-; CHECK-P9-NEXT:    blr
   %v = load <2 x double>, <2 x double>* %p1
   %r = extractelement <2 x double> %v, i32 0
   ret double %r
 
+; CHECK-LABEL: teste0
+; CHECK: lxvd2x 1, 0, 3
 
+; CHECK-P9-LABEL: teste0
+; CHECK-P9: lfd 1, 0(3)
 }
 
 define double @teste1(<2 x double>* %p1) {
-; CHECK-LABEL: teste1:
-; CHECK:       # %bb.0:
-; CHECK-NEXT:    lxvd2x vs0, 0, r3
-; CHECK-NEXT:    xxswapd vs1, vs0
-; CHECK:         blr
-;
-; CHECK-P9-VECTOR-LABEL: teste1:
-; CHECK-P9-VECTOR:       # %bb.0:
-; CHECK-P9-VECTOR-NEXT:    lxvd2x vs0, 0, r3
-; CHECK-P9-VECTOR-NEXT:    xxswapd vs1, vs0
-; CHECK-P9-VECTOR:         blr
-;
-; CHECK-P9-LABEL: teste1:
-; CHECK-P9:       # %bb.0:
-; CHECK-P9-NEXT:    lfd f1, 8(r3)
-; CHECK-P9-NEXT:    blr
   %v = load <2 x double>, <2 x double>* %p1
   %r = extractelement <2 x double> %v, i32 1
   ret double %r
 
+; CHECK-LABEL: teste1
+; CHECK: lxvd2x 0, 0, 3
+; CHECK: xxswapd 1, 0
 
+; CHECK-P9-LABEL: teste1
+; CHECK-P9: lfd 1, 8(3)
 }
diff --git a/test/CodeGen/X86/mingw-comdats.ll b/test/CodeGen/X86/mingw-comdats.ll
index 35f4fd12..2e9ebd8 100644
--- a/test/CodeGen/X86/mingw-comdats.ll
+++ b/test/CodeGen/X86/mingw-comdats.ll
@@ -1,14 +1,13 @@
-; RUN: llc -function-sections -mtriple=x86_64-windows-itanium < %s | FileCheck %s
-; RUN: llc -function-sections -mtriple=x86_64-windows-msvc < %s | FileCheck %s
-; RUN: llc -function-sections -mtriple=x86_64-w64-windows-gnu < %s | FileCheck %s --check-prefix=GNU
-; RUN: llc -function-sections -mtriple=i686-w64-windows-gnu < %s | FileCheck %s --check-prefix=GNU32
-; RUN: llc -function-sections -mtriple=x86_64-w64-windows-gnu < %s -filetype=obj | llvm-objdump - -headers | FileCheck %s --check-prefix=GNUOBJ
+; RUN: llc -mtriple=x86_64-windows-itanium < %s | FileCheck %s
+; RUN: llc -mtriple=x86_64-windows-msvc < %s | FileCheck %s
+; RUN: llc -mtriple=x86_64-w64-windows-gnu < %s | FileCheck %s --check-prefix=GNU
+; RUN: llc -mtriple=i686-w64-windows-gnu < %s | FileCheck %s --check-prefix=GNU32
+; RUN: llc -mtriple=x86_64-w64-windows-gnu < %s -filetype=obj | llvm-objdump - -headers | FileCheck %s --check-prefix=GNUOBJ
 
 ; GCC and MSVC handle comdats completely differently. Make sure we do the right
 ; thing for each.
 
-; Modeled on this C++ source, with additional modifications for
-; -ffunction-sections:
+; Generated with this C++ source:
 ; int bar(int);
 ; __declspec(selectany) int gv = 42;
 ; inline int foo(int x) { return bar(x) + gv; }
@@ -27,24 +26,8 @@ entry:
   ret i32 %call
 }
 
-; CHECK: .section        .text,"xr",one_only,main
 ; CHECK: main:
-; GNU: .section        .text$main,"xr",one_only,main
 ; GNU: main:
-; GNU32: .section        .text$main,"xr",one_only,_main
-; GNU32: _main:
-
-define dso_local x86_fastcallcc i32 @fastcall(i32 %x, i32 %y) {
-  %rv = add i32 %x, %y
-  ret i32 %rv
-}
-
-; CHECK: .section        .text,"xr",one_only,fastcall
-; CHECK: fastcall:
-; GNU: .section        .text$fastcall,"xr",one_only,fastcall
-; GNU: fastcall:
-; GNU32: .section        .text$fastcall,"xr",one_only,@fastcall@8
-; GNU32: @fastcall@8:
 
 ; Function Attrs: inlinehint uwtable
 define linkonce_odr dso_local i32 @_Z3fooi(i32 %x) #1 comdat {
@@ -67,9 +50,9 @@ entry:
 ; GNU: gv:
 ; GNU: .long 42
 
-; GNU32: .section        .text$_Z3fooi,"xr",discard,__Z3fooi
+; GNU32: .section        .text$__Z3fooi,"xr",discard,__Z3fooi
 ; GNU32: __Z3fooi:
-; GNU32: .section        .data$gv,"dw",discard,_gv
+; GNU32: .section        .data$_gv,"dw",discard,_gv
 ; GNU32: _gv:
 ; GNU32: .long 42
 
diff --git a/test/DebugInfo/Mips/eh_frame.ll b/test/DebugInfo/Mips/eh_frame.ll
deleted file mode 100644
index 4687443..0000000
--- a/test/DebugInfo/Mips/eh_frame.ll
+++ /dev/null
@@ -1,38 +0,0 @@
-; RUN: llc -mtriple mips-unknown-linux-gnu -mattr=+micromips -O3 -filetype=obj -o - %s | llvm-readelf -r | FileCheck %s
-
-; CHECK: .rel.eh_frame
-; CHECK: DW.ref.__gxx_personality_v0
-; CHECK-NEXT: .text
-; CHECK-NEXT: .gcc_except_table
-
-@_ZTIi = external constant i8*
-
-define dso_local i32 @main() local_unnamed_addr personality i8* bitcast (i32 (...)* @__gxx_personality_v0 to i8*) {
-entry:
-  %exception.i = tail call i8* @__cxa_allocate_exception(i32 4) nounwind
-  %0 = bitcast i8* %exception.i to i32*
-  store i32 5, i32* %0, align 16
-  invoke void @__cxa_throw(i8* %exception.i, i8* bitcast (i8** @_ZTIi to i8*), i8* null) noreturn
-          to label %.noexc unwind label %return
-
-.noexc:
-  unreachable
-
-return:
-  %1 = landingpad { i8*, i32 }
-          catch i8* null
-  %2 = extractvalue { i8*, i32 } %1, 0
-  %3 = tail call i8* @__cxa_begin_catch(i8* %2) nounwind
-  tail call void @__cxa_end_catch()
-  ret i32 0
-}
-
-declare i32 @__gxx_personality_v0(...)
-
-declare i8* @__cxa_begin_catch(i8*) local_unnamed_addr
-
-declare void @__cxa_end_catch() local_unnamed_addr
-
-declare i8* @__cxa_allocate_exception(i32) local_unnamed_addr
-
-declare void @__cxa_throw(i8*, i8*, i8*) local_unnamed_addr
diff --git a/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-sge-to-icmp-sle.ll b/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-sge-to-icmp-sle.ll
index ca1b86c..7be784a 100644
--- a/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-sge-to-icmp-sle.ll
+++ b/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-sge-to-icmp-sle.ll
@@ -23,6 +23,18 @@ define i1 @p0(i8 %x) {
   ret i1 %ret
 }
 
+define i1 @pv(i8 %x, i8 %y) {
+; CHECK-LABEL: @pv(
+; CHECK-NEXT:    [[TMP0:%.*]] = lshr i8 -1, [[Y:%.*]]
+; CHECK-NEXT:    [[TMP1:%.*]] = icmp sge i8 [[TMP0]], [[X:%.*]]
+; CHECK-NEXT:    ret i1 [[TMP1]]
+;
+  %tmp0 = lshr i8 -1, %y
+  %tmp1 = and i8 %tmp0, %x
+  %ret = icmp sge i8 %tmp1, %x
+  ret i1 %ret
+}
+
 ; ============================================================================ ;
 ; Vector tests
 ; ============================================================================ ;
@@ -108,9 +120,8 @@ define i1 @cv0(i8 %y) {
 ; CHECK-LABEL: @cv0(
 ; CHECK-NEXT:    [[X:%.*]] = call i8 @gen8()
 ; CHECK-NEXT:    [[TMP0:%.*]] = lshr i8 -1, [[Y:%.*]]
-; CHECK-NEXT:    [[TMP1:%.*]] = and i8 [[X]], [[TMP0]]
-; CHECK-NEXT:    [[RET:%.*]] = icmp sge i8 [[TMP1]], [[X]]
-; CHECK-NEXT:    ret i1 [[RET]]
+; CHECK-NEXT:    [[TMP1:%.*]] = icmp sle i8 [[X]], [[TMP0]]
+; CHECK-NEXT:    ret i1 [[TMP1]]
 ;
   %x = call i8 @gen8()
   %tmp0 = lshr i8 -1, %y
@@ -185,42 +196,3 @@ define <2 x i1> @n2(<2 x i8> %x) {
   %ret = icmp sge <2 x i8> %tmp0, %x
   ret <2 x i1> %ret
 }
-
-; ============================================================================ ;
-; Potential miscompiles.
-; ============================================================================ ;
-
-define i1 @nv(i8 %x, i8 %y) {
-; CHECK-LABEL: @nv(
-; CHECK-NEXT:    [[TMP0:%.*]] = lshr i8 -1, [[Y:%.*]]
-; CHECK-NEXT:    [[TMP1:%.*]] = and i8 [[TMP0]], [[X:%.*]]
-; CHECK-NEXT:    [[RET:%.*]] = icmp sge i8 [[TMP1]], [[X]]
-; CHECK-NEXT:    ret i1 [[RET]]
-;
-  %tmp0 = lshr i8 -1, %y
-  %tmp1 = and i8 %tmp0, %x
-  %ret = icmp sge i8 %tmp1, %x
-  ret i1 %ret
-}
-
-define <2 x i1> @n3_vec(<2 x i8> %x) {
-; CHECK-LABEL: @n3_vec(
-; CHECK-NEXT:    [[TMP0:%.*]] = and <2 x i8> [[X:%.*]], <i8 3, i8 -1>
-; CHECK-NEXT:    [[RET:%.*]] = icmp sge <2 x i8> [[TMP0]], [[X]]
-; CHECK-NEXT:    ret <2 x i1> [[RET]]
-;
-  %tmp0 = and <2 x i8> %x, <i8 3, i8 -1>
-  %ret = icmp sge <2 x i8> %tmp0, %x
-  ret <2 x i1> %ret
-}
-
-define <3 x i1> @n4_vec(<3 x i8> %x) {
-; CHECK-LABEL: @n4_vec(
-; CHECK-NEXT:    [[TMP0:%.*]] = and <3 x i8> [[X:%.*]], <i8 3, i8 undef, i8 -1>
-; CHECK-NEXT:    [[RET:%.*]] = icmp sge <3 x i8> [[TMP0]], [[X]]
-; CHECK-NEXT:    ret <3 x i1> [[RET]]
-;
-  %tmp0 = and <3 x i8> %x, <i8 3, i8 undef, i8 -1>
-  %ret = icmp sge <3 x i8> %tmp0, %x
-  ret <3 x i1> %ret
-}
diff --git a/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-slt-to-icmp-sgt.ll b/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-slt-to-icmp-sgt.ll
index 2957ad5..d1792d1 100644
--- a/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-slt-to-icmp-sgt.ll
+++ b/test/Transforms/InstCombine/canonicalize-constant-low-bit-mask-and-icmp-slt-to-icmp-sgt.ll
@@ -23,6 +23,18 @@ define i1 @p0(i8 %x) {
   ret i1 %ret
 }
 
+define i1 @pv(i8 %x, i8 %y) {
+; CHECK-LABEL: @pv(
+; CHECK-NEXT:    [[TMP0:%.*]] = lshr i8 -1, [[Y:%.*]]
+; CHECK-NEXT:    [[TMP1:%.*]] = icmp slt i8 [[TMP0]], [[X:%.*]]
+; CHECK-NEXT:    ret i1 [[TMP1]]
+;
+  %tmp0 = lshr i8 -1, %y
+  %tmp1 = and i8 %tmp0, %x
+  %ret = icmp slt i8 %tmp1, %x
+  ret i1 %ret
+}
+
 ; ============================================================================ ;
 ; Vector tests
 ; ============================================================================ ;
@@ -108,9 +120,8 @@ define i1 @cv0(i8 %y) {
 ; CHECK-LABEL: @cv0(
 ; CHECK-NEXT:    [[X:%.*]] = call i8 @gen8()
 ; CHECK-NEXT:    [[TMP0:%.*]] = lshr i8 -1, [[Y:%.*]]
-; CHECK-NEXT:    [[TMP1:%.*]] = and i8 [[X]], [[TMP0]]
-; CHECK-NEXT:    [[RET:%.*]] = icmp slt i8 [[TMP1]], [[X]]
-; CHECK-NEXT:    ret i1 [[RET]]
+; CHECK-NEXT:    [[TMP1:%.*]] = icmp sgt i8 [[X]], [[TMP0]]
+; CHECK-NEXT:    ret i1 [[TMP1]]
 ;
   %x = call i8 @gen8()
   %tmp0 = lshr i8 -1, %y
@@ -185,42 +196,3 @@ define <2 x i1> @n2(<2 x i8> %x) {
   %ret = icmp slt <2 x i8> %tmp0, %x
   ret <2 x i1> %ret
 }
-
-; ============================================================================ ;
-; Potential miscompiles.
-; ============================================================================ ;
-
-define i1 @nv(i8 %x, i8 %y) {
-; CHECK-LABEL: @nv(
-; CHECK-NEXT:    [[TMP0:%.*]] = lshr i8 -1, [[Y:%.*]]
-; CHECK-NEXT:    [[TMP1:%.*]] = and i8 [[TMP0]], [[X:%.*]]
-; CHECK-NEXT:    [[RET:%.*]] = icmp slt i8 [[TMP1]], [[X]]
-; CHECK-NEXT:    ret i1 [[RET]]
-;
-  %tmp0 = lshr i8 -1, %y
-  %tmp1 = and i8 %tmp0, %x
-  %ret = icmp slt i8 %tmp1, %x
-  ret i1 %ret
-}
-
-define <2 x i1> @n3(<2 x i8> %x) {
-; CHECK-LABEL: @n3(
-; CHECK-NEXT:    [[TMP0:%.*]] = and <2 x i8> [[X:%.*]], <i8 3, i8 -1>
-; CHECK-NEXT:    [[RET:%.*]] = icmp slt <2 x i8> [[TMP0]], [[X]]
-; CHECK-NEXT:    ret <2 x i1> [[RET]]
-;
-  %tmp0 = and <2 x i8> %x, <i8 3, i8 -1>
-  %ret = icmp slt <2 x i8> %tmp0, %x
-  ret <2 x i1> %ret
-}
-
-define <3 x i1> @n4(<3 x i8> %x) {
-; CHECK-LABEL: @n4(
-; CHECK-NEXT:    [[TMP0:%.*]] = and <3 x i8> [[X:%.*]], <i8 3, i8 undef, i8 -1>
-; CHECK-NEXT:    [[RET:%.*]] = icmp slt <3 x i8> [[TMP0]], [[X]]
-; CHECK-NEXT:    ret <3 x i1> [[RET]]
-;
-  %tmp0 = and <3 x i8> %x, <i8 3, i8 undef, i8 -1>
-  %ret = icmp slt <3 x i8> %tmp0, %x
-  ret <3 x i1> %ret
-}
diff --git a/test/Transforms/LCSSA/rewrite-existing-dbg-values.ll b/test/Transforms/LCSSA/rewrite-existing-dbg-values.ll
deleted file mode 100644
index 231e716..0000000
--- a/test/Transforms/LCSSA/rewrite-existing-dbg-values.ll
+++ /dev/null
@@ -1,69 +0,0 @@
-; RUN: opt -S -lcssa < %s | FileCheck %s
-
-target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
-target triple = "x86_64-unknown-linux-gnu"
-
-; Reproducer for PR39019.
-;
-; Verify that the llvm.dbg.value in the %for.cond.cleanup2 block is rewritten
-; to use the PHI node for %add that is created by LCSSA.
-
-; CHECK-LABEL: for.cond.cleanup2:
-; CHECK-NEXT: [[PN:%[^ ]*]] = phi i32 [ %add.lcssa, %for.cond.cleanup1 ]
-; CHECK-NEXT: call void @llvm.dbg.value(metadata i32 [[PN]], metadata [[VAR:![0-9]+]], metadata !DIExpression())
-; CHECK-NEXT: call void @bar(i32 [[PN]])
-
-; CHECK-LABEL: for.body:
-; CHECK: %add = add nsw i32 0, 2
-; CHECK: call void @llvm.dbg.value(metadata i32 %add, metadata [[VAR]], metadata !DIExpression())
-
-; CHECK: [[VAR]] = !DILocalVariable(name: "sum",
-
-; Function Attrs: nounwind
-define void @foo() #0 !dbg !6 {
-entry:
-  br label %for.cond.preheader, !dbg !12
-
-for.cond.preheader:                               ; preds = %for.cond.cleanup1, %entry
-  br label %for.body, !dbg !12
-
-for.cond.cleanup2:                                ; preds = %for.cond.cleanup1
-  call void @llvm.dbg.value(metadata i32 %add, metadata !9, metadata !DIExpression()), !dbg !12
-  tail call void @bar(i32 %add) #0, !dbg !12
-  ret void, !dbg !12
-
-for.cond.cleanup1:                                ; preds = %for.body
-  br i1 false, label %for.cond.preheader, label %for.cond.cleanup2, !dbg !12
-
-for.body:                                         ; preds = %for.body, %for.cond.preheader
-  %add = add nsw i32 0, 2, !dbg !12
-  call void @llvm.dbg.value(metadata i32 %add, metadata !9, metadata !DIExpression()), !dbg !12
-  br i1 false, label %for.body, label %for.cond.cleanup1, !dbg !12
-}
-
-; Function Attrs: nounwind
-declare void @bar(i32) #0
-
-; Function Attrs: nounwind readnone speculatable
-declare void @llvm.dbg.value(metadata, metadata, metadata) #1
-
-attributes #0 = { nounwind }
-attributes #1 = { nounwind readnone speculatable }
-
-!llvm.dbg.cu = !{!0}
-!llvm.module.flags = !{!3, !4}
-!llvm.ident = !{!5}
-
-!0 = distinct !DICompileUnit(language: DW_LANG_C99, file: !1, producer: "clang version 8.0.0", isOptimized: true, runtimeVersion: 0, emissionKind: FullDebug, enums: !2, globals: !2)
-!1 = !DIFile(filename: "foo.c", directory: "/")
-!2 = !{}
-!3 = !{i32 2, !"Dwarf Version", i32 4}
-!4 = !{i32 2, !"Debug Info Version", i32 3}
-!5 = !{!"clang version 8.0.0"}
-!6 = distinct !DISubprogram(name: "foo", scope: !1, file: !1, line: 10, type: !7, isLocal: false, isDefinition: true, scopeLine: 10, isOptimized: true, unit: !0, retainedNodes: !8)
-!7 = !DISubroutineType(types: !2)
-!8 = !{!9}
-!9 = !DILocalVariable(name: "sum", scope: !10, file: !1, line: 11, type: !11)
-!10 = !DILexicalBlockFile(scope: !6, file: !1, discriminator: 0)
-!11 = !DIBasicType(name: "int", size: 32, encoding: DW_ATE_signed)
-!12 = !DILocation(line: 0, scope: !10)
diff --git a/tools/llvm-config/CMakeLists.txt b/tools/llvm-config/CMakeLists.txt
index f59402a..a0bd36c 100644
--- a/tools/llvm-config/CMakeLists.txt
+++ b/tools/llvm-config/CMakeLists.txt
@@ -37,7 +37,7 @@ set(LLVM_CFLAGS "${CMAKE_C_FLAGS} ${CMAKE_C_FLAGS_${uppercase_CMAKE_BUILD_TYPE}}
 set(LLVM_CXXFLAGS "${CMAKE_CXX_FLAGS} ${CMAKE_CXX_FLAGS_${uppercase_CMAKE_BUILD_TYPE}} ${COMPILE_FLAGS} ${LLVM_DEFINITIONS}")
 set(LLVM_BUILD_SYSTEM cmake)
 set(LLVM_HAS_RTTI ${LLVM_CONFIG_HAS_RTTI})
-set(LLVM_DYLIB_VERSION "${LLVM_VERSION_MAJOR}.${LLVM_VERSION_MINOR}${LLVM_VERSION_SUFFIX}")
+set(LLVM_DYLIB_VERSION "${LLVM_VERSION_MAJOR}${LLVM_VERSION_SUFFIX}")
 set(LLVM_HAS_GLOBAL_ISEL "ON")
 
 # Use the C++ link flags, since they should be a superset of C link flags.
diff --git a/tools/llvm-exegesis/lib/CMakeLists.txt b/tools/llvm-exegesis/lib/CMakeLists.txt
index 194304a..175c2ad 100644
--- a/tools/llvm-exegesis/lib/CMakeLists.txt
+++ b/tools/llvm-exegesis/lib/CMakeLists.txt
@@ -1,16 +1,12 @@
-set(TARGETS_TO_APPEND "")
-
 if (LLVM_TARGETS_TO_BUILD MATCHES "X86")
   add_subdirectory(X86)
-  set(TARGETS_TO_APPEND "${TARGETS_TO_APPEND} X86")
+  set(LLVM_EXEGESIS_TARGETS "${LLVM_EXEGESIS_TARGETS} X86" PARENT_SCOPE)
 endif()
 if (LLVM_TARGETS_TO_BUILD MATCHES "AArch64")
   add_subdirectory(AArch64)
-  set(TARGETS_TO_APPEND "${TARGETS_TO_APPEND} AArch64")
+  set(LLVM_EXEGESIS_TARGETS "${LLVM_EXEGESIS_TARGETS} AArch64" PARENT_SCOPE)
 endif()
 
-set(LLVM_EXEGESIS_TARGETS "${LLVM_EXEGESIS_TARGETS} ${TARGETS_TO_APPEND}" PARENT_SCOPE)
-
 add_library(LLVMExegesis
   STATIC
   Analysis.cpp
diff --git a/tools/llvm-shlib/simple_version_script.map.in b/tools/llvm-shlib/simple_version_script.map.in
index e9515fe..d58a8b3 100644
--- a/tools/llvm-shlib/simple_version_script.map.in
+++ b/tools/llvm-shlib/simple_version_script.map.in
@@ -1 +1 @@
-LLVM_@LLVM_VERSION_MAJOR@.@LLVM_VERSION_MINOR@ { global: *; };
+LLVM_@LLVM_VERSION_MAJOR@ { global: *; };
diff --git a/unittests/ADT/OptionalTest.cpp b/unittests/ADT/OptionalTest.cpp
index 20bc9da..2e09c53 100644
--- a/unittests/ADT/OptionalTest.cpp
+++ b/unittests/ADT/OptionalTest.cpp
@@ -518,5 +518,13 @@ TEST_F(OptionalTest, OperatorGreaterEqual) {
   CheckRelation<GreaterEqual>(InequalityLhs, InequalityRhs, !IsLess);
 }
 
+#if __has_feature(is_trivially_copyable) && defined(_LIBCPP_VERSION)
+static_assert(std::is_trivially_copyable<Optional<int>>::value,
+              "Should be trivially copyable");
+static_assert(
+    !std::is_trivially_copyable<Optional<NonDefaultConstructible>>::value,
+    "Shouldn't be trivially copyable");
+#endif
+
 } // end anonymous namespace
 
